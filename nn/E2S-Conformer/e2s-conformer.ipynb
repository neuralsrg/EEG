{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG to Speech Transformer with Conformer as encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import subprocess\n",
    "\n",
    "import math\n",
    "import pickle\n",
    "from glob import glob\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from typing import List, Tuple, Optional\n",
    "from IPython.display import Audio, FileLink, display, clear_output\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(1337)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from wavelets_pytorch.transform import WaveletTransformTorch\n",
    "from wavelets_pytorch.wavelets import Morlet\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/kaggle/input/internal-speech-recognition/Vartanov/audios'\n",
    "\n",
    "A = \"A.wav\"\n",
    "B = \"B.wav\"\n",
    "F = \"F.wav\"\n",
    "G = \"G.wav\"\n",
    "M = \"M.wav\"\n",
    "R = \"R.wav\"\n",
    "U = \"U.wav\"\n",
    "\n",
    "Ba = \"Ba.wav\"\n",
    "Bu = \"Bu.wav\"\n",
    "Fa = \"Fa.wav\"\n",
    "Fu = \"Fu.wav\"\n",
    "Ga = \"Ga.wav\"\n",
    "Gu = \"Gu.wav\"\n",
    "Ma = \"Ma.wav\"\n",
    "Mu = \"Mu.wav\"\n",
    "Ra = \"Ra.wav\"\n",
    "Ru = \"Ru.wav\"\n",
    "\n",
    "Biblioteka = \"St1.wav\"\n",
    "Raketa = \"St2.wav\"\n",
    "Kurier = \"St3.wav\"\n",
    "Ograda = \"St4.wav\"\n",
    "Haketa = \"St5.wav\"\n",
    "\n",
    "phonemes_m3_labels = {\n",
    "    12: os.path.join(base, \"phonemes\", A),\n",
    "    22: os.path.join(base, \"phonemes\", A),\n",
    "    13: os.path.join(base, \"phonemes\", B),\n",
    "    23: os.path.join(base, \"phonemes\", B),\n",
    "    14: os.path.join(base, \"phonemes\", F),\n",
    "    24: os.path.join(base, \"phonemes\", F),\n",
    "    15: os.path.join(base, \"phonemes\", G),\n",
    "    25: os.path.join(base, \"phonemes\", G),\n",
    "    16: os.path.join(base, \"phonemes\", M),\n",
    "    26: os.path.join(base, \"phonemes\", M),\n",
    "    17: os.path.join(base, \"phonemes\", R),\n",
    "    27: os.path.join(base, \"phonemes\", R),\n",
    "    18: os.path.join(base, \"phonemes\", U),\n",
    "    28: os.path.join(base, \"phonemes\", U)\n",
    "}\n",
    "\n",
    "phonemes_m4_labels = {\n",
    "    1: os.path.join(base, \"phonemes\", A),\n",
    "    11: os.path.join(base, \"phonemes\", A),\n",
    "    2: os.path.join(base, \"phonemes\", B),\n",
    "    12: os.path.join(base, \"phonemes\", B),\n",
    "    3: os.path.join(base, \"phonemes\", F),\n",
    "    13: os.path.join(base, \"phonemes\", F),\n",
    "    4: os.path.join(base, \"phonemes\", G),\n",
    "    14: os.path.join(base, \"phonemes\", G),\n",
    "    5: os.path.join(base, \"phonemes\", M),\n",
    "    15: os.path.join(base, \"phonemes\", M),\n",
    "    6: os.path.join(base, \"phonemes\", R),\n",
    "    16: os.path.join(base, \"phonemes\", R),\n",
    "    7: os.path.join(base, \"phonemes\", U),\n",
    "    17: os.path.join(base, \"phonemes\", U)\n",
    "}\n",
    "\n",
    "syllables_labels = {\n",
    "    1: os.path.join(base, \"syllables\", Ba),\n",
    "    11: os.path.join(base, \"syllables\", Ba),\n",
    "    2: os.path.join(base, \"syllables\", Fa),\n",
    "    12: os.path.join(base, \"syllables\", Fa),\n",
    "    3: os.path.join(base, \"syllables\", Ga),\n",
    "    13: os.path.join(base, \"syllables\", Ga),\n",
    "    4: os.path.join(base, \"syllables\", Ma),\n",
    "    14: os.path.join(base, \"syllables\", Ma),\n",
    "    5: os.path.join(base, \"syllables\", Ra),\n",
    "    15: os.path.join(base, \"syllables\", Ra),\n",
    "    6: os.path.join(base, \"syllables\", Bu),\n",
    "    16: os.path.join(base, \"syllables\", Bu),\n",
    "    7: os.path.join(base, \"syllables\", Ru),\n",
    "    17: os.path.join(base, \"syllables\", Ru),\n",
    "    8: os.path.join(base, \"syllables\", Mu),\n",
    "    18: os.path.join(base, \"syllables\", Mu),\n",
    "    9: os.path.join(base, \"syllables\", Fu),\n",
    "    19: os.path.join(base, \"syllables\", Fu),\n",
    "    10: os.path.join(base, \"syllables\", Gu),\n",
    "    20: os.path.join(base, \"syllables\", Gu)\n",
    "}\n",
    "\n",
    "words_labels = {\n",
    "    11: os.path.join(base, \"words\", Biblioteka),\n",
    "    21: os.path.join(base, \"words\", Biblioteka),\n",
    "    12: os.path.join(base, \"words\", Raketa),\n",
    "    22: os.path.join(base, \"words\", Raketa),\n",
    "    13: os.path.join(base, \"words\", Kurier),\n",
    "    23: os.path.join(base, \"words\", Kurier),\n",
    "    14: os.path.join(base, \"words\", Ograda),\n",
    "    24: os.path.join(base, \"words\", Ograda),\n",
    "    15: os.path.join(base, \"words\", Haketa),\n",
    "    25: os.path.join(base, \"words\", Haketa)\n",
    "}\n",
    "\n",
    "sections = [\"syllables\", \"phonemes_m3\", \"phonemes_m4\", \"words\"]\n",
    "\n",
    "audio_map = {\n",
    "    \"syllables\": syllables_labels,\n",
    "    \"phonemes_m3\": phonemes_m3_labels,\n",
    "    \"phonemes_m4\": phonemes_m4_labels,\n",
    "    \"words\": words_labels\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \n",
    "    'path': '/kaggle/input/internal-speech-recognition/Vartanov/feather',\n",
    "    'audio_maps': audio_map,\n",
    "    \n",
    "    # Dataset\n",
    "    'fragment_length': 2012,\n",
    "    'partition_size': 32,\n",
    "    'val_ratio': 0.15,\n",
    "    'seed': 1337,\n",
    "    'sound_channel': 1,\n",
    "    \n",
    "    # EEG\n",
    "    'eeg_sr': 1006,\n",
    "    'n_channels': 63,\n",
    "    'in_seq_len': 1145,\n",
    "    \n",
    "    # Audio\n",
    "    'audio_sr': 44100,\n",
    "    'sound_size': 50176,\n",
    "    \n",
    "    # STFT Patameters\n",
    "    'n_fft': 2048,\n",
    "    'hop_size': 512,\n",
    "    \n",
    "    # Model\n",
    "    \n",
    "    # Convolution Module\n",
    "    'kernel_size': 31,\n",
    "    'conv_module_dropout': .1,\n",
    "    \n",
    "    # Positional Encoding\n",
    "    'emb_dropout': .1,\n",
    "    \n",
    "    # Transformer\n",
    "    'd_model': 512,\n",
    "    'nhead': 8,\n",
    "    'num_encoder_layers': 6,\n",
    "    'num_decoder_layers': 6,\n",
    "    'dim_feedforward': 2048,\n",
    "    'dropout': 0.1,\n",
    "    'activation': 'relu',\n",
    "    \n",
    "    # Scheduler\n",
    "    'base_lr': 0.2,\n",
    "    'min_lr': 1e-5,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrogram to sound\n",
    "\n",
    "def restore(D: np.array, frame_size=config['n_fft'], hop_length=config['hop_size'], epochs: int = 2, window: str = 'hann'):\n",
    "\n",
    "    length = (D.shape[1] + 1) * hop_length  # (D.shape[1] - 1 + 2) * hop_length\n",
    "    D = np.concatenate((np.zeros((D.shape[0], 1)), D, np.zeros((D.shape[0], 1))), axis=1)\n",
    "    mag, _ = librosa.magphase(D)\n",
    "    phase = np.exp(1.j * np.random.uniform(0., 2*np.pi, size=mag.shape))\n",
    "    x_ = librosa.istft(mag * phase, hop_length=hop_length, center=True, window=window, length=length)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        _, phase = librosa.magphase(librosa.stft(x_, n_fft=frame_size, hop_length=hop_length, center=True,\n",
    "                                                 window=window))\n",
    "        x_ = librosa.istft(mag * phase, hop_length=hop_length, center=True, window=window, length=length)\n",
    "\n",
    "    return x_[hop_length:-hop_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, path: str, audio_maps: dict, fragment_length: int = 2012, partition_size: int = 32,\n",
    "                 sample_rate: int = 44100, sound_channel: int = 1, val_ratio: float = 0.15, seed: int = 1337):\n",
    "        '''\n",
    "        path: path to sections (folders)\n",
    "        audio_maps: two-level map: section names -> labels -> audio_paths\n",
    "        fragment_lengtht: length of fragment after label\n",
    "        partition_size: number of nonzero labels in each csv file\n",
    "        '''\n",
    "        super().__init__()\n",
    "        rnd = random.Random(seed)\n",
    "        \n",
    "        self.sections = os.listdir(path)\n",
    "        rnd.shuffle(self.sections)\n",
    "        assert set(self.sections) == set(audio_maps.keys()), \"Sections must be the same!\"\n",
    "        self.audio_maps = audio_maps\n",
    "        \n",
    "        all_paths = []\n",
    "        for sec in self.sections:\n",
    "            l = os.listdir(os.path.join(path, sec))\n",
    "            rnd.shuffle(l)\n",
    "            all_paths.append([os.path.join(path, sec, file) for file in l])\n",
    "                \n",
    "        # all_paths = [[os.path.join(path, sec, file) for file in sorted(os.listdir(os.path.join(path, sec)))] for sec in self.sections]\n",
    "        num_all_files = [len(elem) for elem in all_paths]\n",
    "        splits = [int(elem * val_ratio) for elem in num_all_files]\n",
    "        \n",
    "        self.val_paths = [sec_paths[:split] for sec_paths, split in zip(all_paths, splits)]\n",
    "        self.paths = [sec_paths[split:] for sec_paths, split in zip(all_paths, splits)]\n",
    "        \n",
    "        self.sec_num_files = [len(elem) for elem in self.paths]\n",
    "        self.sec_cumnum = np.cumsum(self.sec_num_files) * partition_size\n",
    "        self.total_num_files = sum(self.sec_num_files)\n",
    "        \n",
    "        self.sec_num_val_files = [len(elem) for elem in self.val_paths]\n",
    "        self.sec_val_cumnum = np.cumsum(self.sec_num_val_files) * partition_size\n",
    "        self.total_num_val_files = sum(self.sec_num_val_files)\n",
    "        \n",
    "        self.partition_size = partition_size\n",
    "        self.fragment_length = fragment_length\n",
    "        self.sr = sample_rate\n",
    "        self.sound_channel = sound_channel\n",
    "        self.val_mode = False\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        num = self.total_num_val_files if self.val_mode else self.total_num_files\n",
    "        return num * self.partition_size\n",
    "    \n",
    "    def set_val_mode(self, mode: bool):\n",
    "        '''\n",
    "        Switch between train/val subsets\n",
    "        '''\n",
    "        assert mode in [True, False], \"Incorrect mode type!\"\n",
    "        self.val_mode = mode\n",
    "        return self\n",
    "    \n",
    "    def to_section(self, idx: int) -> Tuple[int, int]:\n",
    "        '''\n",
    "        Get file section and inner index by its absolute index\n",
    "        '''\n",
    "        cumnum = self.sec_val_cumnum if self.val_mode else self.sec_cumnum\n",
    "        section = np.where(idx < cumnum)[0][0]\n",
    "        section_idx = idx if (section == 0) else (idx - cumnum[section - 1])\n",
    "        return section, section_idx\n",
    "    \n",
    "    def get_audio(self, section: str, label: int) -> torch.Tensor:\n",
    "        '''\n",
    "        Get audio by section and corresponding label\n",
    "        '''\n",
    "        section_name = self.sections[section]\n",
    "        audio, current_sr = torchaudio.load(self.audio_maps[section_name][label])\n",
    "        audio = torchaudio.functional.resample(audio, orig_freq=current_sr, new_freq=self.sr)\n",
    "        return audio[self.sound_channel]\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        '''\n",
    "        int idx: file ID\n",
    "        return: EEG fragment with its corresponding audio\n",
    "        '''\n",
    "        section, section_idx = self.to_section(idx)\n",
    "        paths_source = self.val_paths if self.val_mode else self.paths\n",
    "        file_path = paths_source[section][section_idx // self.partition_size]\n",
    "        \n",
    "        start = (section_idx % self.partition_size) * self.fragment_length\n",
    "        end = start + self.fragment_length\n",
    "        \n",
    "        data = pd.read_feather(file_path).to_numpy()\n",
    "        x, label = torch.tensor(data[start:end, 1:]), data[start, 0].astype(int)\n",
    "        \n",
    "        audio = self.get_audio(section, label)\n",
    "        \n",
    "        # Cut model inputs so that they match desirable sizes\n",
    "        E, S = config['in_seq_len'], config['sound_size']\n",
    "        x = x[:E] if x.size(0) >= E else nn.functional.pad(x, (0, E-x.size(0)), value=0)\n",
    "        audio = audio[:S] if audio.size(0) >= S else nn.functional.pad(audio, (0, S-audio.size(0)), value=0)\n",
    "        \n",
    "        x = x.t()  # (n_channels, in_seq_len)\n",
    "        \n",
    "        return x.float(), audio.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = EEGDataset(path=config['path'], audio_maps=config['audio_maps'])\n",
    "val_ds = EEGDataset(path=config['path'], audio_maps=config['audio_maps']).set_val_mode(True)\n",
    "\n",
    "# train_dl = DataLoader(train_ds, 8, shuffle=True, num_workers=2)\n",
    "# val_dl = DataLoader(val_ds, 8, shuffle=False, num_workers=2)\n",
    "train_dl = partial(DataLoader, dataset=train_ds, shuffle=True, num_workers=2)\n",
    "val_dl = partial(DataLoader, dataset=val_ds, num_workers=2)\n",
    "\n",
    "batch_size = 8\n",
    "print(f'Batch size: {batch_size}')\n",
    "print(f'{\"Train dataset len:\": <20} {len(train_ds)};\\t{\"Validation datset len:\": <25} {len(val_ds)};')\n",
    "print(f'{\"Num train batches:\": <20} {len(train_dl(batch_size=batch_size))};\\t{\"Num validation batches:\": <25} {len(val_dl(batch_size=batch_size))};')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerFeedForward(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
    "        '''\n",
    "        :param int d_model: Input dimension\n",
    "        :param int d_ff: Hidden dimension\n",
    "        :param float dropout: Dropout probability for linear layers\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(self.d_model)\n",
    "        self.linear_1 = nn.Linear(self.d_model, self.d_ff)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(self.d_ff, self.d_model)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        :param torch.Tensor x: (batch, time, d_model)\n",
    "        :return: (batch, time, d_model)\n",
    "        :rtype: torch.Tensor\n",
    "        '''\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.linear_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        ff_1_max = self.d_model ** -0.5\n",
    "        ff_2_max = self.d_ff ** -0.5\n",
    "        with torch.no_grad():\n",
    "            torch.nn.init.uniform_(self.linear_1.weight, -ff_1_max, ff_1_max)\n",
    "            torch.nn.init.uniform_(self.linear_2.weight, -ff_2_max, ff_2_max)\n",
    "            \n",
    "            torch.nn.init.uniform_(self.linear_1.bias, -ff_1_max, ff_1_max)\n",
    "            torch.nn.init.uniform_(self.linear_2.bias, -ff_2_max, ff_2_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerConvolution(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, kernel_size: int, dropout: float):\n",
    "        '''\n",
    "        :param int d_model: Input dimension\n",
    "        :param int kernel_size: Kernel size of Depthwise Convolution\n",
    "        :param float dropout: Dropout probability \n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(self.d_model)\n",
    "        self.pointwise_conv_1 = nn.Conv1d(self.d_model, 2 * self.d_model, kernel_size=1)\n",
    "        self.activation_1 = nn.GLU()\n",
    "        self.depthwise_conv = nn.Conv1d(self.d_model, self.d_model, kernel_size=self.kernel_size, groups=self.d_model, padding='same')\n",
    "        self.batch_norm = nn.BatchNorm1d(self.d_model)\n",
    "        self.activation_2 = nn.SiLU()\n",
    "        self.pointwise_conv_2 = nn.Conv1d(self.d_model, self.d_model, kernel_size=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, pad_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        '''\n",
    "        :param torch.Tensor x: (batch, time, d_model)\n",
    "        :param torch.Tensor pad_mask: (batch, time) takes True value for the positions corresponding to the padding\n",
    "        :return: (batch, time, d_model)\n",
    "        :rtype: torch.Tensor\n",
    "        '''\n",
    "        \n",
    "        x = self.layer_norm(x)\n",
    "        x = self.pointwise_conv_1(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.activation_1(x)\n",
    "\n",
    "        if pad_mask is not None:\n",
    "            x = x.masked_fill(pad_mask[..., None], 0.0)\n",
    "\n",
    "        x = self.depthwise_conv(x.permute(0, 2, 1))\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.activation_2(x)\n",
    "        x = self.pointwise_conv_2(x).permute(0, 2, 1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        pw_max = self.d_model ** -0.5\n",
    "        dw_max = self.kernel_size ** -0.5\n",
    "        with torch.no_grad():\n",
    "            torch.nn.init.uniform_(self.pointwise_conv_1.weight, -pw_max, pw_max)\n",
    "            torch.nn.init.uniform_(self.pointwise_conv_2.weight, -pw_max, pw_max)\n",
    "            torch.nn.init.uniform_(self.depthwise_conv.weight, -dw_max, dw_max)\n",
    "            \n",
    "            torch.nn.init.uniform_(self.pointwise_conv_1.bias, -pw_max, pw_max)\n",
    "            torch.nn.init.uniform_(self.pointwise_conv_2.bias, -pw_max, pw_max)\n",
    "            torch.nn.init.uniform_(self.depthwise_conv.bias, -dw_max, dw_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelPositionMultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, dropout: float):\n",
    "        '''\n",
    "        x:param int d_model: Input dimension\n",
    "        x:param int kernel_size: Number of MHSA heads\n",
    "        x:param float dropout: Dropout probability for attention probabilities\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % n_head == 0\n",
    "        \n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // n_head\n",
    "\n",
    "        # Linear transformations for queries, keys and values (W_{q}, W_{k}, W_{v})\n",
    "        self.linear_q = nn.Linear(d_model, d_model)\n",
    "        self.linear_k = nn.Linear(d_model, d_model)\n",
    "        self.linear_v = nn.Linear(d_model, d_model) \n",
    "        \n",
    "        # Linear transformation for positional encoding (W_{k,R})\n",
    "        self.linear_pos = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # These two learnable biases that are used in matrix c and matrix d\n",
    "        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n",
    "        self.pos_bias_u = nn.Parameter(torch.randn(d_model))\n",
    "        self.pos_bias_v = nn.Parameter(torch.randn(d_model))\n",
    "        \n",
    "        # Dropout layer for attention probabilities\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Linear transformation for model output\n",
    "        self.linear_out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    @staticmethod\n",
    "    def rel_shift(x: torch.Tensor) -> torch.Tensor:\n",
    "        '''Compute relative positional encoding.\n",
    "        :param torch.Tensor x: (batch, head, time_x, time_y)\n",
    "        :return: (batch, head, time_x, time_y)\n",
    "        :rtype: torch.Tensor\n",
    "        '''\n",
    "        batch, head, time_x, time_y = x.shape\n",
    "        \n",
    "        # Add a column of zeros on the left side of last dimension to perform the relative shifting\n",
    "        x = torch.cat((x.new_zeros(batch, head, time_x, 1), x), dim=-1)\n",
    "        # x = torch.nn.functional.pad(x, pad=(1, 0))\n",
    "\n",
    "        # Reshape matrix\n",
    "        # x = x.view(batch, head, -1, x.shape[-1] // 2)\n",
    "        x = x.view(batch, head, -1, time_x)\n",
    "\n",
    "        # Drop the first row and reshape matrix back\n",
    "        x = x[:, :, 1:, :].reshape(batch, head, time_x, -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_qkv(\n",
    "        self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Transform query, key and value.\n",
    "        :param torch.Tensor query: (batch, time_1, d_model)\n",
    "        :param torch.Tensor key:   (batch, time_2, d_model)\n",
    "        :param torch.Tensor value: (batch, time_2, d_model)\n",
    "\n",
    "        :return: (q, k, v):\n",
    "            torch.Tensor q: (batch, head, time_1, d_k)\n",
    "            torch.Tensor k: (batch, head, time_2, d_k)\n",
    "            torch.Tensor v: (batch, head, time_2, d_k)\n",
    "        :rtype: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "        \"\"\"\n",
    "        q = self.linear_q(query).view(query.shape[0], query.shape[1], self.n_head, self.d_k).permute(0, 2, 1, 3)\n",
    "        k = self.linear_k(key).view(key.shape[0], key.shape[1], self.n_head, self.d_k).permute(0, 2, 1, 3)\n",
    "        v = self.linear_v(value).view(value.shape[0], value.shape[1], self.n_head, self.d_k).permute(0, 2, 1, 3)\n",
    "\n",
    "        return q, k, v\n",
    "\n",
    "    def forward_attention(self, value: torch.Tensor, scores: torch.Tensor, mask: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Compute attention context vector.\n",
    "        :param torch.Tensor value:    (batch, head, time_2, d_k)\n",
    "        :param torch.Tensor scores:   (batch, head, time_1, time_2)\n",
    "        :param Optional[torch.Tensor] mask: (batch, time_1, time_2) attention mask between queries and keys\n",
    "            Takes True value for the positions corresponding to which tokens should NOT be attended to\n",
    "\n",
    "        :return: Transformed `value` of shape (batch, time_1, d_model) weighted by the attention scores\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            if mask.ndim == 2:\n",
    "                mask = mask.unsqueeze(0)\n",
    "            # Mask scores so that the won't be used in attention probabilities\n",
    "            scores = scores.masked_fill(mask.unsqueeze(1), -1e+30 if scores.dtype == torch.float32 else -1e+4)\n",
    "            \n",
    "            # Calculate attention probabilities\n",
    "            # Do not forget to mask probabilities\n",
    "            attn = torch.softmax(scores, dim=-1).masked_fill(mask.unsqueeze(1), 0)  # 1e-9 might cause issues when dealing with mixed precision\n",
    "        else:\n",
    "            # Calculate attention probabilities\n",
    "            attn = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Apply attention dropout\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # Reweigh value w.r.t. attention probabilities\n",
    "        out = attn @ value\n",
    "        \n",
    "        # Apply output linear transformation\n",
    "        return self.linear_out(out.permute(0, 2, 1, 3).reshape(scores.shape[0], scores.shape[2], -1))\n",
    "\n",
    "    def forward(\n",
    "        self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, \n",
    "        mask: Optional[torch.Tensor], pos_emb: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        '''Compute 'Scaled Dot Product Attention' with rel. positional encoding.\n",
    "        :param torch.Tensor query:          (batch, time_1, d_model)\n",
    "        :param torch.Tensor key:            (batch, time_2, d_model)\n",
    "        :param torch.Tensor value:          (batch, time_2, d_model)\n",
    "        :param Optional[torch.Tensor] mask: (batch, time_1, time_2) attention mask between queries and keys\n",
    "            Takes True value for the positions corresponding to which tokens should NOT be attended to\n",
    "        :param torch.Tensor pos_emb:        (batch, 2*time_2-1, d_model) relative positional embeddings \n",
    "            for all possible values of i - j\n",
    "\n",
    "        :return: Transformed `value` of shape (batch, time_1, d_model) weighted by the query-key attention\n",
    "        :rtype: torch.Tensor\n",
    "        '''\n",
    "        # Apply linear transformation for positional embeddings\n",
    "        pos_emb = self.linear_pos(pos_emb)\n",
    "\n",
    "        # Apply linear transformation for queries, keys and values\n",
    "        q, k, v = self.forward_qkv(query, key, value)\n",
    "\n",
    "        # Sum q with biases\n",
    "        # I.e (W_{q}E_{x_{i}} + u) and (W_{q}E_{x_{i}} + v)\n",
    "        a = q + self.pos_bias_u.view(1, -1, 1, self.d_k)\n",
    "        b = q + self.pos_bias_v.view(1, -1, 1, self.d_k)\n",
    "\n",
    "        # Compute attention scores\n",
    "        # First compute matrix a + matrix c\n",
    "        #   as described in https://arxiv.org/abs/1901.02860 Section 3.3\n",
    "        # (batch, head, time1, time2)\n",
    "        matrix_ac = a @ k.permute(0, 1, 3, 2)\n",
    "        \n",
    "        # Compute matrix b + matrix d\n",
    "        # (batch, head, time1, 2*time_2 - 1)\n",
    "        matrix_bd = b @ pos_emb.view(pos_emb.shape[0], pos_emb.shape[1], -1, self.d_k).permute(0, 2, 3, 1)\n",
    "\n",
    "        # Apply relative shift to b + d matrix\n",
    "        matrix_bd = self.rel_shift(matrix_bd)\n",
    "\n",
    "        # Drops extra elements in the matrix_bd to match the matrix_ac's size\n",
    "        matrix_bd = matrix_bd[:, :, :, :matrix_ac.shape[3]]\n",
    "\n",
    "        scores = (matrix_ac + matrix_bd) / math.sqrt(self.d_k)  # (batch, head, time_1, time_2)\n",
    "        \n",
    "        # Compute reweighed values using scores and mask\n",
    "        out = self.forward_attention(v, scores, mask)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        f_max = self.d_model ** -0.5\n",
    "        with torch.no_grad():\n",
    "            torch.nn.init.uniform_(self.linear_q.weight, -f_max, f_max)\n",
    "            torch.nn.init.uniform_(self.linear_k.weight, -f_max, f_max)\n",
    "            torch.nn.init.uniform_(self.linear_v.weight, -f_max, f_max)\n",
    "            torch.nn.init.uniform_(self.linear_out.weight, -f_max, f_max)\n",
    "            torch.nn.init.uniform_(self.linear_pos.weight, -f_max, f_max)\n",
    "\n",
    "            torch.nn.init.uniform_(self.linear_q.bias, -f_max, f_max)\n",
    "            torch.nn.init.uniform_(self.linear_k.bias, -f_max, f_max)\n",
    "            torch.nn.init.uniform_(self.linear_v.bias, -f_max, f_max)\n",
    "            torch.nn.init.uniform_(self.linear_out.bias, -f_max, f_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelPositionalEncoding(torch.nn.Module):\n",
    "    '''Relative positional encoding for TransformerXL's layers\n",
    "    See : Appendix B in https://arxiv.org/abs/1901.02860\n",
    "    '''\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000, xscale=False, dropout_emb=0.0):\n",
    "        '''Construct an RelPositionalEncoding object.\n",
    "        :param int d_model: Embedding dim\n",
    "        :param float dropout: Dropout probability for input embeddings\n",
    "        :param int max_len: Maximum input length\n",
    "        :param bool xscale: Whether to scale the input by sqrt(d_model)\n",
    "        :param float dropout_emb: Dropout probability for positional embeddings\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.xscale = xscale\n",
    "\n",
    "        # Create Dropout layer for input embeddings\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Create Dropout layer for positional embeddings\n",
    "        self.dropout_emb = nn.Dropout(dropout_emb)\n",
    "        \n",
    "        # Positions must be from positive numbers to negative\n",
    "        # Positive positions will be used for left positions and negative for right positions\n",
    "        positions = torch.arange(max_len-1, -max_len, -1)\n",
    "        self.create_pe(positions)\n",
    "\n",
    "    def create_pe(self, positions: torch.Tensor):\n",
    "        '''Compute positional encoding for given indices\n",
    "        :attr torch.Tensor pe: (1, pos_length, d_model)\n",
    "        :param torch.Tensor positions: (pos_length)\n",
    "        '''\n",
    "        pos_length = positions.size(0)\n",
    "\n",
    "        # Compute positional encoding\n",
    "        # as described in https://arxiv.org/abs/1706.03762 Section 3.5\n",
    "        pe = torch.zeros(pos_length, self.d_model, requires_grad=False)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * -(math.log(10000.0) / self.d_model))\n",
    "        pe[:, 0::2] = torch.sin(positions.unsqueeze(1) * div_term)\n",
    "        pe[:, 1::2] = torch.cos(positions.unsqueeze(1) * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # Save precomputed positional embeddings\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Compute positional encoding.\n",
    "        :param torch.Tensor x: Input of size(batch, time, feature_size)\n",
    "\n",
    "        :return Tuple[torch.Tensor, torch.Tensor]: (x, pos_emb):\n",
    "            torch.Tensor x: (batch, time, feature_size)\n",
    "            torch.Tensor pos_emb: (1, 2*time-1, feature_size)\n",
    "        '''\n",
    "\n",
    "        # Rescale input\n",
    "        if self.xscale:\n",
    "            x = x * (self.d_model ** 0.5)\n",
    "            \n",
    "        # Apply embeddings dropout\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Center_pos would be the index of position 0\n",
    "        # Negative positions would be used for right and positive for left tokens\n",
    "        # for input of length L, 2*L-1 positions are needed, positions from (L-1) to -(L-1)\n",
    "        time = x.size(1)\n",
    "        center_pos = self.pe.size(1) // 2\n",
    "        start_pos = center_pos - (time-1)\n",
    "\n",
    "        pos_emb = self.pe[:, start_pos:start_pos+2*time-1, :]\n",
    "\n",
    "        # Apply positional embeddings dropout\n",
    "        pos_emb = self.dropout_emb(pos_emb.to(x.device))\n",
    "\n",
    "        return x, pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerEncoderBlock(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, n_heads: int, kernel_size: int, dropout: float, dropout_att: float):\n",
    "        \"\"\"\n",
    "        :param int d_model: Input dimension\n",
    "        :param int d_ff: Hidden dimension for Feed Forward Module\n",
    "        :param int n_heads: Number of MHSA heads\n",
    "        :param int kernel_size: Kernel size of Depthwise Convolution\n",
    "        :param float dropout: Dropout probability for Feed Forward and Convolution Modules\n",
    "        :param float dropout_att: Dropout probability for attention probabilities\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_factor = 0.5\n",
    "\n",
    "        self.feed_forward_1 = ConformerFeedForward(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
    "\n",
    "        self.layer_norm_attn = nn.LayerNorm(d_model)\n",
    "        self.self_attn = RelPositionMultiHeadAttention(d_model=d_model, n_head=n_heads, dropout=dropout_att)\n",
    "        self.dropout_attn = nn.Dropout(dropout_att)\n",
    "\n",
    "        self.conv = ConformerConvolution(d_model=d_model, kernel_size=kernel_size, dropout=dropout)\n",
    "\n",
    "        self.feed_forward_2 = ConformerFeedForward(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
    "\n",
    "        self.layer_norm_out = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, pos_emb: torch.Tensor, \n",
    "        att_mask: Optional[torch.Tensor] = None, pad_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param torch.Tensor x: (batch, time, d_model) input features \n",
    "        :param torch.Tensor pos_emb: (batch, 2*time-1, d_model) relative positional embeddings \n",
    "            for all possible values of i - j\n",
    "        :param Optional[torch.Tensor] att_mask: (batch, time, time) attention mask between queries and keys\n",
    "            Takes True value for the positions corresponding to which tokens should NOT be attended to\n",
    "        :param Optional[torch.Tensor] pad_mask: (batch, time) padding mask\n",
    "            Takes True value for the positions corresponding to the padding\n",
    "        :return: (batch, time, d_model)\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        # Apply first Feed Forward Block with residual connection\n",
    "        x = x + self.feed_forward_1(x) * self.fc_factor\n",
    "        \n",
    "        # Apply MHSA Block with residual connection\n",
    "        mhsa = self.layer_norm_attn(x)\n",
    "        mhsa = self.self_attn(mhsa, mhsa, mhsa, att_mask, pos_emb)\n",
    "        mhsa = self.dropout_attn(mhsa)\n",
    "        x = x + mhsa\n",
    "\n",
    "        # Apply Convolutional Block with residual connection\n",
    "        x = x + self.conv(x, pad_mask)\n",
    "        \n",
    "        # Apply second Feed Forward Block with residual connection\n",
    "        x = x + self.feed_forward_2(x) * self.fc_factor\n",
    "        x = self.layer_norm_out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerEncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, n_layers: int, d_model: int, d_ff: int, n_heads: int,  kernel_size: int, \n",
    "        max_len: int, xscale: bool, dropout_emb: float, dropout: float, dropout_att: float\n",
    "    ):\n",
    "        '''\n",
    "        :param int n_layers: Number of Conformer Blocks\n",
    "        :param int d_model: Input dimension\n",
    "        :param int d_ff: Hidden dimension for Feed Forward Module\n",
    "        :param int n_heads: Number of MHSA heads\n",
    "        :param int kernel_size: Kernel size of Depthwise Convolution\n",
    "        :param int max_len: Maximum input length\n",
    "        :param bool xscale: Whether to scale the input by sqrt(d_model)\n",
    "        :param float dropout_emb: Dropout probability for positional embeddings\n",
    "        :param float dropout: Dropout probability for Feed Forward and Convolution Modules\n",
    "        :param float dropout_att: Dropout probability for attention probabilities\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoding = RelPositionalEncoding(d_model=d_model, dropout=dropout, max_len=max_len, xscale=xscale, dropout_emb=dropout_emb)\n",
    "        self.layers = nn.ModuleList([ConformerEncoderBlock(d_model=d_model, d_ff=d_ff, n_heads=n_heads, kernel_size=kernel_size,\n",
    "                                                           dropout=dropout, dropout_att=dropout_att) for _ in range(n_layers)])\n",
    "        \n",
    "    @staticmethod\n",
    "    def _create_masks(max_length, length: torch.Tensor) -> Tuple[torch.BoolTensor, torch.BoolTensor]:\n",
    "        \n",
    "        '''\n",
    "        :param int max_length: Maximum size of time dimension in the batch\n",
    "        :param torch.Tensor length: (batch) length of sequences in batch\n",
    "        :return: (pad_mask, att_mask):\n",
    "            torch.BoolTensor pad_mask: (batch, max_length)\n",
    "                Takes True value for the positions corresponding to the padding\n",
    "            torch.BoolTensor att_mask: (batch, max_length, max_length)\n",
    "                Takes True value for the positions corresponding to which tokens should NOT be attended to\n",
    "            Where max_length is a size of time dimension of the batch\n",
    "        :rtype: Tuple[torch.BoolTensor, torch.BoolTensor]\n",
    "        '''\n",
    "        # pad_mask is the masking to be used to ignore paddings\n",
    "        pad_mask = torch.zeros(length.size(0), max_length, dtype=torch.bool, device=length.device)\n",
    "        for i, l in enumerate(length):\n",
    "            pad_mask[i, l:] = True\n",
    "\n",
    "        # att_mask is the masking to be used in self-attention to ignore paddings\n",
    "        att_mask = torch.ones(length.size(0), max_length, max_length, dtype=torch.bool, device=length.device)\n",
    "        for i, l in enumerate(length):\n",
    "            att_mask[i, :l, :l] = False\n",
    "\n",
    "        return pad_mask, att_mask\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        :param torch.Tensor x: (batch, time, d_model) input features\n",
    "        :param torch.Tensor length: (batch) length of sequences in batch\n",
    "        :return:\n",
    "        :rtype: torch.Tensor\n",
    "        '''\n",
    "        # We do not use masks since all inputs are of the same size\n",
    "        # pad_mask, att_mask = self._create_masks(x.size(1), length)\n",
    "        \n",
    "        # Encode input features\n",
    "        x, enc = self.encoding(x)\n",
    "        \n",
    "        # Apply Conformer Blocks\n",
    "        for conf_block in self.layers:\n",
    "            x = conf_block(x, enc)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerDecoderBlock(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, n_heads: int, dropout: float, dropout_att: float):\n",
    "        \"\"\"\n",
    "        :param int d_model: Input dimension\n",
    "        :param int d_ff: Hidden dimension for Feed Forward Module\n",
    "        :param int n_heads: Number of MHSA heads\n",
    "        :param float dropout: Dropout probability for Feed Forward and Convolution Modules\n",
    "        :param float dropout_att: Dropout probability for attention probabilities\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_factor = 0.5\n",
    "\n",
    "        self.feed_forward_1 = ConformerFeedForward(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
    "\n",
    "        self.layer_norm_attn1 = nn.LayerNorm(d_model)\n",
    "        self.self_attn1 = RelPositionMultiHeadAttention(d_model=d_model, n_head=n_heads, dropout=dropout_att)\n",
    "        self.dropout_attn1 = nn.Dropout(dropout_att)\n",
    "\n",
    "        self.layer_norm_attn2 = nn.LayerNorm(d_model)\n",
    "        self.self_attn2 = RelPositionMultiHeadAttention(d_model=d_model, n_head=n_heads, dropout=dropout_att)\n",
    "        self.dropout_attn2 = nn.Dropout(dropout_att)\n",
    "\n",
    "        self.feed_forward_2 = ConformerFeedForward(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
    "\n",
    "        self.layer_norm_out = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(\n",
    "        self, memory: torch.Tensor, target_input: torch.Tensor, pos_emb: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param torch.Tensor memory: (batch, time_x, d_model) encoder output\n",
    "        :param torch.Tensor target_input: (batch, time_y, d_model) input features \n",
    "        :param torch.Tensor pos_emb: (batch, 2*time-1, d_model) relative positional embeddings \n",
    "            for all possible values of i - j\n",
    "        :param Optional[torch.Tensor] lookahead_mask: (batch, time, time) attention mask between queries and keys\n",
    "            Takes True value for the positions corresponding to which tokens should NOT be attended to\n",
    "        :return: (batch, time_y, d_model)\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        # Apply first Feed Forward Block with residual connection\n",
    "        x = target_input + self.feed_forward_1(target_input) * self.fc_factor\n",
    "        \n",
    "        # Apply MHSA Block with residual connection\n",
    "        lookahead_mask = nn.Transformer.generate_square_subsequent_mask(sz=target_input.size(1),\n",
    "                                                                        device=target_input.device)\n",
    "        mhsa = self.layer_norm_attn1(x)\n",
    "        mhsa = self.self_attn1(mhsa, mhsa, mhsa, mask=lookahead_mask.type(torch.bool), pos_emb=pos_emb)\n",
    "        mhsa = self.dropout_attn1(mhsa)\n",
    "        x = x + mhsa\n",
    "\n",
    "        # Apply second MHSA Block with residual connection\n",
    "        mhsa = self.layer_norm_attn2(x)\n",
    "        mhsa = self.self_attn2(mhsa, memory, memory, mask=None, pos_emb=pos_emb)\n",
    "        mhsa = self.dropout_attn2(mhsa)\n",
    "        x = x + mhsa\n",
    "\n",
    "        # Apply second Feed Forward Block with residual connection\n",
    "        x = x + self.feed_forward_2(x) * self.fc_factor\n",
    "        x = self.layer_norm_out(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerDecoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, n_layers: int, d_model: int, d_ff: int, n_heads: int, max_len: int,\n",
    "        xscale: bool, dropout_emb: float, dropout: float, dropout_att: float\n",
    "    ):\n",
    "        '''\n",
    "        :param int n_layers: Number of Conformer Blocks\n",
    "        :param int d_model: Input dimension\n",
    "        :param int d_ff: Hidden dimension for Feed Forward Module\n",
    "        :param int n_heads: Number of MHSA heads\n",
    "        :param int max_len: Maximum input length\n",
    "        :param bool xscale: Whether to scale the input by sqrt(d_model)\n",
    "        :param float dropout_emb: Dropout probability for positional embeddings\n",
    "        :param float dropout: Dropout probability for Feed Forward and Convolution Modules\n",
    "        :param float dropout_att: Dropout probability for attention probabilities\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoding = RelPositionalEncoding(d_model=d_model, dropout=dropout, max_len=max_len, xscale=xscale, dropout_emb=dropout_emb)\n",
    "        self.layers = nn.ModuleList([ConformerDecoderBlock(d_model=d_model, d_ff=d_ff, n_heads=n_heads,\n",
    "                                                           dropout=dropout, dropout_att=dropout_att) for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, memory: torch.Tensor, target_input: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        :param torch.Tensor memory: (batch, time_x, d_model) encoder output\n",
    "        :param torch.Tensor target_input: (batch, time_y, d_model) input features\n",
    "        :return:\n",
    "        :rtype: torch.Tensor\n",
    "        '''\n",
    "        # Encode input features\n",
    "        x, enc = self.encoding(target_input)\n",
    "        \n",
    "        # Apply Conformer Blocks\n",
    "        for conf_block in self.layers:\n",
    "            x = conf_block(memory, x, enc)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E2SConformer(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 n_fft: int, hop_size: int, d_model: int,\n",
    "                 eeg_sr: int, audio_sr: int, n_channels: int,\n",
    "                 \n",
    "                 # Conformer Encoder\n",
    "                 num_encoder_layers: int, d_ff: int,\n",
    "                 n_heads: int, kernel_size: int, \n",
    "                 in_seq_len: int, xscale: bool,\n",
    "                 dropout_emb: float, dropout: float, dropout_att: float,\n",
    "\n",
    "                 # Conformer Decoder\n",
    "                 num_decoder_layers: int, out_seq_len: int,\n",
    "\n",
    "                 audio_paths: List[str]):\n",
    "        \"\"\"\n",
    "        :param dict config: dictionart with all model parameters\n",
    "        :param List[str] audio_paths: list of audio file paths to fit PCA on\n",
    "        :param torch.tensor example_input: input to compute wavelet filters on. Should have shape (n_channels, in_seq_len)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_size = hop_size\n",
    "        self.d_model = d_model\n",
    "        self.eeg_sr = eeg_sr\n",
    "        self.audio_sr = audio_sr\n",
    "        \n",
    "        # self.conv_downsampling = torch.nn.Conv1d(n_channels, 1, kernel_size=1) # (N, c_in, L) -> (N, 1, L)\n",
    "        # self.ln = nn.LayerNorm(n_wvt_bins)\n",
    "        # self.ffn = nn.Linear(n_wvt_bins, d_model)\n",
    "        self.ln = nn.LayerNorm(n_channels)\n",
    "        self.pointwise = nn.Conv1d(n_channels, d_model, kernel_size=1)\n",
    "        self.encoder = ConformerEncoder(\n",
    "            n_layers=num_encoder_layers,\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            n_heads=n_heads,\n",
    "            kernel_size=kernel_size,\n",
    "            max_len=in_seq_len+2,\n",
    "            xscale=xscale,\n",
    "            dropout_emb=dropout_emb,\n",
    "            dropout=dropout,\n",
    "            dropout_att=dropout_att\n",
    "        )\n",
    "        self.decoder = ConformerDecoder(\n",
    "            n_layers=num_decoder_layers,\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            n_heads=n_heads,\n",
    "            max_len=out_seq_len+2,\n",
    "            xscale=xscale,\n",
    "            dropout_emb=dropout_emb,\n",
    "            dropout=dropout,\n",
    "            dropout_att=dropout_att\n",
    "        )\n",
    "\n",
    "        self.compute_pca_components(audio_paths)\n",
    "        \n",
    "        # Specials\n",
    "        self.src_sos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n",
    "        self.src_eos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n",
    "        self.tgt_sos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n",
    "        self.tgt_eos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        pw_max = self.d_model ** -0.5\n",
    "        with torch.no_grad():\n",
    "            torch.nn.init.uniform_(self.src_sos, -pw_max, pw_max)\n",
    "            torch.nn.init.uniform_(self.src_eos, -pw_max, pw_max)\n",
    "            torch.nn.init.uniform_(self.tgt_sos, -pw_max, pw_max)\n",
    "            torch.nn.init.uniform_(self.tgt_eos, -pw_max, pw_max)\n",
    "        \n",
    "    def compute_pca_components(self, audio_paths):\n",
    "        \"\"\"\n",
    "        :param List[str] audio_paths: list of audio file paths to fit PCA on\n",
    "        \"\"\"\n",
    "        audios_srs = [torchaudio.load(path) for path in audio_paths]\n",
    "        all_audios = []\n",
    "        for audio, sr in audios_srs:\n",
    "            if sr != self.audio_sr:\n",
    "                audio = torchaudio.functional.resample(waveform=audio, orig_freq=sr, new_freq=self.audio_sr)[0]\n",
    "            all_audios.append(audio)\n",
    "        \n",
    "        all_audios = torch.cat(all_audios)\n",
    "        all_audios = torch.stft(all_audios, n_fft=self.n_fft, hop_length=self.hop_size, return_complex=True)  # (n_freq_bins, n_frames)\n",
    "        all_audios = torch.abs(all_audios).t().numpy()\n",
    "        \n",
    "        pca = PCA(n_components=self.d_model)\n",
    "        pca.fit(all_audios)\n",
    "        \n",
    "        components = torch.tensor(pca.components_)  # (d_model, n_freq_bins)\n",
    "        mean = torch.tensor(pca.mean_)  # (n_freq_bins)\n",
    "        \n",
    "        self.register_buffer('components', components)\n",
    "        self.register_buffer('mean', mean)\n",
    "        \n",
    "    def prepare_src(self, x):\n",
    "        \"\"\"\n",
    "        :param torch.tensor x: input of shape (batch_size, n_channels, in_seq_len)\n",
    "        :rtype torch.tensor\n",
    "        :return out of shape (batch_size, in_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        \n",
    "        # LayerNorm & Feed Forward\n",
    "        out = x.permute(0, 2, 1)  # (batch_size, in_seq_len, n_channels)\n",
    "        out = self.ln(out)  # (batch_size, in_seq_len, n_channels)\n",
    "        out = out.permute(0, 2, 1)  # (batch_size, n_channels, in_seq_len)\n",
    "        out = self.pointwise(out)  # (batch_size, d_model, in_seq_len)\n",
    "        \n",
    "        return out.permute(0, 2, 1)\n",
    "    \n",
    "    def prepare_tgt(self, x):  # Add some audio normalization???\n",
    "        \"\"\"\n",
    "        :param torch.tensor x: input of shape (batch_size, audio_len)\n",
    "        :rtype torch.tensor\n",
    "        :return out of shape (batch_size, out_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # n_freq_bins = self.n_fft // 2 + 1\n",
    "        # out_seq_len = self.n_fft // self.hop_size + 1\n",
    "        \n",
    "        # STFT\n",
    "        out = torch.stft(x, n_fft=self.n_fft, hop_length=self.hop_size, return_complex=True)  # (batch_size, n_freq_bins, out_seq_len)\n",
    "        out = torch.abs(out.permute(0, 2, 1))  # (batch_size, out_seq_len, n_freq_bins)\n",
    "        \n",
    "        # PCA\n",
    "        out = out - self.mean\n",
    "        out = out @ self.components.t()  # (batch_size, out_seq_len, d_model)\n",
    "        return out\n",
    "        \n",
    "    def forward(self, eeg, audio):\n",
    "        \"\"\"\n",
    "        :param torch.tensor eeg: input of shape (batch_size, n_channels, in_seq_len)\n",
    "        :rtype torch.tensor\n",
    "        :return out of shape (batch_size, out_seq_len, n_freq_bins)\n",
    "        \"\"\"\n",
    "        batch_size = eeg.size(0)\n",
    "        src = self.prepare_src(eeg)  # (batch_size, in_seq_len, d_model)\n",
    "        tgt = self.prepare_tgt(audio)  # (batch_size, out_seq_len, d_model)\n",
    "\n",
    "        # Add <sos> and <eos>\n",
    "        src = torch.cat((self.src_sos.repeat(batch_size, 1, 1), src, self.src_eos.repeat(batch_size, 1, 1)),\n",
    "                        dim=1)  # (batch_size, 1 + in_seq_len + 1, d_model)\n",
    "        tgt = torch.cat((self.tgt_sos.repeat(batch_size, 1, 1), tgt, self.tgt_eos.repeat(batch_size, 1, 1)),\n",
    "                        dim=1)  # (batch_size, 1 + out_seq_len + 1, d_model)\n",
    "        \n",
    "        # tgt_input <sos>, token_1, token_2, ..., token_n\n",
    "        tgt_input = tgt[:, :-1, :]  # (batch_size, 1 + out_seq_len, d_model)\n",
    "\n",
    "        # tgt_output token_1, token_2, ..., token_n, <eos>\n",
    "        tgt_output = tgt[:, 1:, :]  # (batch_size, out_seq_len + 1, d_model)\n",
    "        \n",
    "        memory = self.encoder(src)\n",
    "        out = self.decoder(memory, tgt_input)\n",
    "\n",
    "        return out, tgt_output\n",
    "\n",
    "    def predict(self, eeg, out_seq_len):\n",
    "        \"\"\"\n",
    "        :param torch.tensor eeg: input of shape ([batch_size], n_channels, in_seq_len)\n",
    "        :param int out_seq_len: output sequence length\n",
    "        :rtype torch.tensor\n",
    "        :return predicted_encoding of shape (batch_size, out_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        device = eeg.device\n",
    "        if eeg.ndim == 2:\n",
    "            eeg.unsqueeze_(0)\n",
    "\n",
    "        self.eval().to(device)\n",
    "        with torch.no_grad():\n",
    "            src = self.prepare_src(eeg)  # (batch_size, in_seq_len, d_model)\n",
    "            src = torch.cat((self.src_sos.repeat(src.size(0), 1, 1), src, self.src_eos.repeat(src.size(0), 1, 1)),\n",
    "                            dim=1)  # (batch_size, 1 + in_seq_len + 1, d_model)\n",
    "            memory = self.encoder(src)  # (batch_size, in_seq_len, d_model)\n",
    "\n",
    "            pred = self.tgt_sos.repeat(eeg.size(0), 1, 1).to(device)  # (batch_size, 1, d_model)\n",
    "            for _ in range(out_seq_len):\n",
    "                # (batch_size, d_model)\n",
    "                new_window = self.decoder(memory, pred)[:, -1, :]\n",
    "                pred = torch.cat((pred, new_window.unsqueeze(1)), dim=1)\n",
    "\n",
    "        self.train()\n",
    "        return pred[:, 1:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noam Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NoamAnnealing(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(\n",
    "        self, optimizer: torch.optim.Optimizer, *, \n",
    "        d_model: int, warmup_steps: int, min_lr: float = 0.0, last_epoch: int = -1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param torch.optim.Optimizer optimizer:\n",
    "        :param int d_model: Model input dimension\n",
    "        :param int warmup_steps:\n",
    "        :param float min_lr: Lower bound for learning rate after warmup\n",
    "        :param int last_epoch:\n",
    "        \"\"\"\n",
    "        assert warmup_steps\n",
    "        \n",
    "        # It is necessary to assign all attributes *before* __init__,\n",
    "        # as class is wrapped by an inner class.\n",
    "        self.min_lr = min_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.normalization = d_model ** (-0.5)\n",
    "\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\n",
    "                \"To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\", UserWarning\n",
    "            )\n",
    "\n",
    "        step = max(1, self.last_epoch)\n",
    "        new_lrs = [\n",
    "            self._noam_annealing(initial_lr=initial_lr, step=step) \n",
    "            for initial_lr in self.base_lrs\n",
    "        ]\n",
    "        return new_lrs\n",
    "\n",
    "    def _noam_annealing(self, initial_lr: float, step: int) -> float:\n",
    "        \"\"\"Compute noam annealing learning rate \n",
    "            as described in https://arxiv.org/abs/1706.03762 Section 5.3.\n",
    "            After warmup_steps learning rate should be always greater than min_lr\n",
    "\n",
    "        :param float initial_lr: Additional multiplicative factor for learning rate\n",
    "        :param int step: Current optimization step\n",
    "        :return: Learning rate at given step\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        lrate = self.normalization * min(step ** (-0.5), step * self.warmup_steps ** (-1.5)) * initial_lr\n",
    "        if step > self.warmup_steps:\n",
    "            lrate = max(self.min_lr, lrate)\n",
    "        \n",
    "        return lrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = E2SConformer(\n",
    "    n_fft=2048,\n",
    "    hop_size=512,\n",
    "    d_model=512,\n",
    "    eeg_sr=1006,\n",
    "    audio_sr=44100,\n",
    "    n_channels=63,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    d_ff=1024,\n",
    "    n_heads=4,\n",
    "    kernel_size=31,\n",
    "    in_seq_len=1145,\n",
    "    xscale=True,\n",
    "    dropout_emb=0.1,\n",
    "    dropout=0.1,\n",
    "    dropout_att=0.1,\n",
    "    out_seq_len=99,\n",
    "    audio_paths=[\n",
    "        '/kaggle/input/internal-speech-recognition/Vartanov/audios/syllables/Bu.wav',\n",
    "        '/kaggle/input/internal-speech-recognition/Vartanov/audios/syllables/Fa.wav', \n",
    "        '/kaggle/input/internal-speech-recognition/Vartanov/audios/syllables/Mu.wav', \n",
    "        '/kaggle/input/internal-speech-recognition/Vartanov/audios/syllables/Ga.wav', \n",
    "        '/kaggle/input/internal-speech-recognition/Vartanov/audios/syllables/Ba.wav', \n",
    "        '/kaggle/input/internal-speech-recognition/Vartanov/audios/syllables/Ra.wav', \n",
    "        '/kaggle/input/internal-speech-recognition/Vartanov/audios/syllables/Ma.wav', \n",
    "        '/kaggle/input/internal-speech-recognition/Vartanov/audios/syllables/Ru.wav', \n",
    "        '/kaggle/input/internal-speech-recognition/Vartanov/audios/syllables/Gu.wav', \n",
    "        '/kaggle/input/internal-speech-recognition/Vartanov/audios/syllables/Fu.wav', \n",
    "        '/kaggle/input/internal-speech-recognition/Vartanov/audios/words/St2.wav', \n",
    "        '/kaggle/input/internal-speech-recognition/Vartanov/audios/words/St5.wav', \n",
    "        '/kaggle/input/internal-speech-recognition/Vartanov/audios/words/St4.wav', \n",
    "        '/kaggle/input/internal-speech-recognition/Vartanov/audios/words/St3.wav', \n",
    "        '/kaggle/input/internal-speech-recognition/Vartanov/audios/words/St1.wav'\n",
    "    ]\n",
    ").to(device)\n",
    "\n",
    "eeg, audio = train_ds[15296]\n",
    "eeg, audio = eeg.unsqueeze(0).to(device), audio.unsqueeze(0).to(device)\n",
    "\n",
    "out, tgt = model(eeg, audio)\n",
    "\n",
    "with torch.no_grad():\n",
    "    restored = tgt[:, :-1, :].squeeze()  # (out_seq_len, d_model)\n",
    "    restored = restored @ model.components  # (out_seq_len, n_freq_bins)\n",
    "    restored = (restored + model.mean).t().cpu().numpy()  # (n_freq_bins, out_seq_len)\n",
    "\n",
    "restored = restore(restored)\n",
    "Audio(restored, rate=train_ds.sr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
