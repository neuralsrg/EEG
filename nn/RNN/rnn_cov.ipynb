{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X_path, y_path, transforms=None,\n",
    "                 n_windows=115, window_size=36) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.X = np.genfromtxt(X_path, delimiter=',').reshape(-1, n_windows, window_size)\n",
    "        self.y = np.genfromtxt(y_path, delimiter=',')\n",
    "        assert self.X.shape[0] == self.y.shape[0]\n",
    "\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, index) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if self.transforms is not None:\n",
    "            return self.transforms(self.X[index]), self.y[index]\n",
    "        else:\n",
    "            return self.X[index], self.y[index]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jittering(object):\n",
    "\n",
    "    def __init__(self, std) -> None:\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, obj) -> object:\n",
    "        noise = np.random.normal(loc=0, scale=self.std, size=obj.shape)\n",
    "        return obj + noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "X_train_path = os.path.join(cwd, 'data', 'pack1', 'converted', 'listen-noise', 'A_train.csv')\n",
    "y_train_path = os.path.join(cwd, 'data', 'pack1', 'converted', 'listen-noise', 'y_train.csv')\n",
    "\n",
    "X_val_path = os.path.join(cwd, 'data', 'pack1', 'converted', 'listen-noise', 'A_val.csv')\n",
    "y_val_path = os.path.join(cwd, 'data', 'pack1', 'converted', 'listen-noise', 'y_val.csv')\n",
    "\n",
    "X_test_path = os.path.join(cwd, 'data', 'pack1', 'converted', 'listen-noise', 'A_test.csv')\n",
    "y_test_path = os.path.join(cwd, 'data', 'pack1', 'converted', 'listen-noise', 'y_test.csv')\n",
    "\n",
    "# creating datasets\n",
    "tr = transforms.Compose([Jittering(0.1)])\n",
    "\n",
    "train_dataset = CovDataset(X_train_path, y_train_path, transforms=tr)\n",
    "val_dataset = CovDataset(X_val_path, y_val_path, transforms=None)\n",
    "test_dataset = CovDataset(X_test_path, y_test_path, transforms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, data_loader, loss_function=nn.BCELoss(reduction='sum')) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    length = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "\n",
    "            X, y = batch[0].float().to(device), batch[1].float().to(device)\n",
    "            output = model(X).squeeze()\n",
    "            \n",
    "            loss += loss_function(output, y).item() \n",
    "            pred = (output > 0.5).float()\n",
    "            correct += (pred == y).sum().item()\n",
    "            length += batch[1].shape[0]\n",
    "\n",
    "    return loss / length, correct / length\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, val_loader, test_loader, n_epoch, optimizer, scheduler, \n",
    "          writer, loss_function=nn.BCELoss(reduction='sum'), max_norm=None, track_gradient=False,\n",
    "          initial_epoch=1, global_step=0):\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(initial_epoch, n_epoch + initial_epoch):\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader:\n",
    "\n",
    "            global_step += 1\n",
    "            X, y = batch[0].float().to(device), batch[1].float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(X).squeeze()\n",
    "            loss = loss_function(output, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            if max_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "                \n",
    "            if track_gradient:\n",
    "                norm = 0\n",
    "                for p in model.parameters():\n",
    "                    norm += (p.grad.data.detach().norm(2) ** 2).item()\n",
    "                norm = norm ** 0.5\n",
    "                writer.add_scalar('gradient_norm', norm, global_step)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "        train_loss, train_acc = evaluate(model, device, train_loader)\n",
    "        val_loss, val_acc = evaluate(model, device, val_loader)\n",
    "        test_loss, test_acc = evaluate(model, device, test_loader)\n",
    "\n",
    "        print(f'Epoch {epoch}|\\t train_loss: {train_loss:.3f};\\t train_acc: {train_acc:.3f}')\n",
    "\n",
    "        writer.add_scalars('loss', {\n",
    "            'train': train_loss,\n",
    "            'val': val_loss,\n",
    "            'test': test_loss\n",
    "        }, epoch)\n",
    "        writer.add_scalars('accuracy', {\n",
    "            'train': train_acc,\n",
    "            'val': val_acc,\n",
    "            'test': test_acc\n",
    "        }, epoch)\n",
    "\n",
    "        writer.flush()\n",
    "\n",
    "    return n_epoch + initial_epoch, global_step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, \n",
    "                 activation=nn.ReLU(),  dropout_p=0.2) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=3, batch_first=True, dropout=dropout_p)\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size, 256), nn.Dropout(dropout_p), activation,\n",
    "            nn.Linear(256, 128), nn.Dropout(dropout_p), activation,\n",
    "            nn.Linear(128, 64), nn.Dropout(dropout_p), activation,\n",
    "            nn.Linear(64, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        output, _, = self.rnn(X)\n",
    "        output = output[:, -1, :]  # (batch, hidden_size)\n",
    "        \n",
    "        output = self.clf(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('.', 'runs', '3-Layer-LSTM-fixDropout')\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = LSTM_Net(input_size=36, hidden_size=256)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1|\t train_loss: 0.693;\t train_acc: 0.525\n",
      "Epoch 2|\t train_loss: 0.691;\t train_acc: 0.569\n",
      "Epoch 3|\t train_loss: 0.684;\t train_acc: 0.589\n",
      "Epoch 4|\t train_loss: 0.664;\t train_acc: 0.612\n",
      "Epoch 5|\t train_loss: 0.642;\t train_acc: 0.619\n",
      "Epoch 6|\t train_loss: 0.611;\t train_acc: 0.646\n",
      "Epoch 7|\t train_loss: 0.587;\t train_acc: 0.682\n",
      "Epoch 8|\t train_loss: 0.564;\t train_acc: 0.686\n",
      "Epoch 9|\t train_loss: 0.539;\t train_acc: 0.707\n",
      "Epoch 10|\t train_loss: 0.517;\t train_acc: 0.717\n",
      "Epoch 11|\t train_loss: 0.501;\t train_acc: 0.731\n",
      "Epoch 12|\t train_loss: 0.486;\t train_acc: 0.742\n",
      "Epoch 13|\t train_loss: 0.476;\t train_acc: 0.750\n",
      "Epoch 14|\t train_loss: 0.470;\t train_acc: 0.751\n",
      "Epoch 15|\t train_loss: 0.462;\t train_acc: 0.751\n",
      "Epoch 16|\t train_loss: 0.457;\t train_acc: 0.761\n",
      "Epoch 17|\t train_loss: 0.453;\t train_acc: 0.757\n",
      "Epoch 18|\t train_loss: 0.449;\t train_acc: 0.762\n",
      "Epoch 19|\t train_loss: 0.446;\t train_acc: 0.764\n",
      "Epoch 20|\t train_loss: 0.445;\t train_acc: 0.763\n",
      "Epoch 21|\t train_loss: 0.442;\t train_acc: 0.766\n",
      "Epoch 22|\t train_loss: 0.442;\t train_acc: 0.763\n",
      "Epoch 23|\t train_loss: 0.441;\t train_acc: 0.768\n",
      "Epoch 24|\t train_loss: 0.440;\t train_acc: 0.766\n",
      "Epoch 25|\t train_loss: 0.439;\t train_acc: 0.768\n",
      "Epoch 26|\t train_loss: 0.440;\t train_acc: 0.767\n",
      "Epoch 27|\t train_loss: 0.439;\t train_acc: 0.770\n",
      "Epoch 28|\t train_loss: 0.440;\t train_acc: 0.764\n",
      "Epoch 29|\t train_loss: 0.439;\t train_acc: 0.767\n",
      "Epoch 30|\t train_loss: 0.437;\t train_acc: 0.768\n",
      "Epoch 31|\t train_loss: 0.437;\t train_acc: 0.770\n",
      "Epoch 32|\t train_loss: 0.438;\t train_acc: 0.766\n",
      "Epoch 33|\t train_loss: 0.437;\t train_acc: 0.769\n",
      "Epoch 34|\t train_loss: 0.437;\t train_acc: 0.767\n",
      "Epoch 35|\t train_loss: 0.437;\t train_acc: 0.768\n",
      "Epoch 36|\t train_loss: 0.436;\t train_acc: 0.766\n",
      "Epoch 37|\t train_loss: 0.437;\t train_acc: 0.769\n",
      "Epoch 38|\t train_loss: 0.436;\t train_acc: 0.768\n",
      "Epoch 39|\t train_loss: 0.438;\t train_acc: 0.769\n",
      "Epoch 40|\t train_loss: 0.437;\t train_acc: 0.768\n",
      "Epoch 41|\t train_loss: 0.437;\t train_acc: 0.768\n",
      "Epoch 42|\t train_loss: 0.436;\t train_acc: 0.765\n",
      "Epoch 43|\t train_loss: 0.439;\t train_acc: 0.766\n",
      "Epoch 44|\t train_loss: 0.437;\t train_acc: 0.770\n",
      "Epoch 45|\t train_loss: 0.437;\t train_acc: 0.770\n",
      "Epoch 46|\t train_loss: 0.435;\t train_acc: 0.767\n",
      "Epoch 47|\t train_loss: 0.436;\t train_acc: 0.769\n",
      "Epoch 48|\t train_loss: 0.437;\t train_acc: 0.768\n",
      "Epoch 49|\t train_loss: 0.437;\t train_acc: 0.765\n",
      "Epoch 50|\t train_loss: 0.437;\t train_acc: 0.767\n"
     ]
    }
   ],
   "source": [
    "last_epoch, last_step = train(\n",
    "    model, device, train_loader, val_loader, test_loader, 50, optimizer, scheduler, writer, track_gradient=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
