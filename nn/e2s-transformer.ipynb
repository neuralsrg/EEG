{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/neuralsrg/e2s-transformer?scriptVersionId=139176482\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# EEG to Speech Transformer","metadata":{}},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"execution":{"iopub.status.busy":"2023-08-07T11:51:17.44939Z","iopub.execute_input":"2023-08-07T11:51:17.451303Z","iopub.status.idle":"2023-08-07T11:51:17.500509Z","shell.execute_reply.started":"2023-08-07T11:51:17.451271Z","shell.execute_reply":"2023-08-07T11:51:17.499591Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"! rm -rf /kaggle/working/PyTorchWavelets\n! git clone https://github.com/neuralsrg/PyTorchWavelets.git\n# ! git clone -b develop --single-branch https://github.com/neuralsrg/PyTorchWavelets.git","metadata":{"execution":{"iopub.status.busy":"2023-08-07T11:51:17.502351Z","iopub.execute_input":"2023-08-07T11:51:17.502769Z","iopub.status.idle":"2023-08-07T11:51:21.147031Z","shell.execute_reply.started":"2023-08-07T11:51:17.502728Z","shell.execute_reply":"2023-08-07T11:51:21.145649Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'PyTorchWavelets'...\nremote: Enumerating objects: 108, done.\u001b[K\nremote: Counting objects: 100% (8/8), done.\u001b[K\nremote: Compressing objects: 100% (7/7), done.\u001b[K\nremote: Total 108 (delta 1), reused 5 (delta 1), pack-reused 100\u001b[K\nReceiving objects: 100% (108/108), 1.08 MiB | 3.53 MiB/s, done.\nResolving deltas: 100% (48/48), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport sys\nimport random\nsys.path.append('/kaggle/working/PyTorchWavelets/')\nimport subprocess\n\nimport math\nimport pickle\nfrom glob import glob\nfrom functools import partial\nfrom tqdm.notebook import tqdm, trange\nfrom typing import List, Tuple, Optional\nfrom IPython.display import Audio, FileLink, display, clear_output\n\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchaudio\nimport torch.nn as nn\ntorch.manual_seed(1337)\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom wavelets_pytorch.transform import WaveletTransformTorch\nfrom wavelets_pytorch.wavelets import Morlet\n\nfrom sklearn.decomposition import PCA\n# clear_output()","metadata":{"execution":{"iopub.status.busy":"2023-08-07T11:51:21.153254Z","iopub.execute_input":"2023-08-07T11:51:21.157901Z","iopub.status.idle":"2023-08-07T11:51:26.331876Z","shell.execute_reply.started":"2023-08-07T11:51:21.157848Z","shell.execute_reply":"2023-08-07T11:51:26.330661Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"base = '/kaggle/input/internal-speech-recognition/Vartanov/audios'\n\nA = \"A.wav\"\nB = \"B.wav\"\nF = \"F.wav\"\nG = \"G.wav\"\nM = \"M.wav\"\nR = \"R.wav\"\nU = \"U.wav\"\n\nBa = \"Ba.wav\"\nBu = \"Bu.wav\"\nFa = \"Fa.wav\"\nFu = \"Fu.wav\"\nGa = \"Ga.wav\"\nGu = \"Gu.wav\"\nMa = \"Ma.wav\"\nMu = \"Mu.wav\"\nRa = \"Ra.wav\"\nRu = \"Ru.wav\"\n\nBiblioteka = \"St1.wav\"\nRaketa = \"St2.wav\"\nKurier = \"St3.wav\"\nOgrada = \"St4.wav\"\nHaketa = \"St5.wav\"\n\nphonemes_m3_labels = {\n    12: os.path.join(base, \"phonemes\", A),\n    22: os.path.join(base, \"phonemes\", A),\n    13: os.path.join(base, \"phonemes\", B),\n    23: os.path.join(base, \"phonemes\", B),\n    14: os.path.join(base, \"phonemes\", F),\n    24: os.path.join(base, \"phonemes\", F),\n    15: os.path.join(base, \"phonemes\", G),\n    25: os.path.join(base, \"phonemes\", G),\n    16: os.path.join(base, \"phonemes\", M),\n    26: os.path.join(base, \"phonemes\", M),\n    17: os.path.join(base, \"phonemes\", R),\n    27: os.path.join(base, \"phonemes\", R),\n    18: os.path.join(base, \"phonemes\", U),\n    28: os.path.join(base, \"phonemes\", U)\n}\n\nphonemes_m4_labels = {\n    1: os.path.join(base, \"phonemes\", A),\n    11: os.path.join(base, \"phonemes\", A),\n    2: os.path.join(base, \"phonemes\", B),\n    12: os.path.join(base, \"phonemes\", B),\n    3: os.path.join(base, \"phonemes\", F),\n    13: os.path.join(base, \"phonemes\", F),\n    4: os.path.join(base, \"phonemes\", G),\n    14: os.path.join(base, \"phonemes\", G),\n    5: os.path.join(base, \"phonemes\", M),\n    15: os.path.join(base, \"phonemes\", M),\n    6: os.path.join(base, \"phonemes\", R),\n    16: os.path.join(base, \"phonemes\", R),\n    7: os.path.join(base, \"phonemes\", U),\n    17: os.path.join(base, \"phonemes\", U)\n}\n\nsyllables_labels = {\n    1: os.path.join(base, \"syllables\", Ba),\n    11: os.path.join(base, \"syllables\", Ba),\n    2: os.path.join(base, \"syllables\", Fa),\n    12: os.path.join(base, \"syllables\", Fa),\n    3: os.path.join(base, \"syllables\", Ga),\n    13: os.path.join(base, \"syllables\", Ga),\n    4: os.path.join(base, \"syllables\", Ma),\n    14: os.path.join(base, \"syllables\", Ma),\n    5: os.path.join(base, \"syllables\", Ra),\n    15: os.path.join(base, \"syllables\", Ra),\n    6: os.path.join(base, \"syllables\", Bu),\n    16: os.path.join(base, \"syllables\", Bu),\n    7: os.path.join(base, \"syllables\", Ru),\n    17: os.path.join(base, \"syllables\", Ru),\n    8: os.path.join(base, \"syllables\", Mu),\n    18: os.path.join(base, \"syllables\", Mu),\n    9: os.path.join(base, \"syllables\", Fu),\n    19: os.path.join(base, \"syllables\", Fu),\n    10: os.path.join(base, \"syllables\", Gu),\n    20: os.path.join(base, \"syllables\", Gu)\n}\n\nwords_labels = {\n    11: os.path.join(base, \"words\", Biblioteka),\n    21: os.path.join(base, \"words\", Biblioteka),\n    12: os.path.join(base, \"words\", Raketa),\n    22: os.path.join(base, \"words\", Raketa),\n    13: os.path.join(base, \"words\", Kurier),\n    23: os.path.join(base, \"words\", Kurier),\n    14: os.path.join(base, \"words\", Ograda),\n    24: os.path.join(base, \"words\", Ograda),\n    15: os.path.join(base, \"words\", Haketa),\n    25: os.path.join(base, \"words\", Haketa)\n}\n\nsections = [\"syllables\", \"phonemes_m3\", \"phonemes_m4\", \"words\"]\n\naudio_map = {\n    \"syllables\": syllables_labels,\n    \"phonemes_m3\": phonemes_m3_labels,\n    \"phonemes_m4\": phonemes_m4_labels,\n    \"words\": words_labels\n}\n\nconfig = {\n    \n    'path': '/kaggle/input/internal-speech-recognition/Vartanov/feather',\n    'audio_maps': audio_map,\n    \n    # EEG\n    'eeg_sr': 1006,\n    'n_channels': 63,\n    'in_seq_len': 1145,\n    \n    # Wavelet Transform\n    'dj': 0.8,  # wavelet scaling factor\n    'wavelet': Morlet,\n    'n_wvt_bins': 12,\n    \n    # Audio\n    'audio_sr': 44100,\n    'sound_size': 50176,\n    \n    # STFT Patameters\n    'n_fft': 2048,\n    'hop_size': 512,\n    \n    # Model\n    \n    # Convolution Module\n    'kernel_size': 31,\n    'conv_module_dropout': .1,\n    \n    # Positional Encoding\n    'emb_dropout': .1,\n    \n    # Transformer\n    'd_model': 512,\n    'nhead': 8,\n    'num_encoder_layers': 6,\n    'num_decoder_layers': 6,\n    'dim_feedforward': 2048,\n    'dropout': 0.1,\n    'activation': 'relu',\n    \n    # Scheduler\n    'base_lr': 0.2,\n    'min_lr': 1e-5,\n}\n\naudio_paths = glob('/kaggle/input/internal-speech-recognition/Vartanov/audios/syllables/*.wav') + \\\n              glob('/kaggle/input/internal-speech-recognition/Vartanov/audios/words/*.wav')","metadata":{"execution":{"iopub.status.busy":"2023-08-07T11:51:26.3381Z","iopub.execute_input":"2023-08-07T11:51:26.338878Z","iopub.status.idle":"2023-08-07T11:51:26.493961Z","shell.execute_reply.started":"2023-08-07T11:51:26.338842Z","shell.execute_reply":"2023-08-07T11:51:26.492945Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# spectrogram to sound\n\ndef restore(D: np.array, frame_size=config['n_fft'], hop_length=config['hop_size'], epochs: int = 2, window: str = 'hann'):\n\n    length = (D.shape[1] + 1) * hop_length  # (D.shape[1] - 1 + 2) * hop_length\n    D = np.concatenate((np.zeros((D.shape[0], 1)), D, np.zeros((D.shape[0], 1))), axis=1)\n    mag, _ = librosa.magphase(D)\n    phase = np.exp(1.j * np.random.uniform(0., 2*np.pi, size=mag.shape))\n    x_ = librosa.istft(mag * phase, hop_length=hop_length, center=True, window=window, length=length)\n\n    for i in range(epochs):\n        _, phase = librosa.magphase(librosa.stft(x_, n_fft=frame_size, hop_length=hop_length, center=True,\n                                                 window=window))\n        x_ = librosa.istft(mag * phase, hop_length=hop_length, center=True, window=window, length=length)\n\n    return x_[hop_length:-hop_length]","metadata":{"execution":{"iopub.status.busy":"2023-08-07T11:51:26.499114Z","iopub.execute_input":"2023-08-07T11:51:26.501438Z","iopub.status.idle":"2023-08-07T11:51:26.581374Z","shell.execute_reply.started":"2023-08-07T11:51:26.501403Z","shell.execute_reply":"2023-08-07T11:51:26.58032Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class EEGDataset(Dataset):\n    def __init__(self, path: str, audio_maps: dict, fragment_length: int = 2012, partition_size: int = 32,\n                 sample_rate: int = 44100, sound_channel: int = 1, val_ratio: float = 0.15, seed: int = 1337):\n        '''\n        path: path to sections (folders)\n        audio_maps: two-level map: section names -> labels -> audio_paths\n        fragment_lengtht: length of fragment after label\n        partition_size: number of nonzero labels in each csv file\n        '''\n        super().__init__()\n        rnd = random.Random(seed)\n        \n        self.sections = os.listdir(path)\n        rnd.shuffle(self.sections)\n        assert set(self.sections) == set(audio_maps.keys()), \"Sections must be the same!\"\n        self.audio_maps = audio_maps\n        \n        all_paths = []\n        for sec in self.sections:\n            l = os.listdir(os.path.join(path, sec))\n            rnd.shuffle(l)\n            all_paths.append([os.path.join(path, sec, file) for file in l])\n                \n        # all_paths = [[os.path.join(path, sec, file) for file in sorted(os.listdir(os.path.join(path, sec)))] for sec in self.sections]\n        num_all_files = [len(elem) for elem in all_paths]\n        splits = [int(elem * val_ratio) for elem in num_all_files]\n        \n        self.val_paths = [sec_paths[:split] for sec_paths, split in zip(all_paths, splits)]\n        self.paths = [sec_paths[split:] for sec_paths, split in zip(all_paths, splits)]\n        \n        self.sec_num_files = [len(elem) for elem in self.paths]\n        self.sec_cumnum = np.cumsum(self.sec_num_files) * partition_size\n        self.total_num_files = sum(self.sec_num_files)\n        \n        self.sec_num_val_files = [len(elem) for elem in self.val_paths]\n        self.sec_val_cumnum = np.cumsum(self.sec_num_val_files) * partition_size\n        self.total_num_val_files = sum(self.sec_num_val_files)\n        \n        self.partition_size = partition_size\n        self.fragment_length = fragment_length\n        self.sr = sample_rate\n        self.sound_channel = sound_channel\n        self.val_mode = False\n        \n    def __len__(self) -> int:\n        num = self.total_num_val_files if self.val_mode else self.total_num_files\n        return num * self.partition_size\n    \n    def set_val_mode(self, mode: bool):\n        '''\n        Switch between train/val subsets\n        '''\n        assert mode in [True, False], \"Incorrect mode type!\"\n        self.val_mode = mode\n        return self\n    \n    def to_section(self, idx: int) -> Tuple[int, int]:\n        '''\n        Get file section and inner index by its absolute index\n        '''\n        cumnum = self.sec_val_cumnum if self.val_mode else self.sec_cumnum\n        section = np.where(idx < cumnum)[0][0]\n        section_idx = idx if (section == 0) else (idx - cumnum[section - 1])\n        return section, section_idx\n    \n    def get_audio(self, section: str, label: int) -> torch.Tensor:\n        '''\n        Get audio by section and corresponding label\n        '''\n        section_name = self.sections[section]\n        audio, current_sr = torchaudio.load(self.audio_maps[section_name][label])\n        audio = torchaudio.functional.resample(audio, orig_freq=current_sr, new_freq=self.sr)\n        return audio[self.sound_channel]\n    \n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        '''\n        int idx: file ID\n        return: EEG fragment with its corresponding audio\n        '''\n        section, section_idx = self.to_section(idx)\n        paths_source = self.val_paths if self.val_mode else self.paths\n        file_path = paths_source[section][section_idx // self.partition_size]\n        \n        start = (section_idx % self.partition_size) * self.fragment_length\n        end = start + self.fragment_length\n        \n        data = pd.read_feather(file_path).to_numpy()\n        x, label = torch.tensor(data[start:end, 1:]), data[start, 0].astype(int)\n        \n        audio = self.get_audio(section, label)\n        \n        # Cut model inputs so that they match desirable sizes\n        E, S = config['in_seq_len'], config['sound_size']\n        x = x[:E] if x.size(0) >= E else nn.functional.pad(x, (0, E-x.size(0)), value=0)\n        audio = audio[:S] if audio.size(0) >= S else nn.functional.pad(audio, (0, S-audio.size(0)), value=0)\n        \n        x = x.t()  # (n_channels, in_seq_len)\n        \n        return x, audio","metadata":{"execution":{"iopub.status.busy":"2023-08-07T11:51:26.586785Z","iopub.execute_input":"2023-08-07T11:51:26.589583Z","iopub.status.idle":"2023-08-07T11:51:26.68765Z","shell.execute_reply.started":"2023-08-07T11:51:26.589547Z","shell.execute_reply":"2023-08-07T11:51:26.686642Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class ConvolutionModule(torch.nn.Module):\n    def __init__(self, config):\n        '''\n        :param int d_model: Input dimension\n        :param int kernel_size: Kernel size of Depthwise Convolution\n        :param float dropout: Dropout probability \n        '''\n        super().__init__()\n        \n        self.d_model = config['d_model']\n        self.kernel_size = config['kernel_size']\n\n        self.layer_norm = nn.LayerNorm(self.d_model)\n        self.pointwise_conv_1 = nn.Conv1d(self.d_model, 2 * self.d_model, kernel_size=1)\n        self.activation_1 = nn.GLU()\n        self.depthwise_conv = nn.Conv1d(self.d_model, self.d_model, kernel_size=self.kernel_size, groups=self.d_model, padding='same')\n        self.batch_norm = nn.BatchNorm1d(self.d_model)\n        self.activation_2 = nn.SiLU()\n        self.pointwise_conv_2 = nn.Conv1d(self.d_model, self.d_model, kernel_size=1)\n        self.dropout = nn.Dropout(config['conv_module_dropout'])\n        \n        self.reset_parameters()\n\n    def forward(self, x: torch.Tensor, pad_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        '''\n        :param torch.Tensor x: (batch, time, d_model)\n        :param torch.Tensor pad_mask: (batch, time) takes True value for the positions corresponding to the padding\n        :return: (batch, time, d_model)\n        :rtype: torch.Tensor\n        '''\n        \n        x = self.layer_norm(x)\n        x = self.pointwise_conv_1(x.permute(0, 2, 1)).permute(0, 2, 1)\n        x = self.activation_1(x)\n\n        if pad_mask is not None:\n            x = x.masked_fill(pad_mask[..., None], 0.0)\n\n        x = self.depthwise_conv(x.permute(0, 2, 1))\n        x = self.batch_norm(x)\n        x = self.activation_2(x)\n        x = self.pointwise_conv_2(x).permute(0, 2, 1)\n        x = self.dropout(x)\n\n        return x\n    \n    def reset_parameters(self):\n        pw_max = self.d_model ** -0.5\n        dw_max = self.kernel_size ** -0.5\n        with torch.no_grad():\n            torch.nn.init.uniform_(self.pointwise_conv_1.weight, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.pointwise_conv_2.weight, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.depthwise_conv.weight, -dw_max, dw_max)\n            \n            torch.nn.init.uniform_(self.pointwise_conv_1.bias, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.pointwise_conv_2.bias, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.depthwise_conv.bias, -dw_max, dw_max)\n            \nclass PositionalEncoding(nn.Module):\n    def __init__(self, config):\n        \n        super().__init__()\n        emb_size = config['d_model']\n        dropout = config['emb_dropout']\n        maxlen = config['in_seq_len']\n        \n        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(0)  # (1, in_seq_len, d_model)\n\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n\n    def forward(self, x: torch.Tensor):\n        return self.dropout(x + self.pos_embedding)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T11:23:06.775344Z","iopub.execute_input":"2023-08-07T11:23:06.775714Z","iopub.status.idle":"2023-08-07T11:23:06.849076Z","shell.execute_reply.started":"2023-08-07T11:23:06.775682Z","shell.execute_reply":"2023-08-07T11:23:06.847974Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class E2STransformer(nn.Module):\n    \n    def __init__(self, config: dict, audio_paths: List[str], example_input):\n        \"\"\"\n        :param dict config: dictionart with all model parameters\n        :param List[str] audio_paths: list of audio file paths to fit PCA on\n        :param torch.tensor example_input: input to compute wavelet filters on. Should have shape (n_channels, in_seq_len)\n        \"\"\"\n        super().__init__()\n\n        self.conv_downsampling = torch.nn.Conv1d(config['n_channels'], 1, kernel_size=1) # (N, c_in, L) -> (N, 1, L)\n        self.ln = nn.LayerNorm(config['n_wvt_bins'])\n        self.ffn = nn.Linear(config['n_wvt_bins'], config['d_model'])\n        self.conv_module = ConvolutionModule(config)\n        self.positional_encoding = PositionalEncoding(config)\n        self.n_fft = config['n_fft']\n        self.hop_size = config['hop_size']\n        self.d_model = config['d_model']\n        self.transformer = torch.nn.Transformer(\n            d_model=config['d_model'],\n            nhead=config['nhead'],\n            num_encoder_layers=config['num_encoder_layers'],\n            num_decoder_layers=config['num_decoder_layers'],\n            dim_feedforward=config['dim_feedforward'],\n            dropout=config['dropout'],\n            activation=config['activation'],\n            batch_first=True\n        )\n        self.audio_sr = config['audio_sr']\n        self.compute_pca_components(audio_paths)\n        \n        # Specials\n        self.src_sos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n        self.src_eos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n        self.tgt_sos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n        self.tgt_eos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n        # self.register_parameter('src_sos', torch.Tensor(1, 1, self.d_model))\n        # self.register_parameter('src_eos', torch.Tensor(1, 1, self.d_model))\n        # self.register_parameter('tgt_sos', torch.Tensor(1, 1, self.d_model))\n        # self.register_parameter('tgt_eos', torch.Tensor(1, 1, self.d_model))\n        \n        self.reset_parameters()\n        self.get_wavelet_filters(example_input)\n        \n    def reset_parameters(self):\n        pw_max = self.d_model ** -0.5\n        with torch.no_grad():\n            torch.nn.init.uniform_(self.src_sos, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.src_eos, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.tgt_sos, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.tgt_eos, -pw_max, pw_max)\n    \n    def get_wavelet_filters(self, x):\n        \"\"\"\n        Computes Wavelet convolution weights\n        :param torch.tensor x: example input of shape (n_channels, in_seq_len)\n        \"\"\"\n        wvt_transformer = WaveletTransformTorch(\n            dt=1/config['eeg_sr'],\n            dj=config['dj'],\n            wavelet=config['wavelet'](),\n            cuda=torch.cuda.is_available()\n        )\n        _ = wvt_transformer.cwt(x)\n        filters = nn.ModuleList(wvt_transformer._extractor._filters)  # requires_grad: False\n        self.register_buffer('filters', filters)\n        \n    def compute_pca_components(self, audio_paths):\n        \"\"\"\n        :param List[str] audio_paths: list of audio file paths to fit PCA on\n        \"\"\"\n        audios_srs = [torchaudio.load(path) for path in audio_paths]\n        all_audios = []\n        for audio, sr in audios_srs:\n            if sr != self.audio_sr:\n                audio = torchaudio.functional.resample(waveform=audio, orig_freq=sr, new_freq=self.audio_sr)[0]\n            all_audios.append(audio)\n        \n        all_audios = torch.cat(all_audios)\n        all_audios = torch.stft(all_audios, n_fft=self.n_fft, hop_length=self.hop_size, return_complex=True)  # (n_freq_bins, n_frames)\n        all_audios = torch.abs(all_audios).t().numpy()\n        \n        pca = PCA(n_components=self.d_model)\n        pca.fit(all_audios)\n        \n        components = torch.tensor(pca.components_)  # (d_model, n_freq_bins)\n        mean = torch.tensor(pca.mean_)  # (n_freq_bins)\n        \n        self.register_buffer('components', components)\n        self.register_buffer('mean', mean)\n        \n    def prepare_src(self, x):\n        \"\"\"\n        :param torch.tensor x: input of shape (batch_size, n_channels, in_seq_len)\n        :rtype torch.tensor\n        :return out of shape (batch_size, in_seq_len, d_model)\n        \"\"\"\n        \n        # Wavelet Transform\n        ...\n        \n        # Convolution downsampling\n        out = out.permute(0, 3, 1, 2)  # (batch_size, in_seq_len, n_channels, n_wvt_bins)\n        out = out.reshape(batch_size * in_seq_len, n_channels, n_wvt_bins)  # (batch_size * in_seq_len, n_channels, n_wvt_bins)\n        out = self.conv_downsampling(out).squeeze(1)  # (batch_size * in_seq_len, n_wvt_bins)\n        out = out.reshape(batch_size, in_seq_len, n_wvt_bins)  # (batch_size, in_seq_len, n_wvt_bins)\n        \n        # LayerNorm & Feed Forward\n        out = self.ln(out)  # (batch_size, in_seq_len, n_wvt_bins)\n        out = self.ffn(out)  # (batch_size, in_seq_len, d_model)\n        \n        # Convolution module from https://arxiv.org/pdf/2005.08100.pdf\n        out = self.conv_module(out)  # (batch_size, in_seq_len, d_model)\n        \n        # Positional Encoding\n        out = self.positional_encoding(out)  # (batch_size, in_seq_len, d_model)\n        \n        return out\n    \n    def prepare_tgt(self, x):  # Add some audio normalization???\n        \"\"\"\n        :param torch.tensor x: input of shape (batch_size, audio_len)\n        :rtype torch.tensor\n        :return out of shape (batch_size, out_seq_len, d_model)\n        \"\"\"\n        # n_freq_bins = self.n_fft // 2 + 1\n        # out_seq_len = self.n_fft // self.hop_size + 1\n        \n        # STFT\n        out = torch.stft(x, n_fft=self.n_fft, hop_length=self.hop_size, return_complex=True)  # (batch_size, n_freq_bins, out_seq_len)\n        out = torch.abs(out.permute(0, 2, 1))  # (batch_size, out_seq_len, n_freq_bins)\n        \n        # PCA\n        out = out - self.mean\n        out = out @ self.components.t()  # (batch_size, out_seq_len, d_model)\n        return out\n        \n    def forward(self, eeg, audio):\n        \"\"\"\n        :param torch.tensor eeg: input of shape (batch_size, n_channels, in_seq_len)\n        :rtype torch.tensor\n        :return out of shape (batch_size, out_seq_len, n_freq_bins)\n        \"\"\"\n        batch_size = eeg.size(0)\n        src = self.prepare_src(eeg)  # (batch_size, in_seq_len, d_model)\n        tgt = self.prepare_tgt(audio)  # (batch_size, out_seq_len, d_model)\n        \n        # Add <sos> and <eos>\n        src = torch.cat((self.src_sos.repeat(batch_size, 1, 1), src, self.src_eos.repeat(batch_size, 1, 1)),\n                        dim=1)  # (batch_size, 1 + in_seq_len + 1, d_model)\n        tgt = torch.cat((self.tgt_sos.repeat(batch_size, 1, 1), tgt, self.tgt_eos.repeat(batch_size, 1, 1)),\n                        dim=1)  # (batch_size, 1 + out_seq_len + 1, d_model)\n        \n        # Transformer\n        causal_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(eeg.device)\n        out = self.transformer(src=src, tgt=tgt, tgt_mask=causal_mask)  # (batch_size, 1 + out_seq_len + 1, d_model)\n        # out = out[:, 1:-1, :]  # (batch_size, out_seq_len, d_model)\n        \n        # Inverse PCA\n        # out = out @ self.components  # (batch_size, out_seq_len, n_freq_bins)\n        # out = out + self.mean\n        \n        return out, tgt","metadata":{"execution":{"iopub.status.busy":"2023-08-07T11:23:07.591434Z","iopub.execute_input":"2023-08-07T11:23:07.591831Z","iopub.status.idle":"2023-08-07T11:23:07.668417Z","shell.execute_reply.started":"2023-08-07T11:23:07.591801Z","shell.execute_reply":"2023-08-07T11:23:07.667137Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Noam Annealing","metadata":{}},{"cell_type":"code","source":"class NoamAnnealing(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(\n        self, optimizer: torch.optim.Optimizer, *, \n        d_model: int, warmup_steps: int, min_lr: float = 0.0, last_epoch: int = -1\n    ):\n        \"\"\"\n        :param torch.optim.Optimizer optimizer:\n        :param int d_model: Model input dimension\n        :param int warmup_steps:\n        :param float min_lr: Lower bound for learning rate after warmup\n        :param int last_epoch:\n        \"\"\"\n        assert warmup_steps\n        \n        # It is necessary to assign all attributes *before* __init__,\n        # as class is wrapped by an inner class.\n        self.min_lr = min_lr\n        self.warmup_steps = warmup_steps\n        self.normalization = d_model ** (-0.5)\n\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if not self._get_lr_called_within_step:\n            warnings.warn(\n                \"To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\", UserWarning\n            )\n\n        step = max(1, self.last_epoch)\n        new_lrs = [\n            self._noam_annealing(initial_lr=initial_lr, step=step) \n            for initial_lr in self.base_lrs\n        ]\n        return new_lrs\n\n    def _noam_annealing(self, initial_lr: float, step: int) -> float:\n        \"\"\"Compute noam annealing learning rate \n            as described in https://arxiv.org/abs/1706.03762 Section 5.3.\n            After warmup_steps learning rate should be always greater than min_lr\n\n        :param float initial_lr: Additional multiplicative factor for learning rate\n        :param int step: Current optimization step\n        :return: Learning rate at given step\n        :rtype: float\n        \"\"\"\n        lrate = self.normalization * min(step ** (-0.5), step * self.warmup_steps ** (-1.5)) * initial_lr\n        if step > self.warmup_steps:\n            lrate = max(self.min_lr, lrate)\n        \n        return lrate","metadata":{"execution":{"iopub.status.busy":"2023-08-07T11:23:08.443323Z","iopub.execute_input":"2023-08-07T11:23:08.443707Z","iopub.status.idle":"2023-08-07T11:23:08.508565Z","shell.execute_reply.started":"2023-08-07T11:23:08.443678Z","shell.execute_reply":"2023-08-07T11:23:08.507179Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"%%capture --no-stdout --no-display\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel = E2STransformer(config, audio_paths).to(device)\n\nscheduler = NoamAnnealing(\n    torch.optim.Adam(model.parameters(), lr=0.2),\n    d_model=512, warmup_steps=500, min_lr=1e-5\n)\n\nlearning_rates = []\nfor _ in range(5000):\n    scheduler.step()\n    learning_rates.append(scheduler.get_last_lr()[0])\n    \nfig, ax = plt.subplots(1, 1, figsize=(10, 4))\nax.plot(learning_rates)\nax.grid(True)\nax.set_xlabel('Step')\nax.set_ylabel('Learning Rate')\nax.set_title('NoamAnnealing')\n\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-06T15:21:04.866429Z","iopub.execute_input":"2023-08-06T15:21:04.866789Z","iopub.status.idle":"2023-08-06T15:21:10.159989Z","shell.execute_reply.started":"2023-08-06T15:21:04.866761Z","shell.execute_reply":"2023-08-06T15:21:10.159056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataloaders","metadata":{}},{"cell_type":"code","source":"train_ds = EEGDataset(path=config['path'], audio_maps=config['audio_maps'])\nval_ds = EEGDataset(path=config['path'], audio_maps=config['audio_maps']).set_val_mode(True)\n\ntrain_dl = DataLoader(train_ds, 8, shuffle=True, num_workers=1)\nval_dl = DataLoader(val_ds, 8, shuffle=False, num_workers=1)\n\nprint(f'{\"Train dataset len:\": <20} {len(train_ds)};\\t{\"Validation datset len:\": <25} {len(val_ds)};')\nprint(f'{\"Num train batches:\": <20} {len(train_dl)};\\t{\"Num validation batches:\": <25} {len(val_dl)};')","metadata":{"execution":{"iopub.status.busy":"2023-08-07T11:51:59.167592Z","iopub.execute_input":"2023-08-07T11:51:59.167954Z","iopub.status.idle":"2023-08-07T11:51:59.639141Z","shell.execute_reply.started":"2023-08-07T11:51:59.167922Z","shell.execute_reply":"2023-08-07T11:51:59.637962Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Train dataset len:   22368;\tValidation datset len:    3872;\nNum train batches:   2796;\tNum validation batches:   484;\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Sanity checks","metadata":{}},{"cell_type":"code","source":"eeg, audio = train_ds[15296]\neeg, audio = eeg.unsqueeze(0).to(device), audio.unsqueeze(0).to(device)\n\nout, tgt = model(eeg, audio)","metadata":{"execution":{"iopub.status.busy":"2023-08-06T15:21:15.907564Z","iopub.execute_input":"2023-08-06T15:21:15.908128Z","iopub.status.idle":"2023-08-06T15:21:22.938823Z","shell.execute_reply.started":"2023-08-06T15:21:15.908084Z","shell.execute_reply":"2023-08-06T15:21:22.937653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    restored = tgt[:, 1:-1, :].squeeze()  # (out_seq_len, d_model)\n    restored = restored @ model.components  # (out_seq_len, n_freq_bins)\n    restored = (restored + model.mean).t().cpu().numpy()  # (n_freq_bins, out_seq_len)\n\nrestored = restore(restored)\nAudio(restored, rate=train_ds.sr)","metadata":{"execution":{"iopub.status.busy":"2023-08-06T15:21:22.941005Z","iopub.execute_input":"2023-08-06T15:21:22.941406Z","iopub.status.idle":"2023-08-06T15:21:32.016366Z","shell.execute_reply.started":"2023-08-06T15:21:22.941371Z","shell.execute_reply":"2023-08-06T15:21:32.015299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, device, criterion, optimizer, scheduler, scaler, max_norm, description):\n        self.model = model\n        self.device = device\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.scaler = scaler\n        self.max_norm = max_norm\n        self.description = description\n        self.hist = []  # training history\n    \n    @staticmethod\n    def download_file(path, download_file_name):\n        os.chdir('/kaggle/working/')\n        zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n        command = f\"zip {zip_name} {path} -r\"\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n        if result.returncode != 0:\n            print(\"Unable to run zip command!\")\n            print(result.stderr)\n            return\n        display(FileLink(f'{download_file_name}.zip'))\n        \n    def fit(self, n_epochs, train_dl, val_dl):\n        \n        # model checkpoints directory\n        model_checkpoint_path = os.path.join(os.getcwd(), 'model_checkpoints')    \n        if not os.path.exists(model_checkpoint_path):\n            os.mkdir(model_checkpoint_path)\n        \n        model = self.model.to(self.device).train()\n        current_lr = self.scheduler.get_last_lr()[-1]\n        total_step = 0\n        \n        # mean losses\n        train_epoch_loss = []\n        val_epoch_loss = []\n                \n        for epoch in trange(n_epochs):\n            \n            train_epoch_loss = []\n            for eeg, audio in (pbar := tqdm(train_dl, total=len(train_dl))):\n\n                total_step += 1\n                \n                # Move tensors to device\n                eeg, audio = eeg.to(self.device), audio.to(self.device)\n\n                # Clear gradients\n                self.optimizer.zero_grad()\n\n                # Perform forward pass with mixed precision\n                with torch.autocast(device_type='cuda', dtype=torch.float16):\n                    pred_encoding, encoding = model(eeg, audio)\n                    loss = self.criterion(pred_encoding, encoding)\n                \n                train_epoch_loss.append(loss.item())\n                \n                # Perform backward pass\n                self.scaler.scale(loss).backward()\n                self.scaler.unscale_(self.optimizer)\n\n                # Perform Gradient Clipping\n                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), self.max_norm).item()\n\n                # Perform Optimization Step\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n\n                # Perform Scheduler Step\n                self.scheduler.step()\n                current_lr = self.scheduler.get_last_lr()[-1]\n\n                # saving history\n                self.hist.append((total_step, loss.item(), self.description, 'train'))\n\n                # update tqdm info\n                pbar.set_description(\n                    f'Step: {total_step}|\\t\\\n                      Lr: {current_lr:.3e}|\\t\\\n                      GradNorm: {grad_norm:.3e}|\\t\\\n                      Train Loss: {loss.item():.3f}|\\t'\n                )\n\n                # validation\n                if total_step % 32 == 0:\n\n                    val_epoch_loss = []\n                    model.eval()\n\n                    with torch.no_grad():\n                        for eeg, audio in (val_pbar := tqdm(val_dl, total=len(val_dl))):\n                            eeg, audio = eeg.to(self.device), audio.to(self.device)\n                            pred_encoding, encoding = model(eeg, audio)\n                            loss = self.criterion(pred_encoding, encoding).item()\n                            val_epoch_loss.append(loss)\n\n                            # update tqdm info\n                            val_pbar.set_description(f'Validation|\\tVal Loss: {loss:.3f}')\n                            \n                            if len(val_epoch_loss) == len(val_dl):\n                                val_pbar.set_description(f'Validation|\\tVal Loss: {np.mean(val_epoch_loss):.3f}')\n                        # saving val history\n                        self.hist.append((total_step, np.mean(val_epoch_loss), self.description, 'val'))\n\n                    model.train()\n                    \n                if total_step in [32, 64]:\n                    break\n                    \n                # update tqdm info after epoch\n                if len(train_epoch_loss) == len(train_dl):\n                    pbar.set_description(\n                        f'Step: {total_step}|\\t\\\n                          Lr: {current_lr:.3e}|\\t\\\n                          GradNorm: {grad_norm:.3e}|\\t\\\n                          Train Loss: {np.mean(train_epoch_loss):.3f}|\\t\\\n                          Val Loss: {np.mean(val_epoch_loss):.3f}'\n                    )\n\n        # saving model\n        self.model = model.eval()\n        torch.save(model.state_dict(), os.path.join(model_checkpoint_path, f'{self.description}.pth'))\n\n        with open(os.path.join(model_checkpoint_path, 'hist.pickle'), 'wb') as handle:\n            pickle.dump(self.hist, handle)\n            \n        # download files\n        self.download_file(os.path.join(model_checkpoint_path, f'{self.description}.pth'), 'model')\n        self.download_file(os.path.join(model_checkpoint_path, 'hist.pickle'), 'hist')\n        ","metadata":{"execution":{"iopub.status.busy":"2023-08-07T11:23:16.604253Z","iopub.execute_input":"2023-08-07T11:23:16.604706Z","iopub.status.idle":"2023-08-07T11:23:16.680146Z","shell.execute_reply.started":"2023-08-07T11:23:16.604672Z","shell.execute_reply":"2023-08-07T11:23:16.678487Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Training on a single GPU P100","metadata":{}},{"cell_type":"code","source":"model = E2STransformer(config, audio_paths)\noptimizer = torch.optim.Adam(model.parameters(), lr=config['base_lr'])\n\ntrainer = Trainer(\n    model=model,\n    device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n    criterion=nn.MSELoss(),\n    optimizer=optimizer,\n    scheduler=NoamAnnealing(optimizer=optimizer, d_model=config['d_model'], warmup_steps=len(train_dl) // 5,\n                            min_lr=config['min_lr']),\n    scaler=torch.cuda.amp.GradScaler(),\n    max_norm=10.0,\n    description='Transformer_2gpu'\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T11:27:17.730307Z","iopub.execute_input":"2023-08-07T11:27:17.730738Z","iopub.status.idle":"2023-08-07T11:27:18.84358Z","shell.execute_reply.started":"2023-08-07T11:27:17.730708Z","shell.execute_reply":"2023-08-07T11:27:18.842162Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"%%time\ntrainer.fit(n_epochs=2, train_dl=train_dl, val_dl=val_dl)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T11:27:18.845805Z","iopub.execute_input":"2023-08-07T11:27:18.846554Z","iopub.status.idle":"2023-08-07T11:27:19.425677Z","shell.execute_reply.started":"2023-08-07T11:27:18.846516Z","shell.execute_reply":"2023-08-07T11:27:19.4243Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30acd8ddccb34d2294e85f8ba04f63cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2796 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b6a2bc4b3634ac292407dba1cccd183"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","File \u001b[0;32m<timed eval>:1\u001b[0m\n","Cell \u001b[0;32mIn[11], line 43\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, n_epochs, train_dl, val_dl)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m trange(n_epochs):\n\u001b[1;32m     42\u001b[0m     train_epoch_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m eeg, audio \u001b[38;5;129;01min\u001b[39;00m (pbar \u001b[38;5;241m:=\u001b[39m tqdm(train_dl, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dl))):\n\u001b[1;32m     45\u001b[0m         total_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;66;03m# Move tensors to device\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n","\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_28/2117817392.py\", line 107, in __getitem__\n    x = self.wvt_transformer.cwt(x)  # (n_channels, n_wvt_bins, in_seq_len)\n  File \"/kaggle/working/PyTorchWavelets/wavelets_pytorch/transform.py\", line 299, in cwt\n    self.signal_length = signal_length\n  File \"/kaggle/working/PyTorchWavelets/wavelets_pytorch/transform.py\", line 346, in signal_length\n    self._extractor.set_filters(self._filters)\n  File \"/kaggle/working/PyTorchWavelets/wavelets_pytorch/network.py\", line 96, in set_filters\n    if self._cuda: conv.cuda()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 905, in cuda\n    return self._apply(lambda t: t.cuda(device))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 820, in _apply\n    param_applied = fn(param)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 905, in <lambda>\n    return self._apply(lambda t: t.cuda(device))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 235, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"],"ename":"RuntimeError","evalue":"Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_28/2117817392.py\", line 107, in __getitem__\n    x = self.wvt_transformer.cwt(x)  # (n_channels, n_wvt_bins, in_seq_len)\n  File \"/kaggle/working/PyTorchWavelets/wavelets_pytorch/transform.py\", line 299, in cwt\n    self.signal_length = signal_length\n  File \"/kaggle/working/PyTorchWavelets/wavelets_pytorch/transform.py\", line 346, in signal_length\n    self._extractor.set_filters(self._filters)\n  File \"/kaggle/working/PyTorchWavelets/wavelets_pytorch/network.py\", line 96, in set_filters\n    if self._cuda: conv.cuda()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 905, in cuda\n    return self._apply(lambda t: t.cuda(device))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 820, in _apply\n    param_applied = fn(param)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 905, in <lambda>\n    return self._apply(lambda t: t.cuda(device))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 235, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n","output_type":"error"}]},{"cell_type":"markdown","source":"1.05s/train it\n\nCPU times: user 2min 30s, sys: 6.1 s, total: 2min 36s\n\nWall time: 7min 20s","metadata":{}},{"cell_type":"markdown","source":"## Training on 2 GPU T4 in parallel\n\n![img](https://miro.medium.com/v2/resize:fit:2000/format:webp/1*FpDHkWJhkLL7KxU01Lf9Lw.png)","metadata":{}},{"cell_type":"code","source":"model = E2STransformer(config, audio_paths)\nparallel_model = torch.nn.DataParallel(model)\noptimizer = torch.optim.Adam(parallel_model.parameters(), lr=config['base_lr'])\n\ntrainer = Trainer(\n    model=parallel_model,\n    device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n    criterion=nn.MSELoss(),\n    optimizer=optimizer,\n    scheduler=NoamAnnealing(optimizer=optimizer, d_model=config['d_model'], warmup_steps=len(train_dl) // 5,\n                            min_lr=config['min_lr']),\n    scaler=torch.cuda.amp.GradScaler(),\n    max_norm=10.0, \n    description='Transformer_gpu0'\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-06T18:23:50.010866Z","iopub.execute_input":"2023-08-06T18:23:50.011492Z","iopub.status.idle":"2023-08-06T18:23:51.144582Z","shell.execute_reply.started":"2023-08-06T18:23:50.011454Z","shell.execute_reply":"2023-08-06T18:23:51.143069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrainer.fit(n_epochs=2, train_dl=train_dl, val_dl=val_dl)","metadata":{"execution":{"iopub.status.busy":"2023-08-06T18:23:51.146629Z","iopub.execute_input":"2023-08-06T18:23:51.14722Z","iopub.status.idle":"2023-08-06T18:23:54.867497Z","shell.execute_reply.started":"2023-08-06T18:23:51.14715Z","shell.execute_reply":"2023-08-06T18:23:54.866266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Balanced load on a multi-GPU machine","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}