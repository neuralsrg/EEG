{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/neuralsrg/e2s-transformer?scriptVersionId=139334855\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# EEG to Speech Transformer","metadata":{}},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"execution":{"iopub.status.busy":"2023-08-08T17:52:45.699699Z","iopub.execute_input":"2023-08-08T17:52:45.700167Z","iopub.status.idle":"2023-08-08T17:52:45.767789Z","shell.execute_reply.started":"2023-08-08T17:52:45.70011Z","shell.execute_reply":"2023-08-08T17:52:45.766711Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"! rm -rf /kaggle/working/PyTorchWavelets\n! git clone https://github.com/neuralsrg/PyTorchWavelets.git\n# ! git clone -b develop --single-branch https://github.com/neuralsrg/PyTorchWavelets.git","metadata":{"execution":{"iopub.status.busy":"2023-08-08T17:52:45.865314Z","iopub.execute_input":"2023-08-08T17:52:45.86587Z","iopub.status.idle":"2023-08-08T17:52:50.006912Z","shell.execute_reply.started":"2023-08-08T17:52:45.865822Z","shell.execute_reply":"2023-08-08T17:52:50.005484Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'PyTorchWavelets'...\nremote: Enumerating objects: 108, done.\u001b[K\nremote: Counting objects: 100% (8/8), done.\u001b[K\nremote: Compressing objects: 100% (7/7), done.\u001b[K\nremote: Total 108 (delta 1), reused 5 (delta 1), pack-reused 100\u001b[K\nReceiving objects: 100% (108/108), 1.08 MiB | 7.36 MiB/s, done.\nResolving deltas: 100% (48/48), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport sys\nimport random\nsys.path.append('/kaggle/working/PyTorchWavelets/')\nimport subprocess\n\nimport math\nimport pickle\nfrom glob import glob\nfrom functools import partial\nfrom tqdm.notebook import tqdm, trange\nfrom typing import List, Tuple, Optional\nfrom IPython.display import Audio, FileLink, display, clear_output\n\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchaudio\nimport torch.nn as nn\ntorch.manual_seed(1337)\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom wavelets_pytorch.transform import WaveletTransformTorch\nfrom wavelets_pytorch.wavelets import Morlet\n\nfrom sklearn.decomposition import PCA\n# clear_output()","metadata":{"execution":{"iopub.status.busy":"2023-08-08T17:52:50.012348Z","iopub.execute_input":"2023-08-08T17:52:50.013376Z","iopub.status.idle":"2023-08-08T17:52:57.329848Z","shell.execute_reply.started":"2023-08-08T17:52:50.01334Z","shell.execute_reply":"2023-08-08T17:52:57.328449Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"base = '/kaggle/input/internal-speech-recognition/Vartanov/audios'\n\nA = \"A.wav\"\nB = \"B.wav\"\nF = \"F.wav\"\nG = \"G.wav\"\nM = \"M.wav\"\nR = \"R.wav\"\nU = \"U.wav\"\n\nBa = \"Ba.wav\"\nBu = \"Bu.wav\"\nFa = \"Fa.wav\"\nFu = \"Fu.wav\"\nGa = \"Ga.wav\"\nGu = \"Gu.wav\"\nMa = \"Ma.wav\"\nMu = \"Mu.wav\"\nRa = \"Ra.wav\"\nRu = \"Ru.wav\"\n\nBiblioteka = \"St1.wav\"\nRaketa = \"St2.wav\"\nKurier = \"St3.wav\"\nOgrada = \"St4.wav\"\nHaketa = \"St5.wav\"\n\nphonemes_m3_labels = {\n    12: os.path.join(base, \"phonemes\", A),\n    22: os.path.join(base, \"phonemes\", A),\n    13: os.path.join(base, \"phonemes\", B),\n    23: os.path.join(base, \"phonemes\", B),\n    14: os.path.join(base, \"phonemes\", F),\n    24: os.path.join(base, \"phonemes\", F),\n    15: os.path.join(base, \"phonemes\", G),\n    25: os.path.join(base, \"phonemes\", G),\n    16: os.path.join(base, \"phonemes\", M),\n    26: os.path.join(base, \"phonemes\", M),\n    17: os.path.join(base, \"phonemes\", R),\n    27: os.path.join(base, \"phonemes\", R),\n    18: os.path.join(base, \"phonemes\", U),\n    28: os.path.join(base, \"phonemes\", U)\n}\n\nphonemes_m4_labels = {\n    1: os.path.join(base, \"phonemes\", A),\n    11: os.path.join(base, \"phonemes\", A),\n    2: os.path.join(base, \"phonemes\", B),\n    12: os.path.join(base, \"phonemes\", B),\n    3: os.path.join(base, \"phonemes\", F),\n    13: os.path.join(base, \"phonemes\", F),\n    4: os.path.join(base, \"phonemes\", G),\n    14: os.path.join(base, \"phonemes\", G),\n    5: os.path.join(base, \"phonemes\", M),\n    15: os.path.join(base, \"phonemes\", M),\n    6: os.path.join(base, \"phonemes\", R),\n    16: os.path.join(base, \"phonemes\", R),\n    7: os.path.join(base, \"phonemes\", U),\n    17: os.path.join(base, \"phonemes\", U)\n}\n\nsyllables_labels = {\n    1: os.path.join(base, \"syllables\", Ba),\n    11: os.path.join(base, \"syllables\", Ba),\n    2: os.path.join(base, \"syllables\", Fa),\n    12: os.path.join(base, \"syllables\", Fa),\n    3: os.path.join(base, \"syllables\", Ga),\n    13: os.path.join(base, \"syllables\", Ga),\n    4: os.path.join(base, \"syllables\", Ma),\n    14: os.path.join(base, \"syllables\", Ma),\n    5: os.path.join(base, \"syllables\", Ra),\n    15: os.path.join(base, \"syllables\", Ra),\n    6: os.path.join(base, \"syllables\", Bu),\n    16: os.path.join(base, \"syllables\", Bu),\n    7: os.path.join(base, \"syllables\", Ru),\n    17: os.path.join(base, \"syllables\", Ru),\n    8: os.path.join(base, \"syllables\", Mu),\n    18: os.path.join(base, \"syllables\", Mu),\n    9: os.path.join(base, \"syllables\", Fu),\n    19: os.path.join(base, \"syllables\", Fu),\n    10: os.path.join(base, \"syllables\", Gu),\n    20: os.path.join(base, \"syllables\", Gu)\n}\n\nwords_labels = {\n    11: os.path.join(base, \"words\", Biblioteka),\n    21: os.path.join(base, \"words\", Biblioteka),\n    12: os.path.join(base, \"words\", Raketa),\n    22: os.path.join(base, \"words\", Raketa),\n    13: os.path.join(base, \"words\", Kurier),\n    23: os.path.join(base, \"words\", Kurier),\n    14: os.path.join(base, \"words\", Ograda),\n    24: os.path.join(base, \"words\", Ograda),\n    15: os.path.join(base, \"words\", Haketa),\n    25: os.path.join(base, \"words\", Haketa)\n}\n\nsections = [\"syllables\", \"phonemes_m3\", \"phonemes_m4\", \"words\"]\n\naudio_map = {\n    \"syllables\": syllables_labels,\n    \"phonemes_m3\": phonemes_m3_labels,\n    \"phonemes_m4\": phonemes_m4_labels,\n    \"words\": words_labels\n}\n\nconfig = {\n    \n    'path': '/kaggle/input/internal-speech-recognition/Vartanov/feather',\n    'audio_maps': audio_map,\n    \n    # EEG\n    'eeg_sr': 1006,\n    'n_channels': 63,\n    'in_seq_len': 1145,\n    \n    # Wavelet Transform\n    'dj': 0.8,  # wavelet scaling factor\n    'wavelet': Morlet,\n    'n_wvt_bins': 12,\n    \n    # Audio\n    'audio_sr': 44100,\n    'sound_size': 50176,\n    \n    # STFT Patameters\n    'n_fft': 2048,\n    'hop_size': 512,\n    \n    # Model\n    \n    # Convolution Module\n    'kernel_size': 31,\n    'conv_module_dropout': .1,\n    \n    # Positional Encoding\n    'emb_dropout': .1,\n    \n    # Transformer\n    'd_model': 512,\n    'nhead': 8,\n    'num_encoder_layers': 6,\n    'num_decoder_layers': 6,\n    'dim_feedforward': 2048,\n    'dropout': 0.1,\n    'activation': 'relu',\n    \n    # Scheduler\n    'base_lr': 0.2,\n    'min_lr': 1e-5,\n}\n\naudio_paths = glob('/kaggle/input/internal-speech-recognition/Vartanov/audios/syllables/*.wav') + \\\n              glob('/kaggle/input/internal-speech-recognition/Vartanov/audios/words/*.wav')","metadata":{"execution":{"iopub.status.busy":"2023-08-08T17:52:57.332031Z","iopub.execute_input":"2023-08-08T17:52:57.33266Z","iopub.status.idle":"2023-08-08T17:52:57.442509Z","shell.execute_reply.started":"2023-08-08T17:52:57.332629Z","shell.execute_reply":"2023-08-08T17:52:57.441335Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# spectrogram to sound\n\ndef restore(D: np.array, frame_size=config['n_fft'], hop_length=config['hop_size'], epochs: int = 2, window: str = 'hann'):\n\n    length = (D.shape[1] + 1) * hop_length  # (D.shape[1] - 1 + 2) * hop_length\n    D = np.concatenate((np.zeros((D.shape[0], 1)), D, np.zeros((D.shape[0], 1))), axis=1)\n    mag, _ = librosa.magphase(D)\n    phase = np.exp(1.j * np.random.uniform(0., 2*np.pi, size=mag.shape))\n    x_ = librosa.istft(mag * phase, hop_length=hop_length, center=True, window=window, length=length)\n\n    for i in range(epochs):\n        _, phase = librosa.magphase(librosa.stft(x_, n_fft=frame_size, hop_length=hop_length, center=True,\n                                                 window=window))\n        x_ = librosa.istft(mag * phase, hop_length=hop_length, center=True, window=window, length=length)\n\n    return x_[hop_length:-hop_length]","metadata":{"execution":{"iopub.status.busy":"2023-08-08T17:52:57.444622Z","iopub.execute_input":"2023-08-08T17:52:57.445398Z","iopub.status.idle":"2023-08-08T17:52:57.52384Z","shell.execute_reply.started":"2023-08-08T17:52:57.445362Z","shell.execute_reply":"2023-08-08T17:52:57.522831Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class EEGDataset(Dataset):\n    def __init__(self, path: str, audio_maps: dict, fragment_length: int = 2012, partition_size: int = 32,\n                 sample_rate: int = 44100, sound_channel: int = 1, val_ratio: float = 0.15, seed: int = 1337):\n        '''\n        path: path to sections (folders)\n        audio_maps: two-level map: section names -> labels -> audio_paths\n        fragment_lengtht: length of fragment after label\n        partition_size: number of nonzero labels in each csv file\n        '''\n        super().__init__()\n        rnd = random.Random(seed)\n        \n        self.sections = os.listdir(path)\n        rnd.shuffle(self.sections)\n        assert set(self.sections) == set(audio_maps.keys()), \"Sections must be the same!\"\n        self.audio_maps = audio_maps\n        \n        all_paths = []\n        for sec in self.sections:\n            l = os.listdir(os.path.join(path, sec))\n            rnd.shuffle(l)\n            all_paths.append([os.path.join(path, sec, file) for file in l])\n                \n        # all_paths = [[os.path.join(path, sec, file) for file in sorted(os.listdir(os.path.join(path, sec)))] for sec in self.sections]\n        num_all_files = [len(elem) for elem in all_paths]\n        splits = [int(elem * val_ratio) for elem in num_all_files]\n        \n        self.val_paths = [sec_paths[:split] for sec_paths, split in zip(all_paths, splits)]\n        self.paths = [sec_paths[split:] for sec_paths, split in zip(all_paths, splits)]\n        \n        self.sec_num_files = [len(elem) for elem in self.paths]\n        self.sec_cumnum = np.cumsum(self.sec_num_files) * partition_size\n        self.total_num_files = sum(self.sec_num_files)\n        \n        self.sec_num_val_files = [len(elem) for elem in self.val_paths]\n        self.sec_val_cumnum = np.cumsum(self.sec_num_val_files) * partition_size\n        self.total_num_val_files = sum(self.sec_num_val_files)\n        \n        self.partition_size = partition_size\n        self.fragment_length = fragment_length\n        self.sr = sample_rate\n        self.sound_channel = sound_channel\n        self.val_mode = False\n        \n    def __len__(self) -> int:\n        num = self.total_num_val_files if self.val_mode else self.total_num_files\n        return num * self.partition_size\n    \n    def set_val_mode(self, mode: bool):\n        '''\n        Switch between train/val subsets\n        '''\n        assert mode in [True, False], \"Incorrect mode type!\"\n        self.val_mode = mode\n        return self\n    \n    def to_section(self, idx: int) -> Tuple[int, int]:\n        '''\n        Get file section and inner index by its absolute index\n        '''\n        cumnum = self.sec_val_cumnum if self.val_mode else self.sec_cumnum\n        section = np.where(idx < cumnum)[0][0]\n        section_idx = idx if (section == 0) else (idx - cumnum[section - 1])\n        return section, section_idx\n    \n    def get_audio(self, section: str, label: int) -> torch.Tensor:\n        '''\n        Get audio by section and corresponding label\n        '''\n        section_name = self.sections[section]\n        audio, current_sr = torchaudio.load(self.audio_maps[section_name][label])\n        audio = torchaudio.functional.resample(audio, orig_freq=current_sr, new_freq=self.sr)\n        return audio[self.sound_channel]\n    \n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        '''\n        int idx: file ID\n        return: EEG fragment with its corresponding audio\n        '''\n        section, section_idx = self.to_section(idx)\n        paths_source = self.val_paths if self.val_mode else self.paths\n        file_path = paths_source[section][section_idx // self.partition_size]\n        \n        start = (section_idx % self.partition_size) * self.fragment_length\n        end = start + self.fragment_length\n        \n        data = pd.read_feather(file_path).to_numpy()\n        x, label = torch.tensor(data[start:end, 1:]), data[start, 0].astype(int)\n        \n        audio = self.get_audio(section, label)\n        \n        # Cut model inputs so that they match desirable sizes\n        E, S = config['in_seq_len'], config['sound_size']\n        x = x[:E] if x.size(0) >= E else nn.functional.pad(x, (0, E-x.size(0)), value=0)\n        audio = audio[:S] if audio.size(0) >= S else nn.functional.pad(audio, (0, S-audio.size(0)), value=0)\n        \n        x = x.t()  # (n_channels, in_seq_len)\n        \n        return x.float(), audio.float()","metadata":{"execution":{"iopub.status.busy":"2023-08-08T17:52:57.526958Z","iopub.execute_input":"2023-08-08T17:52:57.527644Z","iopub.status.idle":"2023-08-08T17:52:57.629693Z","shell.execute_reply.started":"2023-08-08T17:52:57.527609Z","shell.execute_reply":"2023-08-08T17:52:57.628603Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Dataloaders","metadata":{}},{"cell_type":"code","source":"train_ds = EEGDataset(path=config['path'], audio_maps=config['audio_maps'])\nval_ds = EEGDataset(path=config['path'], audio_maps=config['audio_maps']).set_val_mode(True)\n\n# train_dl = DataLoader(train_ds, 8, shuffle=True, num_workers=2)\n# val_dl = DataLoader(val_ds, 8, shuffle=False, num_workers=2)\ntrain_dl = partial(DataLoader, dataset=train_ds, shuffle=True, num_workers=2)\nval_dl = partial(DataLoader, dataset=val_ds, num_workers=2)\n\nbatch_size = 8\nprint(f'Batch size: {batch_size}')\nprint(f'{\"Train dataset len:\": <20} {len(train_ds)};\\t{\"Validation datset len:\": <25} {len(val_ds)};')\nprint(f'{\"Num train batches:\": <20} {len(train_dl(batch_size=batch_size))};\\t{\"Num validation batches:\": <25} {len(val_dl(batch_size=batch_size))};')","metadata":{"execution":{"iopub.status.busy":"2023-08-08T17:52:57.631429Z","iopub.execute_input":"2023-08-08T17:52:57.632088Z","iopub.status.idle":"2023-08-08T17:52:57.944146Z","shell.execute_reply.started":"2023-08-08T17:52:57.632053Z","shell.execute_reply":"2023-08-08T17:52:57.943242Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Batch size: 8\nTrain dataset len:   22368;\tValidation datset len:    3872;\nNum train batches:   2796;\tNum validation batches:   484;\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class ConvolutionModule(torch.nn.Module):\n    def __init__(self, config):\n        '''\n        :param int d_model: Input dimension\n        :param int kernel_size: Kernel size of Depthwise Convolution\n        :param float dropout: Dropout probability \n        '''\n        super().__init__()\n        \n        self.d_model = config['d_model']\n        self.kernel_size = config['kernel_size']\n\n        self.layer_norm = nn.LayerNorm(self.d_model)\n        self.pointwise_conv_1 = nn.Conv1d(self.d_model, 2 * self.d_model, kernel_size=1)\n        self.activation_1 = nn.GLU()\n        self.depthwise_conv = nn.Conv1d(self.d_model, self.d_model, kernel_size=self.kernel_size, groups=self.d_model, padding='same')\n        self.batch_norm = nn.BatchNorm1d(self.d_model)\n        self.activation_2 = nn.SiLU()\n        self.pointwise_conv_2 = nn.Conv1d(self.d_model, self.d_model, kernel_size=1)\n        self.dropout = nn.Dropout(config['conv_module_dropout'])\n        \n        self.reset_parameters()\n\n    def forward(self, x: torch.Tensor, pad_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        '''\n        :param torch.Tensor x: (batch, time, d_model)\n        :param torch.Tensor pad_mask: (batch, time) takes True value for the positions corresponding to the padding\n        :return: (batch, time, d_model)\n        :rtype: torch.Tensor\n        '''\n        \n        x = self.layer_norm(x)\n        x = self.pointwise_conv_1(x.permute(0, 2, 1)).permute(0, 2, 1)\n        x = self.activation_1(x)\n\n        if pad_mask is not None:\n            x = x.masked_fill(pad_mask[..., None], 0.0)\n\n        x = self.depthwise_conv(x.permute(0, 2, 1))\n        x = self.batch_norm(x)\n        x = self.activation_2(x)\n        x = self.pointwise_conv_2(x).permute(0, 2, 1)\n        x = self.dropout(x)\n\n        return x\n    \n    def reset_parameters(self):\n        pw_max = self.d_model ** -0.5\n        dw_max = self.kernel_size ** -0.5\n        with torch.no_grad():\n            torch.nn.init.uniform_(self.pointwise_conv_1.weight, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.pointwise_conv_2.weight, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.depthwise_conv.weight, -dw_max, dw_max)\n            \n            torch.nn.init.uniform_(self.pointwise_conv_1.bias, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.pointwise_conv_2.bias, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.depthwise_conv.bias, -dw_max, dw_max)\n            \nclass PositionalEncoding(nn.Module):\n    def __init__(self, config):\n        \n        super().__init__()\n        emb_size = config['d_model']\n        dropout = config['emb_dropout']\n        maxlen = config['in_seq_len']\n        \n        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(0)  # (1, in_seq_len, d_model)\n\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n\n    def forward(self, x: torch.Tensor):\n        return self.dropout(x + self.pos_embedding)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T17:52:57.948812Z","iopub.execute_input":"2023-08-08T17:52:57.951093Z","iopub.status.idle":"2023-08-08T17:52:58.047936Z","shell.execute_reply.started":"2023-08-08T17:52:57.951054Z","shell.execute_reply":"2023-08-08T17:52:58.046604Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class E2STransformer(nn.Module):\n    \n    def __init__(self, config: dict, audio_paths: List[str], example_input):\n        \"\"\"\n        :param dict config: dictionart with all model parameters\n        :param List[str] audio_paths: list of audio file paths to fit PCA on\n        :param torch.tensor example_input: input to compute wavelet filters on. Should have shape (n_channels, in_seq_len)\n        \"\"\"\n        super().__init__()\n\n        self.conv_downsampling = torch.nn.Conv1d(config['n_channels'], 1, kernel_size=1) # (N, c_in, L) -> (N, 1, L)\n        self.ln = nn.LayerNorm(config['n_wvt_bins'])\n        self.ffn = nn.Linear(config['n_wvt_bins'], config['d_model'])\n        self.conv_module = ConvolutionModule(config)\n        self.positional_encoding = PositionalEncoding(config)\n        self.n_fft = config['n_fft']\n        self.hop_size = config['hop_size']\n        self.d_model = config['d_model']\n        self.transformer = torch.nn.Transformer(\n            d_model=config['d_model'],\n            nhead=config['nhead'],\n            num_encoder_layers=config['num_encoder_layers'],\n            num_decoder_layers=config['num_decoder_layers'],\n            dim_feedforward=config['dim_feedforward'],\n            dropout=config['dropout'],\n            activation=config['activation'],\n            batch_first=True\n        )\n        self.audio_sr = config['audio_sr']\n        self.compute_pca_components(audio_paths)\n        \n        # Specials\n        self.src_sos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n        self.src_eos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n        self.tgt_sos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n        self.tgt_eos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n        # self.register_parameter('src_sos', torch.Tensor(1, 1, self.d_model))\n        # self.register_parameter('src_eos', torch.Tensor(1, 1, self.d_model))\n        # self.register_parameter('tgt_sos', torch.Tensor(1, 1, self.d_model))\n        # self.register_parameter('tgt_eos', torch.Tensor(1, 1, self.d_model))\n        \n        self.reset_parameters()\n        self.get_wavelet_filters(example_input)\n        \n    def reset_parameters(self):\n        pw_max = self.d_model ** -0.5\n        with torch.no_grad():\n            torch.nn.init.uniform_(self.src_sos, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.src_eos, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.tgt_sos, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.tgt_eos, -pw_max, pw_max)\n    \n    def get_wavelet_filters(self, x):\n        \"\"\"\n        Computes Wavelet convolution weights\n        :param torch.tensor x: example input of shape (n_channels, in_seq_len)\n        \"\"\"\n        wvt_transformer = WaveletTransformTorch(\n            dt=1/config['eeg_sr'],\n            dj=config['dj'],\n            wavelet=config['wavelet'](),\n            cuda=torch.cuda.is_available()\n        )\n        _ = wvt_transformer.cwt(x)\n        self.filters = nn.ModuleList(wvt_transformer._extractor._filters)  # requires_grad: False\n        # self.register_buffer('filters', filters)\n        \n    def compute_pca_components(self, audio_paths):\n        \"\"\"\n        :param List[str] audio_paths: list of audio file paths to fit PCA on\n        \"\"\"\n        audios_srs = [torchaudio.load(path) for path in audio_paths]\n        all_audios = []\n        for audio, sr in audios_srs:\n            if sr != self.audio_sr:\n                audio = torchaudio.functional.resample(waveform=audio, orig_freq=sr, new_freq=self.audio_sr)[0]\n            all_audios.append(audio)\n        \n        all_audios = torch.cat(all_audios)\n        all_audios = torch.stft(all_audios, n_fft=self.n_fft, hop_length=self.hop_size, return_complex=True)  # (n_freq_bins, n_frames)\n        all_audios = torch.abs(all_audios).t().numpy()\n        \n        pca = PCA(n_components=self.d_model)\n        pca.fit(all_audios)\n        \n        components = torch.tensor(pca.components_)  # (d_model, n_freq_bins)\n        mean = torch.tensor(pca.mean_)  # (n_freq_bins)\n        \n        self.register_buffer('components', components)\n        self.register_buffer('mean', mean)\n        \n    def cwt(self, x):\n        \"\"\"\n        Computes continuous wavelet transform of a given tensor\n        :param torch.tensor x: input of shape (batch, n_channels, in_seq_len)\n        :return torch.tensor out: cwt result of shape (batch, n_channels, n_wvt_bins, in_seq_len)\n        \"\"\"\n\n        batch, n_channels, signal_length = x.size()\n        x = x.view(batch * n_channels, signal_length).unsqueeze(1)  # (N, 1, in_seq_len)\n\n        # x = x.type(torch.FloatTensor)\n        # x.requires_grad_(requires_grad=False)\n\n        results = [None]*len(self.filters)\n        for ind, conv in enumerate(self.filters):\n            results[ind] = conv(x)\n            \n        results = torch.stack(results)     # [n_scales,n_batch,2,t]\n        results = results.permute(1,0,2,3) # [n_batch,n_scales,2,t]\n\n        # results = torch.abs(results[:,:,0,:] + results[:,:,1,:]*1j\n        results = (results[:,:,0,:]**2 + results[:,:,1,:]**2)**0.5\n        \n        results = results.reshape(batch, n_channels, results.size(1), signal_length)\n        return results\n        \n    def prepare_src(self, x):\n        \"\"\"\n        :param torch.tensor x: input of shape (batch_size, n_channels, in_seq_len)\n        :rtype torch.tensor\n        :return out of shape (batch_size, in_seq_len, d_model)\n        \"\"\"\n        \n        # Wavelet Transform\n        out = self.cwt(x)  # (batch_size, n_channels, n_wvt_bins, in_seq_len)\n        batch_size, n_channels, n_wvt_bins, in_seq_len = out.size()\n        \n        # Convolution downsampling\n        out = out.permute(0, 3, 1, 2)  # (batch_size, in_seq_len, n_channels, n_wvt_bins)\n        out = out.reshape(batch_size * in_seq_len, n_channels, n_wvt_bins)  # (batch_size * in_seq_len, n_channels, n_wvt_bins)\n        out = self.conv_downsampling(out).squeeze(1)  # (batch_size * in_seq_len, n_wvt_bins)\n        out = out.reshape(batch_size, in_seq_len, n_wvt_bins)  # (batch_size, in_seq_len, n_wvt_bins)\n        \n        # LayerNorm & Feed Forward\n        out = self.ln(out)  # (batch_size, in_seq_len, n_wvt_bins)\n        out = self.ffn(out)  # (batch_size, in_seq_len, d_model)\n        \n        # Convolution module from https://arxiv.org/pdf/2005.08100.pdf\n        out = self.conv_module(out)  # (batch_size, in_seq_len, d_model)\n        \n        # Positional Encoding\n        out = self.positional_encoding(out)  # (batch_size, in_seq_len, d_model)\n        \n        return out\n    \n    def prepare_tgt(self, x):  # Add some audio normalization???\n        \"\"\"\n        :param torch.tensor x: input of shape (batch_size, audio_len)\n        :rtype torch.tensor\n        :return out of shape (batch_size, out_seq_len, d_model)\n        \"\"\"\n        # n_freq_bins = self.n_fft // 2 + 1\n        # out_seq_len = self.n_fft // self.hop_size + 1\n        \n        # STFT\n        out = torch.stft(x, n_fft=self.n_fft, hop_length=self.hop_size, return_complex=True)  # (batch_size, n_freq_bins, out_seq_len)\n        out = torch.abs(out.permute(0, 2, 1))  # (batch_size, out_seq_len, n_freq_bins)\n        \n        # PCA\n        out = out - self.mean\n        out = out @ self.components.t()  # (batch_size, out_seq_len, d_model)\n        return out\n        \n    def forward(self, eeg, audio):\n        \"\"\"\n        :param torch.tensor eeg: input of shape (batch_size, n_channels, in_seq_len)\n        :rtype torch.tensor\n        :return out of shape (batch_size, out_seq_len, n_freq_bins)\n        \"\"\"\n        batch_size = eeg.size(0)\n        src = self.prepare_src(eeg)  # (batch_size, in_seq_len, d_model)\n        tgt = self.prepare_tgt(audio)  # (batch_size, out_seq_len, d_model)\n        \n        # Add <sos> and <eos>\n        src = torch.cat((self.src_sos.repeat(batch_size, 1, 1), src, self.src_eos.repeat(batch_size, 1, 1)),\n                        dim=1)  # (batch_size, 1 + in_seq_len + 1, d_model)\n        tgt = torch.cat((self.tgt_sos.repeat(batch_size, 1, 1), tgt, self.tgt_eos.repeat(batch_size, 1, 1)),\n                        dim=1)  # (batch_size, 1 + out_seq_len + 1, d_model)\n        \n        # Transformer\n        causal_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(eeg.device).to(torch.bool)\n        out = self.transformer(src=src, tgt=tgt, tgt_mask=causal_mask)  # (batch_size, 1 + out_seq_len + 1, d_model)\n        # out = out[:, 1:-1, :]  # (batch_size, out_seq_len, d_model)\n        \n        # Inverse PCA\n        # out = out @ self.components  # (batch_size, out_seq_len, n_freq_bins)\n        # out = out + self.mean\n        \n        return out, tgt","metadata":{"execution":{"iopub.status.busy":"2023-08-08T17:52:58.053968Z","iopub.execute_input":"2023-08-08T17:52:58.054718Z","iopub.status.idle":"2023-08-08T17:52:58.178974Z","shell.execute_reply.started":"2023-08-08T17:52:58.054682Z","shell.execute_reply":"2023-08-08T17:52:58.177784Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Noam Annealing","metadata":{}},{"cell_type":"code","source":"class NoamAnnealing(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(\n        self, optimizer: torch.optim.Optimizer, *, \n        d_model: int, warmup_steps: int, min_lr: float = 0.0, last_epoch: int = -1\n    ):\n        \"\"\"\n        :param torch.optim.Optimizer optimizer:\n        :param int d_model: Model input dimension\n        :param int warmup_steps:\n        :param float min_lr: Lower bound for learning rate after warmup\n        :param int last_epoch:\n        \"\"\"\n        assert warmup_steps\n        \n        # It is necessary to assign all attributes *before* __init__,\n        # as class is wrapped by an inner class.\n        self.min_lr = min_lr\n        self.warmup_steps = warmup_steps\n        self.normalization = d_model ** (-0.5)\n\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if not self._get_lr_called_within_step:\n            warnings.warn(\n                \"To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\", UserWarning\n            )\n\n        step = max(1, self.last_epoch)\n        new_lrs = [\n            self._noam_annealing(initial_lr=initial_lr, step=step) \n            for initial_lr in self.base_lrs\n        ]\n        return new_lrs\n\n    def _noam_annealing(self, initial_lr: float, step: int) -> float:\n        \"\"\"Compute noam annealing learning rate \n            as described in https://arxiv.org/abs/1706.03762 Section 5.3.\n            After warmup_steps learning rate should be always greater than min_lr\n\n        :param float initial_lr: Additional multiplicative factor for learning rate\n        :param int step: Current optimization step\n        :return: Learning rate at given step\n        :rtype: float\n        \"\"\"\n        lrate = self.normalization * min(step ** (-0.5), step * self.warmup_steps ** (-1.5)) * initial_lr\n        if step > self.warmup_steps:\n            lrate = max(self.min_lr, lrate)\n        \n        return lrate","metadata":{"execution":{"iopub.status.busy":"2023-08-08T17:52:58.180638Z","iopub.execute_input":"2023-08-08T17:52:58.181397Z","iopub.status.idle":"2023-08-08T17:52:58.265371Z","shell.execute_reply.started":"2023-08-08T17:52:58.181358Z","shell.execute_reply":"2023-08-08T17:52:58.263916Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"%%capture --no-stdout --no-display\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel = E2STransformer(config, audio_paths, train_ds[0][0]).to(device)\n\nscheduler = NoamAnnealing(\n    torch.optim.Adam(model.parameters(), lr=0.2),\n    d_model=512, warmup_steps=500, min_lr=1e-5\n)\n\nlearning_rates = []\nfor _ in range(5000):\n    scheduler.step()\n    learning_rates.append(scheduler.get_last_lr()[0])\n    \nfig, ax = plt.subplots(1, 1, figsize=(10, 4))\nax.plot(learning_rates)\nax.grid(True)\nax.set_xlabel('Step')\nax.set_ylabel('Learning Rate')\nax.set_title('NoamAnnealing')\n\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:07:26.356363Z","iopub.execute_input":"2023-08-07T15:07:26.356795Z","iopub.status.idle":"2023-08-07T15:07:27.976955Z","shell.execute_reply.started":"2023-08-07T15:07:26.35676Z","shell.execute_reply":"2023-08-07T15:07:27.976035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sanity checks","metadata":{}},{"cell_type":"code","source":"eeg, audio = train_ds[15296]\neeg, audio = eeg.unsqueeze(0).to(device), audio.unsqueeze(0).to(device)\n\nout, tgt = model(eeg, audio)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:07:27.978667Z","iopub.execute_input":"2023-08-07T15:07:27.979726Z","iopub.status.idle":"2023-08-07T15:07:28.434128Z","shell.execute_reply.started":"2023-08-07T15:07:27.979689Z","shell.execute_reply":"2023-08-07T15:07:28.433108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    restored = tgt[:, 1:-1, :].squeeze()  # (out_seq_len, d_model)\n    restored = restored @ model.components  # (out_seq_len, n_freq_bins)\n    restored = (restored + model.mean).t().cpu().numpy()  # (n_freq_bins, out_seq_len)\n\nrestored = restore(restored)\nAudio(restored, rate=train_ds.sr)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:07:32.773176Z","iopub.execute_input":"2023-08-07T15:07:32.77355Z","iopub.status.idle":"2023-08-07T15:07:41.17824Z","shell.execute_reply.started":"2023-08-07T15:07:32.773517Z","shell.execute_reply":"2023-08-07T15:07:41.177337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, device, criterion, optimizer, scheduler, scaler, max_norm, description):\n        self.model = model\n        self.device = device\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.scaler = scaler\n        self.max_norm = max_norm\n        self.description = description\n        self.hist = []  # training history\n    \n    @staticmethod\n    def download_file(path, download_file_name):\n        os.chdir('/kaggle/working/')\n        zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n        command = f\"zip {zip_name} {path} -r\"\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n        if result.returncode != 0:\n            print(\"Unable to run zip command!\")\n            print(result.stderr)\n            return\n        display(FileLink(f'{download_file_name}.zip'))\n        \n    def fit(self, n_epochs, train_dl, val_dl):\n        \n        # model checkpoints directory\n        model_checkpoint_path = os.path.join(os.getcwd(), 'model_checkpoints')    \n        if not os.path.exists(model_checkpoint_path):\n            os.mkdir(model_checkpoint_path)\n        \n        model = self.model.to(self.device).train()\n        current_lr = self.scheduler.get_last_lr()[-1]\n        total_step = 0\n        \n        # mean losses\n        train_epoch_loss = []\n        val_epoch_loss = []\n                \n        for epoch in trange(n_epochs):\n            \n            train_epoch_loss = []\n            for eeg, audio in (pbar := tqdm(train_dl, total=len(train_dl))):\n\n                total_step += 1\n                \n                # Move tensors to device\n                eeg, audio = eeg.to(self.device), audio.to(self.device)\n\n                # Clear gradients\n                self.optimizer.zero_grad()\n\n                # Perform forward pass with mixed precision\n                with torch.autocast(device_type='cuda', dtype=torch.float16):\n                    # pred_encoding, encoding = model(eeg, audio)\n                    # loss = self.criterion(pred_encoding, encoding)\n                    \n                    predictions = model(eeg, audio)\n                    print(type(predictions))\n                    print(len(predictions))\n                    \n                    pred_encoding, encoding = zip(*predictions)\n                    print('pred_encoding', pred_encoding)\n                    print('encoding', encoding)\n                    loss = self.criterion(pred_encoding, encoding)\n                \n                train_epoch_loss.append(loss.item())\n                \n                # Perform backward pass\n                \n                # Scaler mutliplies gradients by 2**16 to prevent underflow in amp\n                \n                prev_scale_value = self.scaler.get_scale()\n                \n                self.scaler.scale(loss).backward()\n                self.scaler.unscale_(self.optimizer)\n\n                # Perform Gradient Clipping\n                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), self.max_norm).item()\n\n                # Perform Optimization Step\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n                \n                scale_value = self.scaler.get_scale()\n\n                # Perform Scheduler Step\n                if prev_scale_value == scale_value:\n                    self.scheduler.step()\n                # otherwise optimizer step was not performed\n                \n                current_lr = self.scheduler.get_last_lr()[-1]\n\n                # saving history\n                self.hist.append((total_step, loss.item(), self.description, 'train'))\n\n                # update tqdm info\n                pbar.set_description(\n                    f'Step: {total_step}|\\t\\\n                      Lr: {current_lr:.3e}|\\t\\\n                      GradNorm: {grad_norm:.3e}|\\t\\\n                      Train Loss: {loss.item():.3f}|\\t'\n                )\n\n                # validation\n                if total_step % (256 // train_dl.batch_size) == 0:\n\n                    val_epoch_loss = []\n                    model.eval()\n\n                    with torch.no_grad():\n                        for eeg, audio in (val_pbar := tqdm(val_dl, total=len(val_dl))):\n                            eeg, audio = eeg.to(self.device), audio.to(self.device)\n                            pred_encoding, encoding = model(eeg, audio)\n                            loss = self.criterion(pred_encoding, encoding).item()\n                            val_epoch_loss.append(loss)\n\n                            # update tqdm info\n                            val_pbar.set_description(f'Validation|\\tVal Loss: {loss:.3f}')\n                            \n                            if len(val_epoch_loss) == len(val_dl):\n                                val_pbar.set_description(f'Validation|\\tVal Loss: {np.mean(val_epoch_loss):.3f}')\n                        # saving val history\n                        self.hist.append((total_step, np.mean(val_epoch_loss), self.description, 'val'))\n\n                    model.train()\n                    \n                if total_step in [256 // train_dl.batch_size, 512 // train_dl.batch_size]:\n                    break\n                    \n                # update tqdm info after epoch\n                if len(train_epoch_loss) == len(train_dl):\n                    pbar.set_description(\n                        f'Step: {total_step}|\\t\\\n                          Lr: {current_lr:.3e}|\\t\\\n                          GradNorm: {grad_norm:.3e}|\\t\\\n                          Train Loss: {np.mean(train_epoch_loss):.3f}|\\t\\\n                          Val Loss: {np.mean(val_epoch_loss):.3f}'\n                    )\n\n        # saving model\n        self.model = model.eval()\n        torch.save(model.state_dict(), os.path.join(model_checkpoint_path, f'{self.description}.pth'))\n\n        with open(os.path.join(model_checkpoint_path, 'hist.pickle'), 'wb') as handle:\n            pickle.dump(self.hist, handle)\n            \n        # download files\n        self.download_file(os.path.join(model_checkpoint_path, f'{self.description}.pth'), 'model')\n        self.download_file(os.path.join(model_checkpoint_path, 'hist.pickle'), 'hist')\n        ","metadata":{"execution":{"iopub.status.busy":"2023-08-08T17:52:58.731437Z","iopub.execute_input":"2023-08-08T17:52:58.731917Z","iopub.status.idle":"2023-08-08T17:52:58.838298Z","shell.execute_reply.started":"2023-08-08T17:52:58.73188Z","shell.execute_reply":"2023-08-08T17:52:58.837161Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Training on a single GPU P100","metadata":{}},{"cell_type":"code","source":"model = E2STransformer(config, audio_paths, train_ds[0][0])\noptimizer = torch.optim.Adam(model.parameters(), lr=config['base_lr'])\n\ntrainer = Trainer(\n    model=model,\n    device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n    criterion=nn.MSELoss(),\n    optimizer=optimizer,\n    scheduler=NoamAnnealing(optimizer=optimizer, d_model=config['d_model'], warmup_steps=len(train_dl) // 5,\n                            min_lr=config['min_lr']),\n    scaler=torch.cuda.amp.GradScaler(),\n    max_norm=10.0,\n    description='Transformer_2gpu'\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:09:52.471713Z","iopub.execute_input":"2023-08-07T15:09:52.472134Z","iopub.status.idle":"2023-08-07T15:09:53.612271Z","shell.execute_reply.started":"2023-08-07T15:09:52.472101Z","shell.execute_reply":"2023-08-07T15:09:53.610815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrainer.fit(n_epochs=2, train_dl=train_dl(batch_size=8), val_dl=val_dl(batch_size=8))","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:09:53.61426Z","iopub.execute_input":"2023-08-07T15:09:53.614618Z","iopub.status.idle":"2023-08-07T15:17:22.273727Z","shell.execute_reply.started":"2023-08-07T15:09:53.614567Z","shell.execute_reply":"2023-08-07T15:17:22.272325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1.05s/train it\n\nCPU times: user 2min 30s, sys: 6.1 s, total: 2min 36s\n\nWall time: 7min 20s","metadata":{}},{"cell_type":"markdown","source":"## Training on 2 GPU T4 in parallel\n\n![img](https://miro.medium.com/v2/resize:fit:2000/format:webp/1*FpDHkWJhkLL7KxU01Lf9Lw.png)","metadata":{}},{"cell_type":"markdown","source":"### Batch size: 8 (4 per each GPU)","metadata":{}},{"cell_type":"code","source":"model = E2STransformer(config, audio_paths, train_ds[0][0])\nparallel_model = torch.nn.DataParallel(model)\noptimizer = torch.optim.Adam(parallel_model.parameters(), lr=config['base_lr'])\nbatch_size = 8\n\ntrainer = Trainer(\n    model=parallel_model,\n    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n    criterion=nn.MSELoss(),\n    optimizer=optimizer,\n    scheduler=NoamAnnealing(optimizer=optimizer, d_model=config['d_model'], warmup_steps=len(train_dl(batch_size=batch_size)) // 5,\n                            min_lr=config['min_lr']),\n    scaler=torch.cuda.amp.GradScaler(),\n    max_norm=10.0,\n    description='Transformer_2gpu'\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T17:16:25.391715Z","iopub.execute_input":"2023-08-08T17:16:25.392105Z","iopub.status.idle":"2023-08-08T17:16:26.461123Z","shell.execute_reply.started":"2023-08-08T17:16:25.392073Z","shell.execute_reply":"2023-08-08T17:16:26.45974Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"%%time\ntrainer.fit(n_epochs=2, train_dl=train_dl(batch_size=batch_size), val_dl=val_dl(batch_size=batch_size))","metadata":{"execution":{"iopub.status.busy":"2023-08-08T17:16:26.463157Z","iopub.execute_input":"2023-08-08T17:16:26.463538Z","iopub.status.idle":"2023-08-08T17:24:27.664599Z","shell.execute_reply.started":"2023-08-08T17:16:26.463503Z","shell.execute_reply":"2023-08-08T17:24:27.663361Z"},"trusted":true},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f5fc78a706d4700857006720384b0ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2796 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a193580da3f04942a5c89d6b191dc1db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/484 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ab860a1f07b48d0bfc3e33a599deeca"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/transformers/attention.cpp:150.)\n  return torch._native_multi_head_attention(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2796 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f012eb687c84e048cb179b8f16aff75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/484 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02d849cb6a2e4b5b9a801aa28de0a395"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/model.zip","text/html":"<a href='model.zip' target='_blank'>model.zip</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/hist.zip","text/html":"<a href='hist.zip' target='_blank'>hist.zip</a><br>"},"metadata":{}},{"name":"stdout","text":"CPU times: user 4min 55s, sys: 12.5 s, total: 5min 8s\nWall time: 8min 1s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"CPU times: user 4min 49s, sys: 11.7 s, total: 5min\n\nWall time: 7min 58s","metadata":{}},{"cell_type":"markdown","source":"### Batch size: 32 (16 per each GPU)","metadata":{}},{"cell_type":"code","source":"model = E2STransformer(config, audio_paths, train_ds[0][0])\nparallel_model = torch.nn.DataParallel(model)\noptimizer = torch.optim.Adam(parallel_model.parameters(), lr=config['base_lr'])\nbatch_size = 32\n\ntrainer = Trainer(\n    model=parallel_model,\n    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n    criterion=nn.MSELoss(),\n    optimizer=optimizer,\n    scheduler=NoamAnnealing(optimizer=optimizer, d_model=config['d_model'], warmup_steps=len(train_dl(batch_size=batch_size)) // 5,\n                            min_lr=config['min_lr']),\n    scaler=torch.cuda.amp.GradScaler(),\n    max_norm=10.0,\n    description='Transformer_2gpu'\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T16:26:07.714426Z","iopub.execute_input":"2023-08-08T16:26:07.714796Z","iopub.status.idle":"2023-08-08T16:26:09.104157Z","shell.execute_reply.started":"2023-08-08T16:26:07.714764Z","shell.execute_reply":"2023-08-08T16:26:09.102766Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"%%time\ntrainer.fit(n_epochs=2, train_dl=train_dl(batch_size=batch_size), val_dl=val_dl(batch_size=batch_size))","metadata":{"execution":{"iopub.status.busy":"2023-08-08T16:26:09.111264Z","iopub.execute_input":"2023-08-08T16:26:09.114572Z","iopub.status.idle":"2023-08-08T16:33:46.11671Z","shell.execute_reply.started":"2023-08-08T16:26:09.114516Z","shell.execute_reply":"2023-08-08T16:33:46.115653Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b7dbad5e30544cbac415c69208fa18f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/699 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"577b6b0a33ae4c10bcae2e3e3132ba68"}},"metadata":{}},{"name":"stdout","text":"Scaling Parameter Before: 65536.0\nScaling Parameter After: 65536.0\nScaling Parameter Before: 65536.0\nScaling Parameter After: 65536.0\nScaling Parameter Before: 65536.0\nScaling Parameter After: 65536.0\nScaling Parameter Before: 65536.0\nScaling Parameter After: 65536.0\nScaling Parameter Before: 65536.0\nScaling Parameter After: 65536.0\nScaling Parameter Before: 65536.0\nScaling Parameter After: 65536.0\nScaling Parameter Before: 65536.0\nScaling Parameter After: 65536.0\nScaling Parameter Before: 65536.0\nScaling Parameter After: 65536.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea9c98dabff44c70b966f5dcc36a0826"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/transformers/attention.cpp:150.)\n  return torch._native_multi_head_attention(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/699 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75652afc3e254aceb74e00d873887428"}},"metadata":{}},{"name":"stdout","text":"Scaling Parameter Before: 65536.0\nScaling Parameter After: 65536.0\nScaling Parameter Before: 65536.0\nScaling Parameter After: 65536.0\nScaling Parameter Before: 65536.0\nScaling Parameter After: 65536.0\nScaling Parameter Before: 65536.0\nScaling Parameter After: 65536.0\nScaling Parameter Before: 65536.0\nScaling Parameter After: 65536.0\nScaling Parameter Before: 65536.0\nScaling Parameter After: 65536.0\nScaling Parameter Before: 65536.0\nScaling Parameter After: 65536.0\nScaling Parameter Before: 65536.0\nScaling Parameter After: 65536.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41ff75f263294482867d7610742e883d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/model.zip","text/html":"<a href='model.zip' target='_blank'>model.zip</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/hist.zip","text/html":"<a href='hist.zip' target='_blank'>hist.zip</a><br>"},"metadata":{}},{"name":"stdout","text":"CPU times: user 3min 52s, sys: 8.9 s, total: 4min 1s\nWall time: 7min 36s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"CPU times: user 3min 52s, sys: 8.9 s, total: 4min 1s\n\nWall time: 7min 36s","metadata":{}},{"cell_type":"markdown","source":"## Balanced load on a multi-GPU machine\n\n![img](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*F6SXjBp6BCoFTZ26RKnz9A.png)","metadata":{}},{"cell_type":"code","source":"! wget https://gist.githubusercontent.com/neuralsrg/4a70580b9b8fb74f6e7739f1e9ef6ebf/raw/947ba2d0c9767591b799c63b3c550eb4df35af92/parallel.py -O /kaggle/working/parallel.py\n\nimport parallel\nfrom parallel import DataParallelModel, DataParallelCriterion","metadata":{"execution":{"iopub.status.busy":"2023-08-08T17:53:05.495318Z","iopub.execute_input":"2023-08-08T17:53:05.495716Z","iopub.status.idle":"2023-08-08T17:53:06.65478Z","shell.execute_reply.started":"2023-08-08T17:53:05.49568Z","shell.execute_reply":"2023-08-08T17:53:06.653595Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"--2023-08-08 17:53:06--  https://gist.githubusercontent.com/neuralsrg/4a70580b9b8fb74f6e7739f1e9ef6ebf/raw/947ba2d0c9767591b799c63b3c550eb4df35af92/parallel.py\nResolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\nConnecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 11413 (11K) [text/plain]\nSaving to: /kaggle/working/parallel.py\n\n/kaggle/working/par 100%[===================>]  11.15K  --.-KB/s    in 0.001s  \n\n2023-08-08 17:53:06 (21.6 MB/s) - /kaggle/working/parallel.py saved [11413/11413]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"model = E2STransformer(config, audio_paths, train_ds[0][0])\nparallel_model = DataParallelModel(model)\noptimizer = torch.optim.Adam(parallel_model.parameters(), lr=config['base_lr'])\nbatch_size = 32\n\ntrainer = Trainer(\n    model=parallel_model,\n    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n    criterion=DataParallelCriterion(nn.MSELoss()),\n    optimizer=optimizer,\n    scheduler=NoamAnnealing(optimizer=optimizer, d_model=config['d_model'], warmup_steps=len(train_dl(batch_size=batch_size)) // 5,\n                            min_lr=config['min_lr']),\n    scaler=torch.cuda.amp.GradScaler(),\n    max_norm=10.0,\n    description='Transformer_2gpu_balanced'\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T17:37:50.571697Z","iopub.execute_input":"2023-08-08T17:37:50.572085Z","iopub.status.idle":"2023-08-08T17:37:51.732553Z","shell.execute_reply.started":"2023-08-08T17:37:50.572053Z","shell.execute_reply":"2023-08-08T17:37:51.731128Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = E2STransformer(config, audio_paths, train_ds[0][0])\nmodel = DataParallelModel(model).to(device).train()\n\ncriterion = DataParallelCriterion(nn.MSELoss())\ndl = train_dl(batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T17:53:44.653503Z","iopub.execute_input":"2023-08-08T17:53:44.653938Z","iopub.status.idle":"2023-08-08T17:53:46.120664Z","shell.execute_reply.started":"2023-08-08T17:53:44.653905Z","shell.execute_reply":"2023-08-08T17:53:46.117917Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"for eeg, audio in tqdm(dl, total=len(dl)):\n\n    # Move tensors to device\n    eeg, audio = eeg.to(device), audio.to(device)\n\n    # Perform forward pass with mixed precision\n    with torch.autocast(device_type='cuda', dtype=torch.float16):\n        # pred_encoding, encoding = model(eeg, audio)\n        # loss = self.criterion(pred_encoding, encoding)\n\n        predictions = model(eeg, audio)  # list of len 2\n\n        pred_encoding, encoding = zip(*predictions)\n        encoding = parallel.gather(encoding, target_device=device)\n        loss = criterion(pred_encoding, encoding)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrainer.fit(n_epochs=2, train_dl=train_dl(batch_size=batch_size), val_dl=val_dl(batch_size=batch_size))","metadata":{"execution":{"iopub.status.busy":"2023-08-08T17:37:51.734663Z","iopub.execute_input":"2023-08-08T17:37:51.735008Z","iopub.status.idle":"2023-08-08T17:38:07.857715Z","shell.execute_reply.started":"2023-08-08T17:37:51.734975Z","shell.execute_reply":"2023-08-08T17:38:07.856503Z"},"trusted":true},"execution_count":65,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8584322a036a436996e83ebb33a0b6f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/699 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20c153d6eb18416394606e3a750194bd"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","File \u001b[0;32m<timed eval>:1\u001b[0m\n","Cell \u001b[0;32mIn[63], line 58\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, n_epochs, train_dl, val_dl)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Perform forward pass with mixed precision\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16):\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# pred_encoding, encoding = model(eeg, audio)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# loss = self.criterion(pred_encoding, encoding)\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43meeg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(predictions))\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(predictions))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:89\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     87\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 89\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n","\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_28/3154955271.py\", line 183, in forward\n    out = self.transformer(src=src, tgt=tgt, tgt_mask=causal_mask)  # (batch_size, 1 + out_seq_len + 1, d_model)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 146, in forward\n    output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 360, in forward\n    output = mod(output, memory, tgt_mask=tgt_mask,\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 699, in forward\n    x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 717, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py\", line 1189, in forward\n    attn_output, attn_output_weights = F.multi_head_attention_forward(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 5188, in multi_head_attention_forward\n    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 4777, in _in_projection_packed\n    kv_proj = linear(k, w_kv, b_kv)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 14.76 GiB total capacity; 13.64 GiB already allocated; 5.75 MiB free; 13.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"],"ename":"OutOfMemoryError","evalue":"Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_28/3154955271.py\", line 183, in forward\n    out = self.transformer(src=src, tgt=tgt, tgt_mask=causal_mask)  # (batch_size, 1 + out_seq_len + 1, d_model)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 146, in forward\n    output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 360, in forward\n    output = mod(output, memory, tgt_mask=tgt_mask,\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 699, in forward\n    x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 717, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py\", line 1189, in forward\n    attn_output, attn_output_weights = F.multi_head_attention_forward(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 5188, in multi_head_attention_forward\n    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 4777, in _in_projection_packed\n    kv_proj = linear(k, w_kv, b_kv)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 14.76 GiB total capacity; 13.64 GiB already allocated; 5.75 MiB free; 13.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}