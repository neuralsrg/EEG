{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EEG to Speech Transformer","metadata":{}},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:24:46.712634Z","iopub.execute_input":"2023-08-05T12:24:46.713406Z","iopub.status.idle":"2023-08-05T12:24:46.792814Z","shell.execute_reply.started":"2023-08-05T12:24:46.713342Z","shell.execute_reply":"2023-08-05T12:24:46.791775Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"! rm -rf /kaggle/working/PyTorchWavelets\n! git clone https://github.com/neuralsrg/PyTorchWavelets.git","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:24:47.020215Z","iopub.execute_input":"2023-08-05T12:24:47.020659Z","iopub.status.idle":"2023-08-05T12:24:50.256476Z","shell.execute_reply.started":"2023-08-05T12:24:47.020621Z","shell.execute_reply":"2023-08-05T12:24:50.254942Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'PyTorchWavelets'...\nremote: Enumerating objects: 123, done.\u001b[K\nremote: Counting objects: 100% (21/21), done.\u001b[K\nremote: Compressing objects: 100% (21/21), done.\u001b[K\nremote: Total 123 (delta 11), reused 2 (delta 0), pack-reused 102\u001b[K\nReceiving objects: 100% (123/123), 1.08 MiB | 19.71 MiB/s, done.\nResolving deltas: 100% (60/60), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport sys\nimport random\nsys.path.append('/kaggle/working/PyTorchWavelets/')\nimport subprocess\n\nimport math\nimport pickle\nfrom glob import glob\nfrom functools import partial\nfrom tqdm.notebook import tqdm, trange\nfrom typing import List, Tuple, Optional\nfrom IPython.display import Audio, FileLink, display, clear_output\n\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchaudio\nimport torch.nn as nn\ntorch.manual_seed(1337)\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom wavelets_pytorch.transform import WaveletTransformTorch\nfrom wavelets_pytorch.wavelets import Morlet\n\nfrom sklearn.decomposition import PCA\n# clear_output()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:24:55.035204Z","iopub.execute_input":"2023-08-05T12:24:55.035585Z","iopub.status.idle":"2023-08-05T12:24:59.478226Z","shell.execute_reply.started":"2023-08-05T12:24:55.035551Z","shell.execute_reply":"2023-08-05T12:24:59.477148Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"base = '/kaggle/input/internal-speech-recognition/Vartanov/audios'\n\nA = \"A.wav\"\nB = \"B.wav\"\nF = \"F.wav\"\nG = \"G.wav\"\nM = \"M.wav\"\nR = \"R.wav\"\nU = \"U.wav\"\n\nBa = \"Ba.wav\"\nBu = \"Bu.wav\"\nFa = \"Fa.wav\"\nFu = \"Fu.wav\"\nGa = \"Ga.wav\"\nGu = \"Gu.wav\"\nMa = \"Ma.wav\"\nMu = \"Mu.wav\"\nRa = \"Ra.wav\"\nRu = \"Ru.wav\"\n\nBiblioteka = \"St1.wav\"\nRaketa = \"St2.wav\"\nKurier = \"St3.wav\"\nOgrada = \"St4.wav\"\nHaketa = \"St5.wav\"\n\nphonemes_m3_labels = {\n    12: os.path.join(base, \"phonemes\", A),\n    22: os.path.join(base, \"phonemes\", A),\n    13: os.path.join(base, \"phonemes\", B),\n    23: os.path.join(base, \"phonemes\", B),\n    14: os.path.join(base, \"phonemes\", F),\n    24: os.path.join(base, \"phonemes\", F),\n    15: os.path.join(base, \"phonemes\", G),\n    25: os.path.join(base, \"phonemes\", G),\n    16: os.path.join(base, \"phonemes\", M),\n    26: os.path.join(base, \"phonemes\", M),\n    17: os.path.join(base, \"phonemes\", R),\n    27: os.path.join(base, \"phonemes\", R),\n    18: os.path.join(base, \"phonemes\", U),\n    28: os.path.join(base, \"phonemes\", U)\n}\n\nphonemes_m4_labels = {\n    1: os.path.join(base, \"phonemes\", A),\n    11: os.path.join(base, \"phonemes\", A),\n    2: os.path.join(base, \"phonemes\", B),\n    12: os.path.join(base, \"phonemes\", B),\n    3: os.path.join(base, \"phonemes\", F),\n    13: os.path.join(base, \"phonemes\", F),\n    4: os.path.join(base, \"phonemes\", G),\n    14: os.path.join(base, \"phonemes\", G),\n    5: os.path.join(base, \"phonemes\", M),\n    15: os.path.join(base, \"phonemes\", M),\n    6: os.path.join(base, \"phonemes\", R),\n    16: os.path.join(base, \"phonemes\", R),\n    7: os.path.join(base, \"phonemes\", U),\n    17: os.path.join(base, \"phonemes\", U)\n}\n\nsyllables_labels = {\n    1: os.path.join(base, \"syllables\", Ba),\n    11: os.path.join(base, \"syllables\", Ba),\n    2: os.path.join(base, \"syllables\", Fa),\n    12: os.path.join(base, \"syllables\", Fa),\n    3: os.path.join(base, \"syllables\", Ga),\n    13: os.path.join(base, \"syllables\", Ga),\n    4: os.path.join(base, \"syllables\", Ma),\n    14: os.path.join(base, \"syllables\", Ma),\n    5: os.path.join(base, \"syllables\", Ra),\n    15: os.path.join(base, \"syllables\", Ra),\n    6: os.path.join(base, \"syllables\", Bu),\n    16: os.path.join(base, \"syllables\", Bu),\n    7: os.path.join(base, \"syllables\", Ru),\n    17: os.path.join(base, \"syllables\", Ru),\n    8: os.path.join(base, \"syllables\", Mu),\n    18: os.path.join(base, \"syllables\", Mu),\n    9: os.path.join(base, \"syllables\", Fu),\n    19: os.path.join(base, \"syllables\", Fu),\n    10: os.path.join(base, \"syllables\", Gu),\n    20: os.path.join(base, \"syllables\", Gu)\n}\n\nwords_labels = {\n    11: os.path.join(base, \"words\", Biblioteka),\n    21: os.path.join(base, \"words\", Biblioteka),\n    12: os.path.join(base, \"words\", Raketa),\n    22: os.path.join(base, \"words\", Raketa),\n    13: os.path.join(base, \"words\", Kurier),\n    23: os.path.join(base, \"words\", Kurier),\n    14: os.path.join(base, \"words\", Ograda),\n    24: os.path.join(base, \"words\", Ograda),\n    15: os.path.join(base, \"words\", Haketa),\n    25: os.path.join(base, \"words\", Haketa)\n}\n\nsections = [\"syllables\", \"phonemes_m3\", \"phonemes_m4\", \"words\"]\n\naudio_map = {\n    \"syllables\": syllables_labels,\n    \"phonemes_m3\": phonemes_m3_labels,\n    \"phonemes_m4\": phonemes_m4_labels,\n    \"words\": words_labels\n}\n\nconfig = {\n    \n    'path': '/kaggle/input/internal-speech-recognition/Vartanov/feather',\n    'audio_maps': audio_map,\n    \n    # EEG\n    'eeg_sr': 1006,\n    'n_channels': 63,\n    'in_seq_len': 1145,\n    \n    # Wavelet Transform\n    'dj': 0.8,  # wavelet scaling factor\n    'wavelet': Morlet(),\n    'n_wvt_bins': 12,\n    \n    # Audio\n    'audio_sr': 44100,\n    'sound_size': 50176,\n    \n    # STFT Patameters\n    'n_fft': 2048,\n    'hop_size': 512,\n    \n    # Model\n    \n    # Convolution Module\n    'kernel_size': 31,\n    'conv_module_dropout': .1,\n    \n    # Positional Encoding\n    'emb_dropout': .1,\n    \n    # Transformer\n    'd_model': 512,\n    'nhead': 8,\n    'num_encoder_layers': 6,\n    'num_decoder_layers': 6,\n    'dim_feedforward': 2048,\n    'dropout': 0.1,\n    'activation': 'relu',\n    \n    # Scheduler\n    'base_lr': 0.2,\n    'min_lr': 1e-5,\n}\n\naudio_paths = glob('/kaggle/input/internal-speech-recognition/Vartanov/audios/syllables/*.wav') + \\\n              glob('/kaggle/input/internal-speech-recognition/Vartanov/audios/words/*.wav')","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:25:01.018578Z","iopub.execute_input":"2023-08-05T12:25:01.019194Z","iopub.status.idle":"2023-08-05T12:25:01.126337Z","shell.execute_reply.started":"2023-08-05T12:25:01.019161Z","shell.execute_reply":"2023-08-05T12:25:01.125343Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# spectrogram to sound\n\ndef restore(D: np.array, frame_size=config['n_fft'], hop_length=config['hop_size'], epochs: int = 2, window: str = 'hann'):\n\n    length = (D.shape[1] + 1) * hop_length  # (D.shape[1] - 1 + 2) * hop_length\n    D = np.concatenate((np.zeros((D.shape[0], 1)), D, np.zeros((D.shape[0], 1))), axis=1)\n    mag, _ = librosa.magphase(D)\n    phase = np.exp(1.j * np.random.uniform(0., 2*np.pi, size=mag.shape))\n    x_ = librosa.istft(mag * phase, hop_length=hop_length, center=True, window=window, length=length)\n\n    for i in range(epochs):\n        _, phase = librosa.magphase(librosa.stft(x_, n_fft=frame_size, hop_length=hop_length, center=True,\n                                                 window=window))\n        x_ = librosa.istft(mag * phase, hop_length=hop_length, center=True, window=window, length=length)\n\n    return x_[hop_length:-hop_length]","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:25:01.682331Z","iopub.execute_input":"2023-08-05T12:25:01.682685Z","iopub.status.idle":"2023-08-05T12:25:01.742543Z","shell.execute_reply.started":"2023-08-05T12:25:01.682655Z","shell.execute_reply":"2023-08-05T12:25:01.741389Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class EEGDataset(Dataset):\n    def __init__(self, path: str, audio_maps: dict, fragment_length: int = 2012, partition_size: int = 32,\n                 sample_rate: int = 44100, sound_channel: int = 1, val_ratio: float = 0.15, seed: int = 1337):\n        '''\n        path: path to sections (folders)\n        audio_maps: two-level map: section names -> labels -> audio_paths\n        fragment_lengtht: length of fragment after label\n        partition_size: number of nonzero labels in each csv file\n        '''\n        super().__init__()\n        rnd = random.Random(seed)\n        \n        self.sections = os.listdir(path)\n        rnd.shuffle(self.sections)\n        assert set(self.sections) == set(audio_maps.keys()), \"Sections must be the same!\"\n        self.audio_maps = audio_maps\n        \n        all_paths = []\n        for sec in self.sections:\n            l = os.listdir(os.path.join(path, sec))\n            rnd.shuffle(l)\n            all_paths.append([os.path.join(path, sec, file) for file in l])\n                \n        # all_paths = [[os.path.join(path, sec, file) for file in sorted(os.listdir(os.path.join(path, sec)))] for sec in self.sections]\n        num_all_files = [len(elem) for elem in all_paths]\n        splits = [int(elem * val_ratio) for elem in num_all_files]\n        \n        self.val_paths = [sec_paths[:split] for sec_paths, split in zip(all_paths, splits)]\n        self.paths = [sec_paths[split:] for sec_paths, split in zip(all_paths, splits)]\n        \n        self.sec_num_files = [len(elem) for elem in self.paths]\n        self.sec_cumnum = np.cumsum(self.sec_num_files) * partition_size\n        self.total_num_files = sum(self.sec_num_files)\n        \n        self.sec_num_val_files = [len(elem) for elem in self.val_paths]\n        self.sec_val_cumnum = np.cumsum(self.sec_num_val_files) * partition_size\n        self.total_num_val_files = sum(self.sec_num_val_files)\n        \n        self.partition_size = partition_size\n        self.fragment_length = fragment_length\n        self.sr = sample_rate\n        self.sound_channel = sound_channel\n        self.val_mode = False\n        \n    def __len__(self) -> int:\n        num = self.total_num_val_files if self.val_mode else self.total_num_files\n        return num * self.partition_size\n    \n    def set_val_mode(self, mode: bool):\n        '''\n        Switch between train/val subsets\n        '''\n        assert mode in [True, False], \"Incorrect mode type!\"\n        self.val_mode = mode\n        return self\n    \n    def to_section(self, idx: int) -> Tuple[int, int]:\n        '''\n        Get file section and inner index by its absolute index\n        '''\n        cumnum = self.sec_val_cumnum if self.val_mode else self.sec_cumnum\n        section = np.where(idx < cumnum)[0][0]\n        section_idx = idx if (section == 0) else (idx - cumnum[section - 1])\n        return section, section_idx\n    \n    def get_audio(self, section: str, label: int) -> torch.Tensor:\n        '''\n        Get audio by section and corresponding label\n        '''\n        section_name = self.sections[section]\n        audio, current_sr = torchaudio.load(self.audio_maps[section_name][label])\n        audio = torchaudio.functional.resample(audio, orig_freq=current_sr, new_freq=self.sr)\n        return audio[self.sound_channel]\n    \n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        '''\n        int idx: file ID\n        return: EEG fragment with its corresponding audio\n        '''\n        section, section_idx = self.to_section(idx)\n        paths_source = self.val_paths if self.val_mode else self.paths\n        file_path = paths_source[section][section_idx // self.partition_size]\n        \n        start = (section_idx % self.partition_size) * self.fragment_length\n        end = start + self.fragment_length\n        \n        data = pd.read_feather(file_path).to_numpy()\n        x, label = torch.tensor(data[start:end, 1:]), data[start, 0].astype(int)\n        \n        audio = self.get_audio(section, label)\n        \n        # Cut model inputs so that they match desirable sizes\n        E, S = config['in_seq_len'], config['sound_size']\n        x = x[:E] if x.size(0) >= E else nn.functional.pad(x, (0, E-x.size(0)), value=0)\n        audio = audio[:S] if audio.size(0) >= S else nn.functional.pad(audio, (0, S-audio.size(0)), value=0)\n        \n        return x.t(), audio","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:25:02.632423Z","iopub.execute_input":"2023-08-05T12:25:02.632796Z","iopub.status.idle":"2023-08-05T12:25:02.705821Z","shell.execute_reply.started":"2023-08-05T12:25:02.632765Z","shell.execute_reply":"2023-08-05T12:25:02.704858Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class ConvolutionModule(torch.nn.Module):\n    def __init__(self, config):\n        '''\n        :param int d_model: Input dimension\n        :param int kernel_size: Kernel size of Depthwise Convolution\n        :param float dropout: Dropout probability \n        '''\n        super().__init__()\n        \n        self.d_model = config['d_model']\n        self.kernel_size = config['kernel_size']\n\n        self.layer_norm = nn.LayerNorm(self.d_model)\n        self.pointwise_conv_1 = nn.Conv1d(self.d_model, 2 * self.d_model, kernel_size=1)\n        self.activation_1 = nn.GLU()\n        self.depthwise_conv = nn.Conv1d(self.d_model, self.d_model, kernel_size=self.kernel_size, groups=self.d_model, padding='same')\n        self.batch_norm = nn.BatchNorm1d(self.d_model)\n        self.activation_2 = nn.SiLU()\n        self.pointwise_conv_2 = nn.Conv1d(self.d_model, self.d_model, kernel_size=1)\n        self.dropout = nn.Dropout(config['conv_module_dropout'])\n        \n        self.reset_parameters()\n\n    def forward(self, x: torch.Tensor, pad_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        '''\n        :param torch.Tensor x: (batch, time, d_model)\n        :param torch.Tensor pad_mask: (batch, time) takes True value for the positions corresponding to the padding\n        :return: (batch, time, d_model)\n        :rtype: torch.Tensor\n        '''\n        \n        x = self.layer_norm(x)\n        x = self.pointwise_conv_1(x.permute(0, 2, 1)).permute(0, 2, 1)\n        x = self.activation_1(x)\n\n        if pad_mask is not None:\n            x = x.masked_fill(pad_mask[..., None], 0.0)\n\n        x = self.depthwise_conv(x.permute(0, 2, 1))\n        x = self.batch_norm(x)\n        x = self.activation_2(x)\n        x = self.pointwise_conv_2(x).permute(0, 2, 1)\n        x = self.dropout(x)\n\n        return x\n    \n    def reset_parameters(self):\n        pw_max = self.d_model ** -0.5\n        dw_max = self.kernel_size ** -0.5\n        with torch.no_grad():\n            torch.nn.init.uniform_(self.pointwise_conv_1.weight, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.pointwise_conv_2.weight, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.depthwise_conv.weight, -dw_max, dw_max)\n            \n            torch.nn.init.uniform_(self.pointwise_conv_1.bias, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.pointwise_conv_2.bias, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.depthwise_conv.bias, -dw_max, dw_max)\n            \nclass PositionalEncoding(nn.Module):\n    def __init__(self, config):\n        \n        super().__init__()\n        emb_size = config['d_model']\n        dropout = config['emb_dropout']\n        maxlen = config['in_seq_len']\n        \n        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(0)  # (1, in_seq_len, d_model)\n\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n\n    def forward(self, x: torch.Tensor):\n        return self.dropout(x + self.pos_embedding)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:25:05.097487Z","iopub.execute_input":"2023-08-05T12:25:05.097847Z","iopub.status.idle":"2023-08-05T12:25:05.166735Z","shell.execute_reply.started":"2023-08-05T12:25:05.097818Z","shell.execute_reply":"2023-08-05T12:25:05.165691Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class E2STransformer(nn.Module):\n    \n    def __init__(self, config: dict, audio_paths: List[str]):\n        \"\"\"\n        :param dict config: dictionart with all model parameters\n        :param List[str] audio_paths: list of audio file paths to fit PCA on\n        \"\"\"\n        super().__init__()\n        self.wvt_transformer = WaveletTransformTorch(\n            dt=1/config['eeg_sr'],\n            dj=config['dj'],\n            wavelet=config['wavelet'],\n            cuda=torch.cuda.is_available()\n        )\n\n        self.conv_downsampling = torch.nn.Conv1d(config['n_channels'], 1, kernel_size=1) # (N, c_in, L) -> (N, 1, L)\n        self.ln = nn.LayerNorm(config['n_wvt_bins'])\n        self.ffn = nn.Linear(config['n_wvt_bins'], config['d_model'])\n        self.conv_module = ConvolutionModule(config)\n        self.positional_encoding = PositionalEncoding(config)\n        self.n_fft = config['n_fft']\n        self.hop_size = config['hop_size']\n        self.d_model = config['d_model']\n        self.transformer = torch.nn.Transformer(\n            d_model=config['d_model'],\n            nhead=config['nhead'],\n            num_encoder_layers=config['num_encoder_layers'],\n            num_decoder_layers=config['num_decoder_layers'],\n            dim_feedforward=config['dim_feedforward'],\n            dropout=config['dropout'],\n            activation=config['activation'],\n            batch_first=True\n        )\n        self.audio_sr = config['audio_sr']\n        self.compute_pca_components(audio_paths)\n        \n        # Specials\n        self.src_sos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n        self.src_eos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n        self.tgt_sos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n        self.tgt_eos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n        # self.register_parameter('src_sos', torch.Tensor(1, 1, self.d_model))\n        # self.register_parameter('src_eos', torch.Tensor(1, 1, self.d_model))\n        # self.register_parameter('tgt_sos', torch.Tensor(1, 1, self.d_model))\n        # self.register_parameter('tgt_eos', torch.Tensor(1, 1, self.d_model))\n        \n        self.reset_parameters()\n        \n    def reset_parameters(self):\n        pw_max = self.d_model ** -0.5\n        with torch.no_grad():\n            torch.nn.init.uniform_(self.src_sos, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.src_eos, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.tgt_sos, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.tgt_eos, -pw_max, pw_max)\n        \n    def compute_pca_components(self, audio_paths):\n        \"\"\"\n        :param List[str] audio_paths: list of audio file paths to fit PCA on\n        \"\"\"\n        audios_srs = [torchaudio.load(path) for path in audio_paths]\n        all_audios = []\n        for audio, sr in audios_srs:\n            if sr != self.audio_sr:\n                audio = torchaudio.functional.resample(waveform=audio, orig_freq=sr, new_freq=self.audio_sr)[0]\n            all_audios.append(audio)\n        \n        all_audios = torch.cat(all_audios)\n        all_audios = torch.stft(all_audios, n_fft=self.n_fft, hop_length=self.hop_size, return_complex=True)  # (n_freq_bins, n_frames)\n        all_audios = torch.abs(all_audios).t().numpy()\n        \n        pca = PCA(n_components=self.d_model)\n        pca.fit(all_audios)\n        \n        components = torch.tensor(pca.components_)  # (d_model, n_freq_bins)\n        mean = torch.tensor(pca.mean_)  # (n_freq_bins)\n        \n        self.register_buffer('components', components)\n        self.register_buffer('mean', mean)\n        \n    def prepare_src(self, x):\n        \"\"\"\n        :param torch.tensor x: input of shape (batch_size, n_channels, in_seq_len)\n        :rtype torch.tensor\n        :return out of shape (batch_size, in_seq_len, d_model)\n        \"\"\"\n        with torch.no_grad():\n            \n            # Reshaping for wavelet transform which takes as input tensor of shape (N, in_seq_len)\n            batch_size, n_channels, in_seq_len = x.size()\n            out = x.view(batch_size * n_channels, in_seq_len)  # (N, in_seq_len)\n            \n            # Wavelet transform\n            out = self.wvt_transformer.cwt(out)  # (N, n_wvt_bins, in_seq_len)\n            \n            n_wvt_bins = out.size(1)\n            out = out.view(batch_size, n_channels, n_wvt_bins, in_seq_len)  # (batch_size, n_channels, n_wvt_bins, in_seq_len)\n          \n        # Convolution downsampling\n        out = out.permute(0, 3, 1, 2)  # (batch_size, in_seq_len, n_channels, n_wvt_bins)\n        out = out.reshape(batch_size * in_seq_len, n_channels, n_wvt_bins)  # (batch_size * in_seq_len, n_channels, n_wvt_bins)\n        out = self.conv_downsampling(out).squeeze(1)  # (batch_size * in_seq_len, n_wvt_bins)\n        out = out.reshape(batch_size, in_seq_len, n_wvt_bins)  # (batch_size, in_seq_len, n_wvt_bins)\n        \n        # LayerNorm & Feed Forward\n        out = self.ln(out)  # (batch_size, in_seq_len, n_wvt_bins)\n        out = self.ffn(out)  # (batch_size, in_seq_len, d_model)\n        \n        # Convolution module from https://arxiv.org/pdf/2005.08100.pdf\n        out = self.conv_module(out)  # (batch_size, in_seq_len, d_model)\n        \n        # Positional Encoding\n        out = self.positional_encoding(out)  # (batch_size, in_seq_len, d_model)\n        \n        return out\n    \n    def prepare_tgt(self, x):  # Add some audio normalization???\n        \"\"\"\n        :param torch.tensor x: input of shape (batch_size, audio_len)\n        :rtype torch.tensor\n        :return out of shape (batch_size, out_seq_len, d_model)\n        \"\"\"\n        # n_freq_bins = self.n_fft // 2 + 1\n        # out_seq_len = self.n_fft // self.hop_size + 1\n        \n        # STFT\n        out = torch.stft(x, n_fft=self.n_fft, hop_length=self.hop_size, return_complex=True)  # (batch_size, n_freq_bins, out_seq_len)\n        out = torch.abs(out.permute(0, 2, 1))  # (batch_size, out_seq_len, n_freq_bins)\n        \n        # PCA\n        out = out - self.mean\n        out = out @ self.components.t()  # (batch_size, out_seq_len, d_model)\n        return out\n        \n    def forward(self, eeg, audio):\n        \"\"\"\n        :param torch.tensor eeg: input of shape (batch_size, n_channels, in_seq_len)\n        :rtype torch.tensor\n        :return out of shape (batch_size, out_seq_len, n_freq_bins)\n        \"\"\"\n        batch_size = eeg.size(0)\n        src = self.prepare_src(eeg)  # (batch_size, in_seq_len, d_model)\n        tgt = self.prepare_tgt(audio)  # (batch_size, out_seq_len, d_model)\n        \n        # Add <sos> and <eos>\n        src = torch.cat((self.src_sos.repeat(batch_size, 1, 1), src, self.src_eos.repeat(batch_size, 1, 1)),\n                        dim=1)  # (batch_size, 1 + in_seq_len + 1, d_model)\n        tgt = torch.cat((self.tgt_sos.repeat(batch_size, 1, 1), tgt, self.tgt_eos.repeat(batch_size, 1, 1)),\n                        dim=1)  # (batch_size, 1 + out_seq_len + 1, d_model)\n        \n        # Transformer\n        causal_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(eeg.device)\n        out = self.transformer(src=src, tgt=tgt, tgt_mask=causal_mask)  # (batch_size, 1 + out_seq_len + 1, d_model)\n        # out = out[:, 1:-1, :]  # (batch_size, out_seq_len, d_model)\n        \n        # Inverse PCA\n        # out = out @ self.components  # (batch_size, out_seq_len, n_freq_bins)\n        # out = out + self.mean\n        \n        return out, tgt","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:25:07.992203Z","iopub.execute_input":"2023-08-05T12:25:07.993263Z","iopub.status.idle":"2023-08-05T12:25:08.073008Z","shell.execute_reply.started":"2023-08-05T12:25:07.993214Z","shell.execute_reply":"2023-08-05T12:25:08.071874Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Noam Annealing","metadata":{}},{"cell_type":"code","source":"class NoamAnnealing(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(\n        self, optimizer: torch.optim.Optimizer, *, \n        d_model: int, warmup_steps: int, min_lr: float = 0.0, last_epoch: int = -1\n    ):\n        \"\"\"\n        :param torch.optim.Optimizer optimizer:\n        :param int d_model: Model input dimension\n        :param int warmup_steps:\n        :param float min_lr: Lower bound for learning rate after warmup\n        :param int last_epoch:\n        \"\"\"\n        assert warmup_steps\n        \n        # It is necessary to assign all attributes *before* __init__,\n        # as class is wrapped by an inner class.\n        self.min_lr = min_lr\n        self.warmup_steps = warmup_steps\n        self.normalization = d_model ** (-0.5)\n\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if not self._get_lr_called_within_step:\n            warnings.warn(\n                \"To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\", UserWarning\n            )\n\n        step = max(1, self.last_epoch)\n        new_lrs = [\n            self._noam_annealing(initial_lr=initial_lr, step=step) \n            for initial_lr in self.base_lrs\n        ]\n        return new_lrs\n\n    def _noam_annealing(self, initial_lr: float, step: int) -> float:\n        \"\"\"Compute noam annealing learning rate \n            as described in https://arxiv.org/abs/1706.03762 Section 5.3.\n            After warmup_steps learning rate should be always greater than min_lr\n\n        :param float initial_lr: Additional multiplicative factor for learning rate\n        :param int step: Current optimization step\n        :return: Learning rate at given step\n        :rtype: float\n        \"\"\"\n        lrate = self.normalization * min(step ** (-0.5), step * self.warmup_steps ** (-1.5)) * initial_lr\n        if step > self.warmup_steps:\n            lrate = max(self.min_lr, lrate)\n        \n        return lrate","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:25:10.346887Z","iopub.execute_input":"2023-08-05T12:25:10.347579Z","iopub.status.idle":"2023-08-05T12:25:10.408225Z","shell.execute_reply.started":"2023-08-05T12:25:10.347544Z","shell.execute_reply":"2023-08-05T12:25:10.407211Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"%%capture --no-stdout --no-display\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel = E2STransformer(config, audio_paths).to(device)\n\nscheduler = NoamAnnealing(\n    torch.optim.Adam(model.parameters(), lr=0.2),\n    d_model=512, warmup_steps=500, min_lr=1e-5\n)\n\nlearning_rates = []\nfor _ in range(5000):\n    scheduler.step()\n    learning_rates.append(scheduler.get_last_lr()[0])\n    \nfig, ax = plt.subplots(1, 1, figsize=(10, 4))\nax.plot(learning_rates)\nax.grid(True)\nax.set_xlabel('Step')\nax.set_ylabel('Learning Rate')\nax.set_title('NoamAnnealing')\n\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-04T17:35:00.884197Z","iopub.execute_input":"2023-08-04T17:35:00.884703Z","iopub.status.idle":"2023-08-04T17:35:01.446999Z","shell.execute_reply.started":"2023-08-04T17:35:00.884644Z","shell.execute_reply":"2023-08-04T17:35:01.445997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataloaders","metadata":{}},{"cell_type":"code","source":"train_ds = EEGDataset(path=config['path'], audio_maps=config['audio_maps'])\nval_ds = EEGDataset(path=config['path'], audio_maps=config['audio_maps']).set_val_mode(True)\n\ntrain_dl = DataLoader(train_ds, 8, shuffle=True, num_workers=2)\nval_dl = DataLoader(val_ds, 8, shuffle=False, num_workers=2)\n\nprint(f'{\"Train dataset len:\": <20} {len(train_ds)};\\t{\"Validation datset len:\": <25} {len(val_ds)};')\nprint(f'{\"Num train batches:\": <20} {len(train_dl)};\\t{\"Num validation batches:\": <25} {len(val_dl)};')","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:25:40.291562Z","iopub.execute_input":"2023-08-05T12:25:40.291929Z","iopub.status.idle":"2023-08-05T12:25:40.363305Z","shell.execute_reply.started":"2023-08-05T12:25:40.291900Z","shell.execute_reply":"2023-08-05T12:25:40.362164Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Train dataset len:   22368;\tValidation datset len:    3872;\nNum train batches:   2796;\tNum validation batches:   484;\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Sanity checks","metadata":{}},{"cell_type":"code","source":"eeg, audio = train_ds[15296]\neeg, audio = eeg.unsqueeze(0).to(device), audio.unsqueeze(0).to(device)\n\nout, tgt = model(eeg, audio)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T17:35:01.920233Z","iopub.execute_input":"2023-08-04T17:35:01.920782Z","iopub.status.idle":"2023-08-04T17:35:07.797034Z","shell.execute_reply.started":"2023-08-04T17:35:01.920748Z","shell.execute_reply":"2023-08-04T17:35:07.796033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    restored = tgt[:, 1:-1, :].squeeze()  # (out_seq_len, d_model)\n    restored = restored @ model.components  # (out_seq_len, n_freq_bins)\n    restored = (restored + model.mean).t().cpu().numpy()  # (n_freq_bins, out_seq_len)\n\nrestored = restore(restored)\nAudio(restored, rate=train_ds.sr)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T17:35:07.798500Z","iopub.execute_input":"2023-08-04T17:35:07.798890Z","iopub.status.idle":"2023-08-04T17:35:17.947096Z","shell.execute_reply.started":"2023-08-04T17:35:07.798855Z","shell.execute_reply":"2023-08-04T17:35:17.946101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, device, criterion, optimizer, scheduler, scaler, max_norm, description):\n        self.model = model\n        self.device = device\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.scheduler = scheduler(optimizer=self.optimizer)\n        self.scaler = scaler\n        self.max_norm = max_norm\n        self.description = description\n        self.hist = []  # training history\n    \n    @staticmethod\n    def download_file(path, download_file_name):\n        os.chdir('/kaggle/working/')\n        zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n        command = f\"zip {zip_name} {path} -r\"\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n        if result.returncode != 0:\n            print(\"Unable to run zip command!\")\n            print(result.stderr)\n            return\n        display(FileLink(f'{download_file_name}.zip'))\n        \n    def fit(self, n_epochs, train_dl, val_dl):\n        \n        # model checkpoints directory\n        model_checkpoint_path = os.path.join(os.getcwd(), 'model_checkpoints')    \n        if not os.path.exists(model_checkpoint_path):\n            os.mkdir(model_checkpoint_path)\n        \n        model = self.model.to(self.device).train()\n        current_lr = self.scheduler.get_last_lr()[-1]\n        total_step = 0\n        \n        # mean losses\n        train_epoch_loss = []\n        val_epoch_loss = []\n                \n        for epoch in trange(n_epochs):\n            \n            train_epoch_loss = []\n            for eeg, audio in (pbar := tqdm(train_dl, total=len(train_dl))):\n\n                total_step += 1\n                \n                # Move tensors to device\n                eeg, audio = eeg.to(self.device), audio.to(self.device)\n\n                # Clear gradients\n                self.optimizer.zero_grad()\n\n                # Perform forward pass with mixed precision\n                with torch.autocast(device_type='cuda', dtype=torch.float16):\n                    pred_encoding, encoding = model(eeg, audio)\n                    loss = self.criterion(pred_encoding, encoding)\n\n                train_epoch_loss.append(loss.item())\n                \n                # Perform backward pass\n                self.scaler.scale(loss).backward()\n                self.scaler.unscale_(self.optimizer)\n\n                # Perform Gradient Clipping\n                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), self.max_norm).item()\n\n                # Perform Optimization Step\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n\n                # Perform Scheduler Step\n                self.scheduler.step()\n                current_lr = self.scheduler.get_last_lr()[-1]\n\n                # saving history\n                self.hist.append((total_step, loss.item(), self.description, 'train'))\n\n                # update tqdm info\n                pbar.set_description(\n                    f'Step: {total_step}|\\t\\\n                      Lr: {current_lr:.3e}|\\t\\\n                      GradNorm: {grad_norm:.3e}|\\t\\\n                      Train Loss: {loss.item():.3f}|\\t'\n                )\n\n                # validation\n                if total_step % 800 == 0:\n\n                    val_epoch_loss = []\n                    model.eval()\n\n                    with torch.no_grad():\n                        for eeg, audio in (val_pbar := tqdm(val_dl, total=len(val_dl))):\n                            eeg, audio = eeg.to(self.device), audio.to(self.device)\n                            pred_encoding, encoding = model(eeg, audio)\n                            loss = self.criterion(pred_encoding, encoding).item()\n                            val_epoch_loss.append(loss)\n\n                            # update tqdm info\n                            val_pbar.set_description(f'Validation|\\tVal Loss: {loss:.3f}')\n                        \n                        val_pbar.set_description(f'Validation|\\tVal Loss: {np.mean(val_epoch_loss):.3f}')\n                        # saving val history\n                        self.hist.append((total_step, val_loss, self.description, 'val'))\n\n                    model.train()\n                    \n                if total_step in [4, 8]:\n                    break\n                    \n            # update tqdm info after epoch\n            pbar.set_description(\n                f'Step: {total_step}|\\t\\\n                  Lr: {current_lr:.3e}|\\t\\\n                  GradNorm: {grad_norm:.3e}|\\t\\\n                  Train Loss: {np.mean(train_epoch_loss):.3f}|\\t\\\n                  Val Loss: {np.mean(val_epoch_loss):.3f}'\n            )\n\n        # saving model\n        self.model = model.eval()\n        torch.save(model.state_dict(), os.path.join(model_checkpoint_path, f'{self.description}.pth'))\n\n        with open(os.path.join(model_checkpoint_path, 'hist.pickle'), 'wb') as handle:\n            pickle.dump(self.hist, handle)\n            \n        # download files\n        self.download_file(os.path.join(model_checkpoint_path, f'{self.description}.pth'), 'model')\n        self.download_file(os.path.join(model_checkpoint_path, 'hist.pickle'), 'hist')\n        ","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:25:45.298541Z","iopub.execute_input":"2023-08-05T12:25:45.298904Z","iopub.status.idle":"2023-08-05T12:25:45.372710Z","shell.execute_reply.started":"2023-08-05T12:25:45.298873Z","shell.execute_reply":"2023-08-05T12:25:45.371130Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Training on a single GPU P100","metadata":{}},{"cell_type":"code","source":"model = E2STransformer(config, audio_paths)\noptimizer = torch.optim.Adam(model.parameters(), lr=config['base_lr'])\n\ntrainer = Trainer(\n    model=model,\n    device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n    criterion=nn.MSELoss(),\n    optimizer=optimizer,\n    scheduler=partial(NoamAnnealing, d_model=config['d_model'], warmup_steps=len(train_dl) // 5,\n                      min_lr=config['min_lr']),\n    scaler=torch.cuda.amp.GradScaler(),\n    max_norm=10.0, \n    description='Transformer_gpu0'\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:27:15.129649Z","iopub.execute_input":"2023-08-05T12:27:15.130033Z","iopub.status.idle":"2023-08-05T12:27:16.180462Z","shell.execute_reply.started":"2023-08-05T12:27:15.130003Z","shell.execute_reply":"2023-08-05T12:27:16.179105Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"%%time\ntrainer.fit(n_epochs=2, train_dl=train_dl, val_dl=val_dl)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:27:32.737516Z","iopub.execute_input":"2023-08-05T12:27:32.737892Z","iopub.status.idle":"2023-08-05T12:28:08.432193Z","shell.execute_reply.started":"2023-08-05T12:27:32.737861Z","shell.execute_reply":"2023-08-05T12:28:08.431102Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d89905c4ac247d1a05a08c4f1127cea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2796 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2b1910b6ab54f7e914bb8e132433209"}},"metadata":{}},{"name":"stderr","text":"/kaggle/working/PyTorchWavelets/wavelets_pytorch/transform.py:317: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/EmptyTensor.cpp:31.)\n  cwt = torch.abs(cwt[:,:,0,:] + cwt[:,:,1,:]*1j)\n/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2796 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96bd21115c6b4f7e8cbe028b65a14b80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/model.zip","text/html":"<a href='model.zip' target='_blank'>model.zip</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/hist.zip","text/html":"<a href='hist.zip' target='_blank'>hist.zip</a><br>"},"metadata":{}},{"name":"stdout","text":"CPU times: user 7.15 s, sys: 1.92 s, total: 9.08 s\nWall time: 35.6 s\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}