{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/neuralsrg/e2s-transformer?scriptVersionId=139532537\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# EEG to Speech Transformer","metadata":{}},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:52:16.645592Z","iopub.execute_input":"2023-08-10T14:52:16.645948Z","iopub.status.idle":"2023-08-10T14:52:16.694512Z","shell.execute_reply.started":"2023-08-10T14:52:16.645916Z","shell.execute_reply":"2023-08-10T14:52:16.693553Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"! rm -rf /kaggle/working/PyTorchWavelets\n! git clone https://github.com/neuralsrg/PyTorchWavelets.git\n# ! git clone -b develop --single-branch https://github.com/neuralsrg/PyTorchWavelets.git","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:52:16.768489Z","iopub.execute_input":"2023-08-10T14:52:16.768758Z","iopub.status.idle":"2023-08-10T14:52:19.456016Z","shell.execute_reply.started":"2023-08-10T14:52:16.768734Z","shell.execute_reply":"2023-08-10T14:52:19.454927Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'PyTorchWavelets'...\nremote: Enumerating objects: 108, done.\u001b[K\nremote: Counting objects: 100% (8/8), done.\u001b[K\nremote: Compressing objects: 100% (7/7), done.\u001b[K\nremote: Total 108 (delta 1), reused 5 (delta 1), pack-reused 100\u001b[K\nReceiving objects: 100% (108/108), 1.08 MiB | 7.31 MiB/s, done.\nResolving deltas: 100% (48/48), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport sys\nimport random\nsys.path.append('/kaggle/working/PyTorchWavelets/')\nimport subprocess\n\nimport math\nimport pickle\nfrom glob import glob\nfrom functools import partial\nfrom tqdm.notebook import tqdm, trange\nfrom typing import List, Tuple, Optional\nfrom IPython.display import Audio, FileLink, display, clear_output\n\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchaudio\nimport torch.nn as nn\ntorch.manual_seed(1337)\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom wavelets_pytorch.transform import WaveletTransformTorch\nfrom wavelets_pytorch.wavelets import Morlet\n\nfrom sklearn.decomposition import PCA\n# clear_output()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:52:19.458699Z","iopub.execute_input":"2023-08-10T14:52:19.459781Z","iopub.status.idle":"2023-08-10T14:52:23.740389Z","shell.execute_reply.started":"2023-08-10T14:52:19.459742Z","shell.execute_reply":"2023-08-10T14:52:23.739378Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"base = '/kaggle/input/internal-speech-recognition/Vartanov/audios'\n\nA = \"A.wav\"\nB = \"B.wav\"\nF = \"F.wav\"\nG = \"G.wav\"\nM = \"M.wav\"\nR = \"R.wav\"\nU = \"U.wav\"\n\nBa = \"Ba.wav\"\nBu = \"Bu.wav\"\nFa = \"Fa.wav\"\nFu = \"Fu.wav\"\nGa = \"Ga.wav\"\nGu = \"Gu.wav\"\nMa = \"Ma.wav\"\nMu = \"Mu.wav\"\nRa = \"Ra.wav\"\nRu = \"Ru.wav\"\n\nBiblioteka = \"St1.wav\"\nRaketa = \"St2.wav\"\nKurier = \"St3.wav\"\nOgrada = \"St4.wav\"\nHaketa = \"St5.wav\"\n\nphonemes_m3_labels = {\n    12: os.path.join(base, \"phonemes\", A),\n    22: os.path.join(base, \"phonemes\", A),\n    13: os.path.join(base, \"phonemes\", B),\n    23: os.path.join(base, \"phonemes\", B),\n    14: os.path.join(base, \"phonemes\", F),\n    24: os.path.join(base, \"phonemes\", F),\n    15: os.path.join(base, \"phonemes\", G),\n    25: os.path.join(base, \"phonemes\", G),\n    16: os.path.join(base, \"phonemes\", M),\n    26: os.path.join(base, \"phonemes\", M),\n    17: os.path.join(base, \"phonemes\", R),\n    27: os.path.join(base, \"phonemes\", R),\n    18: os.path.join(base, \"phonemes\", U),\n    28: os.path.join(base, \"phonemes\", U)\n}\n\nphonemes_m4_labels = {\n    1: os.path.join(base, \"phonemes\", A),\n    11: os.path.join(base, \"phonemes\", A),\n    2: os.path.join(base, \"phonemes\", B),\n    12: os.path.join(base, \"phonemes\", B),\n    3: os.path.join(base, \"phonemes\", F),\n    13: os.path.join(base, \"phonemes\", F),\n    4: os.path.join(base, \"phonemes\", G),\n    14: os.path.join(base, \"phonemes\", G),\n    5: os.path.join(base, \"phonemes\", M),\n    15: os.path.join(base, \"phonemes\", M),\n    6: os.path.join(base, \"phonemes\", R),\n    16: os.path.join(base, \"phonemes\", R),\n    7: os.path.join(base, \"phonemes\", U),\n    17: os.path.join(base, \"phonemes\", U)\n}\n\nsyllables_labels = {\n    1: os.path.join(base, \"syllables\", Ba),\n    11: os.path.join(base, \"syllables\", Ba),\n    2: os.path.join(base, \"syllables\", Fa),\n    12: os.path.join(base, \"syllables\", Fa),\n    3: os.path.join(base, \"syllables\", Ga),\n    13: os.path.join(base, \"syllables\", Ga),\n    4: os.path.join(base, \"syllables\", Ma),\n    14: os.path.join(base, \"syllables\", Ma),\n    5: os.path.join(base, \"syllables\", Ra),\n    15: os.path.join(base, \"syllables\", Ra),\n    6: os.path.join(base, \"syllables\", Bu),\n    16: os.path.join(base, \"syllables\", Bu),\n    7: os.path.join(base, \"syllables\", Ru),\n    17: os.path.join(base, \"syllables\", Ru),\n    8: os.path.join(base, \"syllables\", Mu),\n    18: os.path.join(base, \"syllables\", Mu),\n    9: os.path.join(base, \"syllables\", Fu),\n    19: os.path.join(base, \"syllables\", Fu),\n    10: os.path.join(base, \"syllables\", Gu),\n    20: os.path.join(base, \"syllables\", Gu)\n}\n\nwords_labels = {\n    11: os.path.join(base, \"words\", Biblioteka),\n    21: os.path.join(base, \"words\", Biblioteka),\n    12: os.path.join(base, \"words\", Raketa),\n    22: os.path.join(base, \"words\", Raketa),\n    13: os.path.join(base, \"words\", Kurier),\n    23: os.path.join(base, \"words\", Kurier),\n    14: os.path.join(base, \"words\", Ograda),\n    24: os.path.join(base, \"words\", Ograda),\n    15: os.path.join(base, \"words\", Haketa),\n    25: os.path.join(base, \"words\", Haketa)\n}\n\nsections = [\"syllables\", \"phonemes_m3\", \"phonemes_m4\", \"words\"]\n\naudio_map = {\n    \"syllables\": syllables_labels,\n    \"phonemes_m3\": phonemes_m3_labels,\n    \"phonemes_m4\": phonemes_m4_labels,\n    \"words\": words_labels\n}\n\nconfig = {\n    \n    'path': '/kaggle/input/internal-speech-recognition/Vartanov/feather',\n    'audio_maps': audio_map,\n    \n    # EEG\n    'eeg_sr': 1006,\n    'n_channels': 63,\n    'in_seq_len': 1145,\n    \n    # Wavelet Transform\n    'dj': 0.8,  # wavelet scaling factor\n    'wavelet': Morlet,\n    'n_wvt_bins': 12,\n    \n    # Audio\n    'audio_sr': 44100,\n    'sound_size': 50176,\n    \n    # STFT Patameters\n    'n_fft': 2048,\n    'hop_size': 512,\n    \n    # Model\n    \n    # Convolution Module\n    'kernel_size': 31,\n    'conv_module_dropout': .1,\n    \n    # Positional Encoding\n    'emb_dropout': .1,\n    \n    # Transformer\n    'd_model': 512,\n    'nhead': 8,\n    'num_encoder_layers': 6,\n    'num_decoder_layers': 6,\n    'dim_feedforward': 2048,\n    'dropout': 0.1,\n    'activation': 'relu',\n    \n    # Scheduler\n    'base_lr': 0.2,\n    'min_lr': 1e-5,\n}\n\naudio_paths = glob('/kaggle/input/internal-speech-recognition/Vartanov/audios/syllables/*.wav') + \\\n              glob('/kaggle/input/internal-speech-recognition/Vartanov/audios/words/*.wav')","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:31:48.63243Z","iopub.execute_input":"2023-08-10T14:31:48.633138Z","iopub.status.idle":"2023-08-10T14:31:48.755321Z","shell.execute_reply.started":"2023-08-10T14:31:48.6331Z","shell.execute_reply":"2023-08-10T14:31:48.754335Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# spectrogram to sound\n\ndef restore(D: np.array, frame_size=config['n_fft'], hop_length=config['hop_size'], epochs: int = 2, window: str = 'hann'):\n\n    length = (D.shape[1] + 1) * hop_length  # (D.shape[1] - 1 + 2) * hop_length\n    D = np.concatenate((np.zeros((D.shape[0], 1)), D, np.zeros((D.shape[0], 1))), axis=1)\n    mag, _ = librosa.magphase(D)\n    phase = np.exp(1.j * np.random.uniform(0., 2*np.pi, size=mag.shape))\n    x_ = librosa.istft(mag * phase, hop_length=hop_length, center=True, window=window, length=length)\n\n    for i in range(epochs):\n        _, phase = librosa.magphase(librosa.stft(x_, n_fft=frame_size, hop_length=hop_length, center=True,\n                                                 window=window))\n        x_ = librosa.istft(mag * phase, hop_length=hop_length, center=True, window=window, length=length)\n\n    return x_[hop_length:-hop_length]","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:31:48.756914Z","iopub.execute_input":"2023-08-10T14:31:48.757286Z","iopub.status.idle":"2023-08-10T14:31:48.818139Z","shell.execute_reply.started":"2023-08-10T14:31:48.757251Z","shell.execute_reply":"2023-08-10T14:31:48.817271Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class EEGDataset(Dataset):\n    def __init__(self, path: str, audio_maps: dict, fragment_length: int = 2012, partition_size: int = 32,\n                 sample_rate: int = 44100, sound_channel: int = 1, val_ratio: float = 0.15, seed: int = 1337):\n        '''\n        path: path to sections (folders)\n        audio_maps: two-level map: section names -> labels -> audio_paths\n        fragment_lengtht: length of fragment after label\n        partition_size: number of nonzero labels in each csv file\n        '''\n        super().__init__()\n        rnd = random.Random(seed)\n        \n        self.sections = os.listdir(path)\n        rnd.shuffle(self.sections)\n        assert set(self.sections) == set(audio_maps.keys()), \"Sections must be the same!\"\n        self.audio_maps = audio_maps\n        \n        all_paths = []\n        for sec in self.sections:\n            l = os.listdir(os.path.join(path, sec))\n            rnd.shuffle(l)\n            all_paths.append([os.path.join(path, sec, file) for file in l])\n                \n        # all_paths = [[os.path.join(path, sec, file) for file in sorted(os.listdir(os.path.join(path, sec)))] for sec in self.sections]\n        num_all_files = [len(elem) for elem in all_paths]\n        splits = [int(elem * val_ratio) for elem in num_all_files]\n        \n        self.val_paths = [sec_paths[:split] for sec_paths, split in zip(all_paths, splits)]\n        self.paths = [sec_paths[split:] for sec_paths, split in zip(all_paths, splits)]\n        \n        self.sec_num_files = [len(elem) for elem in self.paths]\n        self.sec_cumnum = np.cumsum(self.sec_num_files) * partition_size\n        self.total_num_files = sum(self.sec_num_files)\n        \n        self.sec_num_val_files = [len(elem) for elem in self.val_paths]\n        self.sec_val_cumnum = np.cumsum(self.sec_num_val_files) * partition_size\n        self.total_num_val_files = sum(self.sec_num_val_files)\n        \n        self.partition_size = partition_size\n        self.fragment_length = fragment_length\n        self.sr = sample_rate\n        self.sound_channel = sound_channel\n        self.val_mode = False\n        \n    def __len__(self) -> int:\n        num = self.total_num_val_files if self.val_mode else self.total_num_files\n        return num * self.partition_size\n    \n    def set_val_mode(self, mode: bool):\n        '''\n        Switch between train/val subsets\n        '''\n        assert mode in [True, False], \"Incorrect mode type!\"\n        self.val_mode = mode\n        return self\n    \n    def to_section(self, idx: int) -> Tuple[int, int]:\n        '''\n        Get file section and inner index by its absolute index\n        '''\n        cumnum = self.sec_val_cumnum if self.val_mode else self.sec_cumnum\n        section = np.where(idx < cumnum)[0][0]\n        section_idx = idx if (section == 0) else (idx - cumnum[section - 1])\n        return section, section_idx\n    \n    def get_audio(self, section: str, label: int) -> torch.Tensor:\n        '''\n        Get audio by section and corresponding label\n        '''\n        section_name = self.sections[section]\n        audio, current_sr = torchaudio.load(self.audio_maps[section_name][label])\n        audio = torchaudio.functional.resample(audio, orig_freq=current_sr, new_freq=self.sr)\n        return audio[self.sound_channel]\n    \n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        '''\n        int idx: file ID\n        return: EEG fragment with its corresponding audio\n        '''\n        section, section_idx = self.to_section(idx)\n        paths_source = self.val_paths if self.val_mode else self.paths\n        file_path = paths_source[section][section_idx // self.partition_size]\n        \n        start = (section_idx % self.partition_size) * self.fragment_length\n        end = start + self.fragment_length\n        \n        data = pd.read_feather(file_path).to_numpy()\n        x, label = torch.tensor(data[start:end, 1:]), data[start, 0].astype(int)\n        \n        audio = self.get_audio(section, label)\n        \n        # Cut model inputs so that they match desirable sizes\n        E, S = config['in_seq_len'], config['sound_size']\n        x = x[:E] if x.size(0) >= E else nn.functional.pad(x, (0, E-x.size(0)), value=0)\n        audio = audio[:S] if audio.size(0) >= S else nn.functional.pad(audio, (0, S-audio.size(0)), value=0)\n        \n        x = x.t()  # (n_channels, in_seq_len)\n        \n        return x.float(), audio.float()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:31:48.822931Z","iopub.execute_input":"2023-08-10T14:31:48.823472Z","iopub.status.idle":"2023-08-10T14:31:48.899272Z","shell.execute_reply.started":"2023-08-10T14:31:48.823423Z","shell.execute_reply":"2023-08-10T14:31:48.898272Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Dataloaders","metadata":{}},{"cell_type":"code","source":"train_ds = EEGDataset(path=config['path'], audio_maps=config['audio_maps'])\nval_ds = EEGDataset(path=config['path'], audio_maps=config['audio_maps']).set_val_mode(True)\n\n# train_dl = DataLoader(train_ds, 8, shuffle=True, num_workers=2)\n# val_dl = DataLoader(val_ds, 8, shuffle=False, num_workers=2)\ntrain_dl = partial(DataLoader, dataset=train_ds, shuffle=True, num_workers=2)\nval_dl = partial(DataLoader, dataset=val_ds, num_workers=2)\n\nbatch_size = 8\nprint(f'Batch size: {batch_size}')\nprint(f'{\"Train dataset len:\": <20} {len(train_ds)};\\t{\"Validation datset len:\": <25} {len(val_ds)};')\nprint(f'{\"Num train batches:\": <20} {len(train_dl(batch_size=batch_size))};\\t{\"Num validation batches:\": <25} {len(val_dl(batch_size=batch_size))};')","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:31:48.900692Z","iopub.execute_input":"2023-08-10T14:31:48.902144Z","iopub.status.idle":"2023-08-10T14:31:49.306331Z","shell.execute_reply.started":"2023-08-10T14:31:48.902107Z","shell.execute_reply":"2023-08-10T14:31:49.305271Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Batch size: 8\nTrain dataset len:   22368;\tValidation datset len:    3872;\nNum train batches:   2796;\tNum validation batches:   484;\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class ConvolutionModule(torch.nn.Module):\n    def __init__(self, config):\n        '''\n        :param int d_model: Input dimension\n        :param int kernel_size: Kernel size of Depthwise Convolution\n        :param float dropout: Dropout probability \n        '''\n        super().__init__()\n        \n        self.d_model = config['d_model']\n        self.kernel_size = config['kernel_size']\n\n        self.layer_norm = nn.LayerNorm(self.d_model)\n        self.pointwise_conv_1 = nn.Conv1d(self.d_model, 2 * self.d_model, kernel_size=1)\n        self.activation_1 = nn.GLU()\n        self.depthwise_conv = nn.Conv1d(self.d_model, self.d_model, kernel_size=self.kernel_size, groups=self.d_model, padding='same')\n        self.batch_norm = nn.BatchNorm1d(self.d_model)\n        self.activation_2 = nn.SiLU()\n        self.pointwise_conv_2 = nn.Conv1d(self.d_model, self.d_model, kernel_size=1)\n        self.dropout = nn.Dropout(config['conv_module_dropout'])\n        \n        self.reset_parameters()\n\n    def forward(self, x: torch.Tensor, pad_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        '''\n        :param torch.Tensor x: (batch, time, d_model)\n        :param torch.Tensor pad_mask: (batch, time) takes True value for the positions corresponding to the padding\n        :return: (batch, time, d_model)\n        :rtype: torch.Tensor\n        '''\n        \n        x = self.layer_norm(x)\n        x = self.pointwise_conv_1(x.permute(0, 2, 1)).permute(0, 2, 1)\n        x = self.activation_1(x)\n\n        if pad_mask is not None:\n            x = x.masked_fill(pad_mask[..., None], 0.0)\n\n        x = self.depthwise_conv(x.permute(0, 2, 1))\n        x = self.batch_norm(x)\n        x = self.activation_2(x)\n        x = self.pointwise_conv_2(x).permute(0, 2, 1)\n        x = self.dropout(x)\n\n        return x\n    \n    def reset_parameters(self):\n        pw_max = self.d_model ** -0.5\n        dw_max = self.kernel_size ** -0.5\n        with torch.no_grad():\n            torch.nn.init.uniform_(self.pointwise_conv_1.weight, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.pointwise_conv_2.weight, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.depthwise_conv.weight, -dw_max, dw_max)\n            \n            torch.nn.init.uniform_(self.pointwise_conv_1.bias, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.pointwise_conv_2.bias, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.depthwise_conv.bias, -dw_max, dw_max)\n            \nclass PositionalEncoding(nn.Module):\n    def __init__(self, config):\n        \n        super().__init__()\n        emb_size = config['d_model']\n        dropout = config['emb_dropout']\n        maxlen = config['in_seq_len']\n        \n        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(0)  # (1, in_seq_len, d_model)\n\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n\n    def forward(self, x: torch.Tensor):\n        return self.dropout(x + self.pos_embedding)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:31:49.308044Z","iopub.execute_input":"2023-08-10T14:31:49.308503Z","iopub.status.idle":"2023-08-10T14:31:49.382283Z","shell.execute_reply.started":"2023-08-10T14:31:49.308458Z","shell.execute_reply":"2023-08-10T14:31:49.381354Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class E2STransformer(nn.Module):\n    \n    def __init__(self, config: dict, audio_paths: List[str], example_input):\n        \"\"\"\n        :param dict config: dictionart with all model parameters\n        :param List[str] audio_paths: list of audio file paths to fit PCA on\n        :param torch.tensor example_input: input to compute wavelet filters on. Should have shape (n_channels, in_seq_len)\n        \"\"\"\n        super().__init__()\n\n        self.conv_downsampling = torch.nn.Conv1d(config['n_channels'], 1, kernel_size=1) # (N, c_in, L) -> (N, 1, L)\n        self.ln = nn.LayerNorm(config['n_wvt_bins'])\n        self.ffn = nn.Linear(config['n_wvt_bins'], config['d_model'])\n        self.conv_module = ConvolutionModule(config)\n        self.positional_encoding = PositionalEncoding(config)\n        self.n_fft = config['n_fft']\n        self.hop_size = config['hop_size']\n        self.d_model = config['d_model']\n        self.transformer = torch.nn.Transformer(\n            d_model=config['d_model'],\n            nhead=config['nhead'],\n            num_encoder_layers=config['num_encoder_layers'],\n            num_decoder_layers=config['num_decoder_layers'],\n            dim_feedforward=config['dim_feedforward'],\n            dropout=config['dropout'],\n            activation=config['activation'],\n            batch_first=True\n        )\n        self.audio_sr = config['audio_sr']\n        self.compute_pca_components(audio_paths)\n        \n        # Specials\n        self.src_sos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n        self.src_eos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n        self.tgt_sos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n        self.tgt_eos = nn.Parameter(torch.Tensor(1, 1, self.d_model))\n        # self.register_parameter('src_sos', torch.Tensor(1, 1, self.d_model))\n        # self.register_parameter('src_eos', torch.Tensor(1, 1, self.d_model))\n        # self.register_parameter('tgt_sos', torch.Tensor(1, 1, self.d_model))\n        # self.register_parameter('tgt_eos', torch.Tensor(1, 1, self.d_model))\n        \n        self.reset_parameters()\n        self.get_wavelet_filters(example_input)\n        \n    def reset_parameters(self):\n        pw_max = self.d_model ** -0.5\n        with torch.no_grad():\n            torch.nn.init.uniform_(self.src_sos, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.src_eos, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.tgt_sos, -pw_max, pw_max)\n            torch.nn.init.uniform_(self.tgt_eos, -pw_max, pw_max)\n    \n    def get_wavelet_filters(self, x):\n        \"\"\"\n        Computes Wavelet convolution weights\n        :param torch.tensor x: example input of shape (n_channels, in_seq_len)\n        \"\"\"\n        wvt_transformer = WaveletTransformTorch(\n            dt=1/config['eeg_sr'],\n            dj=config['dj'],\n            wavelet=config['wavelet'](),\n            cuda=torch.cuda.is_available()\n        )\n        _ = wvt_transformer.cwt(x)\n        self.filters = nn.ModuleList(wvt_transformer._extractor._filters)  # requires_grad: False\n        # self.register_buffer('filters', filters)\n        \n    def compute_pca_components(self, audio_paths):\n        \"\"\"\n        :param List[str] audio_paths: list of audio file paths to fit PCA on\n        \"\"\"\n        audios_srs = [torchaudio.load(path) for path in audio_paths]\n        all_audios = []\n        for audio, sr in audios_srs:\n            if sr != self.audio_sr:\n                audio = torchaudio.functional.resample(waveform=audio, orig_freq=sr, new_freq=self.audio_sr)[0]\n            all_audios.append(audio)\n        \n        all_audios = torch.cat(all_audios)\n        all_audios = torch.stft(all_audios, n_fft=self.n_fft, hop_length=self.hop_size, return_complex=True)  # (n_freq_bins, n_frames)\n        all_audios = torch.abs(all_audios).t().numpy()\n        \n        pca = PCA(n_components=self.d_model)\n        pca.fit(all_audios)\n        \n        components = torch.tensor(pca.components_)  # (d_model, n_freq_bins)\n        mean = torch.tensor(pca.mean_)  # (n_freq_bins)\n        \n        self.register_buffer('components', components)\n        self.register_buffer('mean', mean)\n        \n    def cwt(self, x):\n        \"\"\"\n        Computes continuous wavelet transform of a given tensor\n        :param torch.tensor x: input of shape (batch, n_channels, in_seq_len)\n        :return torch.tensor out: cwt result of shape (batch, n_channels, n_wvt_bins, in_seq_len)\n        \"\"\"\n\n        batch, n_channels, signal_length = x.size()\n        x = x.view(batch * n_channels, signal_length).unsqueeze(1)  # (N, 1, in_seq_len)\n\n        # x = x.type(torch.FloatTensor)\n        # x.requires_grad_(requires_grad=False)\n\n        results = [None]*len(self.filters)\n        for ind, conv in enumerate(self.filters):\n            results[ind] = conv(x)\n            \n        results = torch.stack(results)     # [n_scales,n_batch,2,t]\n        results = results.permute(1,0,2,3) # [n_batch,n_scales,2,t]\n\n        # results = torch.abs(results[:,:,0,:] + results[:,:,1,:]*1j\n        results = (results[:,:,0,:]**2 + results[:,:,1,:]**2)**0.5\n        \n        results = results.reshape(batch, n_channels, results.size(1), signal_length)\n        return results\n        \n    def prepare_src(self, x):\n        \"\"\"\n        :param torch.tensor x: input of shape (batch_size, n_channels, in_seq_len)\n        :rtype torch.tensor\n        :return out of shape (batch_size, in_seq_len, d_model)\n        \"\"\"\n        \n        # Wavelet Transform\n        out = self.cwt(x)  # (batch_size, n_channels, n_wvt_bins, in_seq_len)\n        batch_size, n_channels, n_wvt_bins, in_seq_len = out.size()\n        \n        # Convolution downsampling\n        out = out.permute(0, 3, 1, 2)  # (batch_size, in_seq_len, n_channels, n_wvt_bins)\n        out = out.reshape(batch_size * in_seq_len, n_channels, n_wvt_bins)  # (batch_size * in_seq_len, n_channels, n_wvt_bins)\n        out = self.conv_downsampling(out).squeeze(1)  # (batch_size * in_seq_len, n_wvt_bins)\n        out = out.reshape(batch_size, in_seq_len, n_wvt_bins)  # (batch_size, in_seq_len, n_wvt_bins)\n        \n        # LayerNorm & Feed Forward\n        out = self.ln(out)  # (batch_size, in_seq_len, n_wvt_bins)\n        out = self.ffn(out)  # (batch_size, in_seq_len, d_model)\n        \n        # Convolution module from https://arxiv.org/pdf/2005.08100.pdf\n        out = self.conv_module(out)  # (batch_size, in_seq_len, d_model)\n        \n        # Positional Encoding\n        out = self.positional_encoding(out)  # (batch_size, in_seq_len, d_model)\n        \n        return out\n    \n    def prepare_tgt(self, x):  # Add some audio normalization???\n        \"\"\"\n        :param torch.tensor x: input of shape (batch_size, audio_len)\n        :rtype torch.tensor\n        :return out of shape (batch_size, out_seq_len, d_model)\n        \"\"\"\n        # n_freq_bins = self.n_fft // 2 + 1\n        # out_seq_len = self.n_fft // self.hop_size + 1\n        \n        # STFT\n        out = torch.stft(x, n_fft=self.n_fft, hop_length=self.hop_size, return_complex=True)  # (batch_size, n_freq_bins, out_seq_len)\n        out = torch.abs(out.permute(0, 2, 1))  # (batch_size, out_seq_len, n_freq_bins)\n        \n        # PCA\n        out = out - self.mean\n        out = out @ self.components.t()  # (batch_size, out_seq_len, d_model)\n        return out\n        \n    def forward(self, eeg, audio):\n        \"\"\"\n        :param torch.tensor eeg: input of shape (batch_size, n_channels, in_seq_len)\n        :rtype torch.tensor\n        :return out of shape (batch_size, out_seq_len, n_freq_bins)\n        \"\"\"\n        batch_size = eeg.size(0)\n        src = self.prepare_src(eeg)  # (batch_size, in_seq_len, d_model)\n        tgt = self.prepare_tgt(audio)  # (batch_size, out_seq_len, d_model)\n        \n        # Add <sos> and <eos>\n        src = torch.cat((self.src_sos.repeat(batch_size, 1, 1), src, self.src_eos.repeat(batch_size, 1, 1)),\n                        dim=1)  # (batch_size, 1 + in_seq_len + 1, d_model)\n        tgt = torch.cat((self.tgt_sos.repeat(batch_size, 1, 1), tgt, self.tgt_eos.repeat(batch_size, 1, 1)),\n                        dim=1)  # (batch_size, 1 + out_seq_len + 1, d_model)\n        \n        # Transformer\n        causal_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(eeg.device).type(torch.bool)\n        out = self.transformer(src=src, tgt=tgt, tgt_mask=causal_mask)  # (batch_size, 1 + out_seq_len + 1, d_model)\n        # out = out[:, 1:-1, :]  # (batch_size, out_seq_len, d_model)\n        \n        # Inverse PCA\n        # out = out @ self.components  # (batch_size, out_seq_len, n_freq_bins)\n        # out = out + self.mean\n        \n        return out, tgt","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:31:49.38421Z","iopub.execute_input":"2023-08-10T14:31:49.384604Z","iopub.status.idle":"2023-08-10T14:31:49.473828Z","shell.execute_reply.started":"2023-08-10T14:31:49.384569Z","shell.execute_reply":"2023-08-10T14:31:49.472627Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Noam Annealing","metadata":{}},{"cell_type":"code","source":"class NoamAnnealing(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(\n        self, optimizer: torch.optim.Optimizer, *, \n        d_model: int, warmup_steps: int, min_lr: float = 0.0, last_epoch: int = -1\n    ):\n        \"\"\"\n        :param torch.optim.Optimizer optimizer:\n        :param int d_model: Model input dimension\n        :param int warmup_steps:\n        :param float min_lr: Lower bound for learning rate after warmup\n        :param int last_epoch:\n        \"\"\"\n        assert warmup_steps\n        \n        # It is necessary to assign all attributes *before* __init__,\n        # as class is wrapped by an inner class.\n        self.min_lr = min_lr\n        self.warmup_steps = warmup_steps\n        self.normalization = d_model ** (-0.5)\n\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if not self._get_lr_called_within_step:\n            warnings.warn(\n                \"To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\", UserWarning\n            )\n\n        step = max(1, self.last_epoch)\n        new_lrs = [\n            self._noam_annealing(initial_lr=initial_lr, step=step) \n            for initial_lr in self.base_lrs\n        ]\n        return new_lrs\n\n    def _noam_annealing(self, initial_lr: float, step: int) -> float:\n        \"\"\"Compute noam annealing learning rate \n            as described in https://arxiv.org/abs/1706.03762 Section 5.3.\n            After warmup_steps learning rate should be always greater than min_lr\n\n        :param float initial_lr: Additional multiplicative factor for learning rate\n        :param int step: Current optimization step\n        :return: Learning rate at given step\n        :rtype: float\n        \"\"\"\n        lrate = self.normalization * min(step ** (-0.5), step * self.warmup_steps ** (-1.5)) * initial_lr\n        if step > self.warmup_steps:\n            lrate = max(self.min_lr, lrate)\n        \n        return lrate","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:31:49.477493Z","iopub.execute_input":"2023-08-10T14:31:49.477873Z","iopub.status.idle":"2023-08-10T14:31:49.542199Z","shell.execute_reply.started":"2023-08-10T14:31:49.477832Z","shell.execute_reply":"2023-08-10T14:31:49.541246Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"%%capture --no-stdout --no-display\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel = E2STransformer(config, audio_paths, train_ds[0][0]).to(device)\n\nscheduler = NoamAnnealing(\n    torch.optim.Adam(model.parameters(), lr=0.2),\n    d_model=512, warmup_steps=500, min_lr=1e-5\n)\n\nlearning_rates = []\nfor _ in range(5000):\n    scheduler.step()\n    learning_rates.append(scheduler.get_last_lr()[0])\n    \nfig, ax = plt.subplots(1, 1, figsize=(10, 4))\nax.plot(learning_rates)\nax.grid(True)\nax.set_xlabel('Step')\nax.set_ylabel('Learning Rate')\nax.set_title('NoamAnnealing')\n\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:07:26.356363Z","iopub.execute_input":"2023-08-07T15:07:26.356795Z","iopub.status.idle":"2023-08-07T15:07:27.976955Z","shell.execute_reply.started":"2023-08-07T15:07:26.35676Z","shell.execute_reply":"2023-08-07T15:07:27.976035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sanity checks","metadata":{}},{"cell_type":"code","source":"eeg, audio = train_ds[15296]\neeg, audio = eeg.unsqueeze(0).to(device), audio.unsqueeze(0).to(device)\n\nout, tgt = model(eeg, audio)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:07:27.978667Z","iopub.execute_input":"2023-08-07T15:07:27.979726Z","iopub.status.idle":"2023-08-07T15:07:28.434128Z","shell.execute_reply.started":"2023-08-07T15:07:27.979689Z","shell.execute_reply":"2023-08-07T15:07:28.433108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    restored = tgt[:, 1:-1, :].squeeze()  # (out_seq_len, d_model)\n    restored = restored @ model.components  # (out_seq_len, n_freq_bins)\n    restored = (restored + model.mean).t().cpu().numpy()  # (n_freq_bins, out_seq_len)\n\nrestored = restore(restored)\nAudio(restored, rate=train_ds.sr)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:07:32.773176Z","iopub.execute_input":"2023-08-07T15:07:32.77355Z","iopub.status.idle":"2023-08-07T15:07:41.17824Z","shell.execute_reply.started":"2023-08-07T15:07:32.773517Z","shell.execute_reply":"2023-08-07T15:07:41.177337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, device, criterion, optimizer, scheduler, scaler, max_norm, description, data_parallel=False):\n        self.model = model\n        self.device = device\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.scaler = scaler\n        self.max_norm = max_norm\n        self.description = description\n        self.hist = []  # training history\n        self.data_parallel = data_parallel\n    \n    @staticmethod\n    def download_file(path, download_file_name):\n        os.chdir('/kaggle/working/')\n        zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n        command = f\"zip {zip_name} {path} -r\"\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n        if result.returncode != 0:\n            print(\"Unable to run zip command!\")\n            print(result.stderr)\n            return\n        display(FileLink(f'{download_file_name}.zip'))\n        \n    def fit(self, n_epochs, train_dl, val_dl):\n        \n        # model checkpoints directory\n        model_checkpoint_path = os.path.join(os.getcwd(), 'model_checkpoints')    \n        if not os.path.exists(model_checkpoint_path):\n            os.mkdir(model_checkpoint_path)\n        \n        model = self.model.to(self.device).train()\n        current_lr = self.scheduler.get_last_lr()[-1]\n        total_step = 0\n        best_val_loss = float('inf')\n        \n        # mean losses\n        train_epoch_loss = []\n        val_epoch_loss = []\n                \n        for epoch in trange(n_epochs):\n            \n            train_epoch_loss = []\n            for eeg, audio in (pbar := tqdm(train_dl, total=len(train_dl))):\n\n                total_step += 1\n                \n                # Move tensors to device\n                eeg, audio = eeg.to(self.device), audio.to(self.device)\n\n                # Clear gradients\n                self.optimizer.zero_grad()\n\n                # Perform forward pass with mixed precision\n                with torch.autocast(device_type='cuda', dtype=torch.float16):\n                    if not self.data_parallel:\n                        pred_encoding, encoding = model(eeg, audio)\n                        loss = self.criterion(pred_encoding, encoding)\n                    else:\n                        predictions = model(eeg, audio)  # tuple of len n_gpus\n                        pred_encoding, encoding = zip(*predictions)  # both are tuples of len n_gpus\n                        loss = criterion(pred_encoding, encoding).mean()\n                \n                train_epoch_loss.append(loss.item())\n                \n                # Perform backward pass\n                \n                # Scaler mutliplies gradients by 2**16 to prevent underflow in amp\n                \n                prev_scale_value = self.scaler.get_scale()\n                \n                self.scaler.scale(loss).backward()\n                self.scaler.unscale_(self.optimizer)\n\n                # Perform Gradient Clipping\n                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), self.max_norm).item()\n\n                # Perform Optimization Step\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n                \n                scale_value = self.scaler.get_scale()\n\n                # Perform Scheduler Step\n                if prev_scale_value == scale_value:\n                    self.scheduler.step()\n                # otherwise optimizer step was not performed\n                \n                current_lr = self.scheduler.get_last_lr()[-1]\n\n                # saving history\n                self.hist.append((total_step, loss.item(), self.description, 'train'))\n\n                # update tqdm info\n                pbar.set_description(\n                    f'Step: {total_step}|\\t\\\n                      Lr: {current_lr:.3e}|\\t\\\n                      GradNorm: {grad_norm:.3e}|\\t\\\n                      Train Loss: {loss.item():.3f}|\\t'\n                )\n\n                # validation\n                if total_step % (256 // train_dl.batch_size) == 0:\n\n                    val_epoch_loss = []\n                    model.eval()\n\n                    with torch.no_grad():\n                        for eeg, audio in (val_pbar := tqdm(val_dl, total=len(val_dl))):\n                            eeg, audio = eeg.to(self.device), audio.to(self.device)\n                            \n                            if not self.data_parallel:\n                                pred_encoding, encoding = model(eeg, audio)\n                                loss = self.criterion(pred_encoding, encoding).item()\n                            else:\n                                predictions = model(eeg, audio)  # tuple of len n_gpus\n                                pred_encoding, encoding = zip(*predictions)  # both are tuples of len n_gpus\n                                loss = criterion(pred_encoding, encoding).mean().item()\n                        \n                            val_epoch_loss.append(loss)\n\n                            # update tqdm info\n                            val_pbar.set_description(f'Validation|\\tVal Loss: {loss:.3f}')\n                            \n                            if len(val_epoch_loss) == len(val_dl):\n                                val_pbar.set_description(f'Validation|\\tVal Loss: {np.mean(val_epoch_loss):.3f}')\n                        \n                        val_epoch_loss = np.mean(val_epoch_loss)\n                        # saving val history\n                        self.hist.append((total_step, val_epoch_loss, self.description, 'val'))\n                        \n                    # save model if it's better than previous ones\n                    if val_epoch_loss < best_val_loss:\n                        best_val_loss = val_epoch_loss\n                        torch.save(model.state_dict(), os.path.join(model_checkpoint_path, 'best_model.pt'))\n                    \n                    model.train()\n                    \n                if total_step in [256 // train_dl.batch_size, 512 // train_dl.batch_size]:\n                    break\n                    \n                # update tqdm info after epoch\n                if len(train_epoch_loss) == len(train_dl):\n                    pbar.set_description(\n                        f'Step: {total_step}|\\t\\\n                          Lr: {current_lr:.3e}|\\t\\\n                          GradNorm: {grad_norm:.3e}|\\t\\\n                          Train Loss: {np.mean(train_epoch_loss):.3f}|\\t\\\n                          Val Loss: {np.mean(val_epoch_loss):.3f}'\n                    )\n\n        # saving objects\n        self.model = model.eval()\n        torch.save(model.state_dict(), os.path.join(model_checkpoint_path, f'{self.description}.pt'))\n        torch.save(self.scaler.state_dict(), os.path.join(model_checkpoint_path, 'scaler.pt'))\n        torch.save(self.optimizer.state_dict(), os.path.join(model_checkpoint_path, 'optimizer.pt'))\n        torch.save(self.scheduler.state_dict(), os.path.join(model_checkpoint_path, 'scheduler.pt'))\n\n        with open(os.path.join(model_checkpoint_path, 'hist.pickle'), 'wb') as handle:\n            pickle.dump(self.hist, handle)\n        \n        print(f'\\nFinished training with the best validation loss {best_val_loss:.3f}\\n')\n        # download files\n        self.download_file(os.path.join(model_checkpoint_path, f'{self.description}.pth'), 'model')\n        self.download_file(os.path.join(model_checkpoint_path, 'hist.pickle'), 'hist')\n        self.download_file(os.path.join(model_checkpoint_path, 'scaler.pt'), 'scaler')\n        self.download_file(os.path.join(model_checkpoint_path, 'optimizer.pt'), 'optimizer')\n        self.download_file(os.path.join(model_checkpoint_path, 'scheduler.pt'), 'scheduler')\n        ","metadata":{"execution":{"iopub.status.busy":"2023-08-09T14:39:10.655267Z","iopub.execute_input":"2023-08-09T14:39:10.655674Z","iopub.status.idle":"2023-08-09T14:39:10.757156Z","shell.execute_reply.started":"2023-08-09T14:39:10.655641Z","shell.execute_reply":"2023-08-09T14:39:10.756051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training on a single GPU P100","metadata":{}},{"cell_type":"code","source":"model = E2STransformer(config, audio_paths, train_ds[0][0])\noptimizer = torch.optim.Adam(model.parameters(), lr=config['base_lr'])\n\ntrainer = Trainer(\n    model=model,\n    device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n    criterion=nn.MSELoss(),\n    optimizer=optimizer,\n    scheduler=NoamAnnealing(optimizer=optimizer, d_model=config['d_model'], warmup_steps=len(train_dl) // 5,\n                            min_lr=config['min_lr']),\n    scaler=torch.cuda.amp.GradScaler(),\n    max_norm=10.0,\n    description='Transformer_2gpu'\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:09:52.471713Z","iopub.execute_input":"2023-08-07T15:09:52.472134Z","iopub.status.idle":"2023-08-07T15:09:53.612271Z","shell.execute_reply.started":"2023-08-07T15:09:52.472101Z","shell.execute_reply":"2023-08-07T15:09:53.610815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrainer.fit(n_epochs=2, train_dl=train_dl(batch_size=8), val_dl=val_dl(batch_size=8))","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:09:53.61426Z","iopub.execute_input":"2023-08-07T15:09:53.614618Z","iopub.status.idle":"2023-08-07T15:17:22.273727Z","shell.execute_reply.started":"2023-08-07T15:09:53.614567Z","shell.execute_reply":"2023-08-07T15:17:22.272325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1.05s/train it\n\nCPU times: user 2min 30s, sys: 6.1 s, total: 2min 36s\n\nWall time: 7min 20s","metadata":{}},{"cell_type":"markdown","source":"## Training on 2 GPU T4 in parallel\n\n![img](https://miro.medium.com/v2/resize:fit:2000/format:webp/1*FpDHkWJhkLL7KxU01Lf9Lw.png)","metadata":{}},{"cell_type":"markdown","source":"### Batch size: 8 (4 per each GPU)","metadata":{}},{"cell_type":"code","source":"model = E2STransformer(config, audio_paths, train_ds[0][0])\nparallel_model = torch.nn.DataParallel(model)\noptimizer = torch.optim.Adam(parallel_model.parameters(), lr=config['base_lr'])\nbatch_size = 8\n\ntrainer = Trainer(\n    model=parallel_model,\n    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n    criterion=nn.MSELoss(),\n    optimizer=optimizer,\n    scheduler=NoamAnnealing(optimizer=optimizer, d_model=config['d_model'], warmup_steps=len(train_dl(batch_size=batch_size)) // 5,\n                            min_lr=config['min_lr']),\n    scaler=torch.cuda.amp.GradScaler(),\n    max_norm=10.0,\n    description='Transformer_2gpu'\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T17:16:25.391715Z","iopub.execute_input":"2023-08-08T17:16:25.392105Z","iopub.status.idle":"2023-08-08T17:16:26.461123Z","shell.execute_reply.started":"2023-08-08T17:16:25.392073Z","shell.execute_reply":"2023-08-08T17:16:26.45974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrainer.fit(n_epochs=2, train_dl=train_dl(batch_size=batch_size), val_dl=val_dl(batch_size=batch_size))","metadata":{"execution":{"iopub.status.busy":"2023-08-08T17:16:26.463157Z","iopub.execute_input":"2023-08-08T17:16:26.463538Z","iopub.status.idle":"2023-08-08T17:24:27.664599Z","shell.execute_reply.started":"2023-08-08T17:16:26.463503Z","shell.execute_reply":"2023-08-08T17:24:27.663361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CPU times: user 4min 49s, sys: 11.7 s, total: 5min\n\nWall time: 7min 58s","metadata":{}},{"cell_type":"markdown","source":"### Batch size: 32 (16 per each GPU)","metadata":{}},{"cell_type":"code","source":"model = E2STransformer(config, audio_paths, train_ds[0][0])\nparallel_model = torch.nn.DataParallel(model)\noptimizer = torch.optim.Adam(parallel_model.parameters(), lr=config['base_lr'])\nbatch_size = 32\n\ntrainer = Trainer(\n    model=parallel_model,\n    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n    criterion=nn.MSELoss(),\n    optimizer=optimizer,\n    scheduler=NoamAnnealing(optimizer=optimizer, d_model=config['d_model'], warmup_steps=len(train_dl(batch_size=batch_size)) // 5,\n                            min_lr=config['min_lr']),\n    scaler=torch.cuda.amp.GradScaler(),\n    max_norm=10.0,\n    description='Transformer_2gpu'\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T16:26:07.714426Z","iopub.execute_input":"2023-08-08T16:26:07.714796Z","iopub.status.idle":"2023-08-08T16:26:09.104157Z","shell.execute_reply.started":"2023-08-08T16:26:07.714764Z","shell.execute_reply":"2023-08-08T16:26:09.102766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrainer.fit(n_epochs=2, train_dl=train_dl(batch_size=batch_size), val_dl=val_dl(batch_size=batch_size))","metadata":{"execution":{"iopub.status.busy":"2023-08-08T16:26:09.111264Z","iopub.execute_input":"2023-08-08T16:26:09.114572Z","iopub.status.idle":"2023-08-08T16:33:46.11671Z","shell.execute_reply.started":"2023-08-08T16:26:09.114516Z","shell.execute_reply":"2023-08-08T16:33:46.115653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CPU times: user 3min 52s, sys: 8.9 s, total: 4min 1s\n\nWall time: 7min 36s","metadata":{}},{"cell_type":"markdown","source":"## Balanced load on a multi-GPU machine\n\n![img](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*F6SXjBp6BCoFTZ26RKnz9A.png)","metadata":{}},{"cell_type":"code","source":"! wget https://gist.githubusercontent.com/neuralsrg/4a70580b9b8fb74f6e7739f1e9ef6ebf/raw/f15f8d5934d4d03e4bb38370f424e652dbd3d955/parallel.py -O /kaggle/working/parallel.py\n\nimport parallel\nfrom parallel import DataParallelModel, DataParallelCriterion","metadata":{"execution":{"iopub.status.busy":"2023-08-09T14:29:12.162219Z","iopub.execute_input":"2023-08-09T14:29:12.162582Z","iopub.status.idle":"2023-08-09T14:29:13.432055Z","shell.execute_reply.started":"2023-08-09T14:29:12.162552Z","shell.execute_reply":"2023-08-09T14:29:13.430916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = E2STransformer(config, audio_paths, train_ds[0][0])\nparallel_model = DataParallelModel(model)\noptimizer = torch.optim.Adam(parallel_model.parameters(), lr=config['base_lr'])\nbatch_size = 32\n\ntrainer = Trainer(\n    model=parallel_model,\n    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n    criterion=DataParallelCriterion(nn.MSELoss(), target_scattered=True),\n    optimizer=optimizer,\n    scheduler=NoamAnnealing(optimizer=optimizer, d_model=config['d_model'], warmup_steps=len(train_dl(batch_size=batch_size)) // 5,\n                            min_lr=config['min_lr']),\n    scaler=torch.cuda.amp.GradScaler(),\n    max_norm=10.0,\n    description='Transformer_2gpu_balanced',\n    data_parallel=True\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T14:41:18.898256Z","iopub.execute_input":"2023-08-09T14:41:18.898667Z","iopub.status.idle":"2023-08-09T14:41:20.261472Z","shell.execute_reply.started":"2023-08-09T14:41:18.898634Z","shell.execute_reply":"2023-08-09T14:41:20.260057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrainer.fit(n_epochs=2, train_dl=train_dl(batch_size=batch_size), val_dl=val_dl(batch_size=batch_size))","metadata":{"execution":{"iopub.status.busy":"2023-08-09T14:41:20.263516Z","iopub.execute_input":"2023-08-09T14:41:20.264205Z","iopub.status.idle":"2023-08-09T14:48:53.532538Z","shell.execute_reply.started":"2023-08-09T14:41:20.264168Z","shell.execute_reply":"2023-08-09T14:48:53.531059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CPU times: user 4min 6s, sys: 9.73 s, total: 4min 16s\n\nWall time: 7min 33s","metadata":{}},{"cell_type":"markdown","source":"# DDP","metadata":{}},{"cell_type":"code","source":"import torch.multiprocessing as mp\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:31:57.296555Z","iopub.execute_input":"2023-08-10T14:31:57.29704Z","iopub.status.idle":"2023-08-10T14:31:57.370709Z","shell.execute_reply.started":"2023-08-10T14:31:57.296998Z","shell.execute_reply":"2023-08-10T14:31:57.369276Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def ddp_setup(rank, world_size):\n    \"\"\"\n    Args:\n        rank: Unique identifier of each process\n        world_size: Total number of processes\n    \"\"\"\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12355\"\n    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n    torch.cuda.set_device(rank)\n\nclass Trainer:\n    def __init__(\n        self,\n        model: torch.nn.Module,\n        train_dl: DataLoader,\n        val_dl: DataLoader,\n        optimizer: torch.optim.Optimizer,\n        scaler: torch.cuda.amp.GradScaler,\n        scheduler: torch.optim.lr_scheduler._LRScheduler,\n        criterion: torch.nn.Module,\n        gpu_id: int,\n        world_size: int = 2,\n        validate_every: int = 300,\n    ) -> None:\n        self.gpu_id = gpu_id\n        self.world_size = world_size\n        self.train_dl = train_dl\n        self.optimizer = optimizer\n        self.model = DDP(model.to(gpu_id), device_ids=[gpu_id])\n        self.scaler = scaler\n        self.scheduler = scheduler\n        self.criterion = criterion\n        self.validate_every = validate_every\n        self.hist = []\n        self.best_val_loss = float('inf') \n\n    def _run_batch(self, eeg, audio):\n        self.optimizer.zero_grad()\n        with torch.autocast(device_type='cuda', dtype=torch.float16):\n            pred_encoding, encoding = self.model(eeg, audio)\n            loss = self.criterion(pred_encoding, encoding)\n            \n        prev_scale_value = self.scaler.get_scale()\n        \n        ##########################################################################################\n        # DDP registers an autograd hook for each parameter given by model.parameters()\n        # and the hook will fire when the corresponding gradient is computed in the backward pass.\n        # Then DDP uses that signal to trigger gradient synchronization across processes.\n        ##########################################################################################\n        # Perform gradient scaling\n        self.scaler.scale(loss).backward()\n        self.scaler.unscale_(self.optimizer)\n        # Perform Gradient Clipping\n        grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), 10.0)\n        # Perform Optimization Step\n        self.scaler.step(self.optimizer)\n        self.scaler.update()\n\n        scale_value = self.scaler.get_scale()\n\n        # Perform Scheduler Step\n        if prev_scale_value == scale_value:\n            self.scheduler.step()\n        # otherwise optimizer step was not performed\n        return loss.item(), grad_norm.item()\n\n    def _run_epoch(self, epoch):\n        self.train_dl.sampler.set_epoch(epoch)\n        train_epoch_loss = []\n        \n        if self.gpu_id == 0:\n            total_steps = len(self.train_dl)\n            for step, (eeg, audio) in enumerate(pbar := tqdm(self.train_dl, total=total_steps)):\n                loss, grad_norm = self._run_batch(eeg.to(self.gpu_id), audio.to(self.gpu_id))\n                train_epoch_loss.append(loss)\n                self.hist.append((loss, 'train'))\n                pbar.set_description(\n                    f'Lr: {self.scheduler.get_last_lr()[-1]:.3e}|\\t\\\n                      GradNorm: {grad_norm:.3e}|\\t\\\n                      Train Loss: {loss:.3f}|\\t'\n                )\n                if step == total_steps - 1:\n                    pbar.set_description(\n                    f'Lr: {self.scheduler.get_last_lr()[-1]:.3e}|\\t\\\n                      GradNorm: {grad_norm:.3e}|\\t\\\n                      Train Loss: {np.mean(train_epoch_loss):.3f}|\\t'\n                    )\n                \n        else:\n            for eeg, audio in self.train_dl:\n                _ = self._run_batch(eeg.to(self.gpu_id), audio.to(self.gpu_id))\n    \n    @staticmethod\n    def download_file(path, download_file_name):\n        os.chdir('/kaggle/working/')\n        zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n        command = f\"zip {zip_name} {path} -r\"\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n        if result.returncode != 0:\n            print(\"Unable to run zip command!\")\n            print(result.stderr)\n            return\n        display(FileLink(f'{download_file_name}.zip'))\n\n    def _save_checkpoint(self, PATH=\"checkpoint.pt\"):\n        ckp = self.model.module.state_dict()\n        torch.save(ckp, PATH)\n        print(f\"Training checkpoint saved at {PATH}\")\n        \n    def _save_objects(self):\n        torch.save(self.scaler.state_dict(), 'scaler.pt')\n        torch.save(self.optimizer.state_dict(), 'optimizer.pt')\n        torch.save(self.scheduler.state_dict(), 'scheduler.pt')\n        with open(os.path.join(model_checkpoint_path, 'hist.pickle'), 'wb') as handle:\n            pickle.dump(self.hist, handle)\n            \n        # download files\n        Trainer.download_file('scaler.pt', 'scaler')\n        Trainer.download_file('optimizer.pt', 'optimizer')\n        Trainer.download_file('scheduler.pt', 'scheduler')\n        Trainer.download_file('hist.pickle', 'hist')\n\n    def train(self, max_epochs: int):\n        for epoch in range(max_epochs):\n            self._run_epoch(epoch)\n            self._validate()\n        if self.gpu_id == 0:\n            print('\\nTraining finished.\\n')\n            self._save_checkpoint(\"final_checkpoint.pt\")\n            self._save_objects()\n    \n    def _validate(self):\n        def run_val_batch(eeg, audio):\n            pred_encoding, encoding = self.model(eeg, audio)\n            return self.criterion(pred_encoding, encoding)\n        \n        self.model.eval()\n        val_epoch_loss = []\n        \n        if self.gpu_id == 0:\n            total_steps = len(self.val_dl)\n            for step, (eeg, audio) in enumerate(val_pbar := tqdm(self.val_dl, total=total_steps)):\n                loss = run_val_batch(eeg.to(self.gpu_id), audio.to(self.gpu_id))\n                tensor_list = [torch.empty(1) for i in range(self.world_size)]\n                torch.distributed.gather(loss, gather_list=tensor_list, dst=0)\n                \n                loss = torch.mean(torch.tensor(tensor_list)).item()\n                val_epoch_loss.append(loss)\n                \n                val_pbar.set_description(f'Val Loss: {loss:.3f}')\n                \n                if step == total_steps - 1:\n                    loss = np.mean(val_epoch_loss)\n                    val_pbar.set_description(f'Val Loss: {loss:.3f}')\n                    self.hist.append((loss, 'val'))\n                    if loss < self.best_val_loss:\n                        self.best_val_loss = loss\n                        self._save_checkpoint()\n        else:\n            for eeg, audio in self.val_dl:\n                loss = run_val_batch(eeg.to(self.gpu_id), audio.to(self.gpu_id))\n                torch.distributed.gather(loss, gather_list=[], dst=0)\n                \n        self.model.train()\n\n    def test(self):\n        t = torch.empty(1).fill_(self.gpu_id)\n        print(f'[{self.gpu_id}]: {t}')","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:31:57.467491Z","iopub.execute_input":"2023-08-10T14:31:57.467864Z","iopub.status.idle":"2023-08-10T14:31:57.552896Z","shell.execute_reply.started":"2023-08-10T14:31:57.467833Z","shell.execute_reply":"2023-08-10T14:31:57.551935Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_dl = partial(DataLoader, dataset=train_ds, shuffle=False)\nval_dl = partial(DataLoader, dataset=val_ds, shuffle=False)\n\ndef main(rank: int, world_size: int, validate_every: int = 300, total_epochs: int = 2, batch_size: int = 32):\n    ddp_setup(rank, world_size)\n    model = E2STransformer(config, audio_paths, train_ds[0][0])\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['base_lr'])\n    trainer = Trainer(\n        model=model,\n        train_dl=train_dl(batch_size=batch_size, pin_memory=True, sampler=DistributedSampler(train_ds)),\n        val_dl=val_dl(batch_size=batch_size, pin_memory=True, sampler=DistributedSampler(val_ds)),\n        optimizer=optimizer,\n        scaler=torch.cuda.amp.GradScaler(),\n        scheduler=NoamAnnealing(optimizer=optimizer, d_model=config['d_model'], warmup_steps=len(train_dl(batch_size=batch_size)) // 5,\n                                min_lr=config['min_lr']),\n        criterion=nn.MSELoss(),\n        gpu_id=rank,\n        world_size=world_size,\n        validate_every=validate_every,\n    )\n    ############################\n    # trainer.train(total_epochs)\n    trainer.test()\n    ############################\n    destroy_process_group()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:32:11.734323Z","iopub.execute_input":"2023-08-10T14:32:11.734687Z","iopub.status.idle":"2023-08-10T14:32:11.79625Z","shell.execute_reply.started":"2023-08-10T14:32:11.734658Z","shell.execute_reply":"2023-08-10T14:32:11.795283Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"world_size = torch.cuda.device_count()\nmp.spawn(main, args=(world_size), nprocs=world_size)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! wget https://raw.githubusercontent.com/pytorch/examples/main/distributed/ddp-tutorial-series/multigpu.py -O multigpu.py\n! wget https://raw.githubusercontent.com/pytorch/examples/main/distributed/ddp-tutorial-series/datautils.py -O datautils.py\n    \nsys.path.append('/kaggle/working')","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:54:39.90493Z","iopub.execute_input":"2023-08-10T14:54:39.905423Z","iopub.status.idle":"2023-08-10T14:54:42.168028Z","shell.execute_reply.started":"2023-08-10T14:54:39.905354Z","shell.execute_reply":"2023-08-10T14:54:42.166893Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"--2023-08-10 14:54:40--  https://raw.githubusercontent.com/pytorch/examples/main/distributed/ddp-tutorial-series/multigpu.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3678 (3.6K) [text/plain]\nSaving to: multigpu.py\n\nmultigpu.py         100%[===================>]   3.59K  --.-KB/s    in 0s      \n\n2023-08-10 14:54:40 (55.6 MB/s) - multigpu.py saved [3678/3678]\n\n--2023-08-10 14:54:41--  https://raw.githubusercontent.com/pytorch/examples/main/distributed/ddp-tutorial-series/datautils.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 331 [text/plain]\nSaving to: datautils.py\n\ndatautils.py        100%[===================>]     331  --.-KB/s    in 0s      \n\n2023-08-10 14:54:42 (15.3 MB/s) - datautils.py saved [331/331]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"! python /kaggle/working/multigpu.py 50 10","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:54:43.900861Z","iopub.execute_input":"2023-08-10T14:54:43.901255Z","iopub.status.idle":"2023-08-10T14:54:59.324509Z","shell.execute_reply.started":"2023-08-10T14:54:43.90122Z","shell.execute_reply":"2023-08-10T14:54:59.323188Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[W socket.cpp:601] [c10d] The client socket has failed to connect to [localhost]:12355 (errno: 99 - Cannot assign requested address).\n[GPU1] Epoch 0 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 0 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 1 | Batchsize: 32 | Steps: 32\nEpoch 0 | Training checkpoint saved at checkpoint.pt\n[GPU0] Epoch 1 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 2 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 2 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 3 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 3 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 4 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 4 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 5 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 5 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 6 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 6 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 7 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 7 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 8 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 8 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 9 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 9 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 10 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 10 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 11 | Batchsize: 32 | Steps: 32\nEpoch 10 | Training checkpoint saved at checkpoint.pt\n[GPU0] Epoch 11 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 12 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 12 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 13 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 13 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 14 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 14 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 15 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 15 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 16 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 16 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 17 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 17 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 18 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 18 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 19 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 19 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 20 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 20 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 21 | Batchsize: 32 | Steps: 32\nEpoch 20 | Training checkpoint saved at checkpoint.pt\n[GPU0] Epoch 21 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 22 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 22 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 23 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 23 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 24 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 24 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 25 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 25 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 26 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 26 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 27 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 27 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 28 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 28 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 29 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 29 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 30 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 30 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 31 | Batchsize: 32 | Steps: 32\nEpoch 30 | Training checkpoint saved at checkpoint.pt\n[GPU0] Epoch 31 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 32 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 32 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 33 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 33 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 34 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 34 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 35 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 35 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 36 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 36 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 37 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 37 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 38 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 38 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 39 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 39 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 40 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 40 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 41 | Batchsize: 32 | Steps: 32\nEpoch 40 | Training checkpoint saved at checkpoint.pt\n[GPU0] Epoch 41 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 42 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 42 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 43 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 43 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 44 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 44 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 45 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 45 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 46 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 46 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 47 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 47 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 48 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 48 | Batchsize: 32 | Steps: 32\n[GPU0] Epoch 49 | Batchsize: 32 | Steps: 32\n[GPU1] Epoch 49 | Batchsize: 32 | Steps: 32\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}